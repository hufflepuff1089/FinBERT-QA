{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"process_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP5yXX/zIpZuZB2Y/IxRCxu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1qGaiNXuzYJN","colab_type":"code","colab":{}},"source":["# docs = {}\n","\n","# with open(path + \"FiQA_train_doc_final.tsv\",'r') as f:\n","#     for line in f:\n","#         # [qid, doc_id, rank]\n","#         line = line.strip().split('\\t')\n","\n","#         if len(line) == 4:\n","#             docid = int(line[1])\n","#             doc = line[2]\n","\n","#             docs[docid] = []\n","#             docs[docid].append(doc)\n","\n","# # take(5, docs.items())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5bV3bU0zgJD","colab_type":"code","colab":{}},"source":["path = \"drive/My Drive/FiQA/\"\n","\n","# Doc ID to Doc text\n","collection = pd.read_csv(path + \"FiQA_train_doc_final.tsv\", sep=\"\\t\")\n","collection = collection[['docid', 'doc']]\n","collection = collection.sort_values(by=['docid'])\n","\n","# Question ID and Question text\n","query_df = pd.read_csv(path + \"FiQA_train_question_final.tsv\", sep=\"\\t\")\n","\n","queries = query_df[['qid', 'question']]\n","\n","all_answers_id = collection['docid'].to_list()\n","#print(all_answers_id)\n","\n","# Question ID and Answer ID pair\n","qid_docid = pd.read_csv(path + \"FiQA_train_question_doc_final.tsv\", sep=\"\\t\")\n","\n","qid_docid = qid_docid [['qid', 'docid']]\n","\n","# Cleaning data\n","empty_docs = []\n","empty_id = []\n","\n","for index, row in collection.iterrows():\n","    if pd.isna(row['doc']):\n","        empty_docs.append(row['docid'])\n","        empty_id.append(index)\n","\n","# print(empty_docs)\n","print(empty_id)\n","\n","# Remove empty answers\n","collection2 = collection.drop(empty_id)\n","\n","# Answers Tokenization\n","collection2['doc_processed'] = collection2['doc'].apply(pre_process)\n","collection2['tokenized_ans'] = collection2.apply(lambda row: wordpunct_tokenize(row['doc_processed']), axis=1)\n","collection2['ans_len'] = collection2.apply(lambda row: len(row['tokenized_ans']), axis=1)\n","\n","collection2.head(5)\n","\n","len(collection2)\n","\n","avg_ans_count = collection2['ans_len'].mean()\n","\n","print(avg_ans_count)\n","\n","# Questions Tokenization\n","queries = queries.copy()\n","queries['q_processed'] = queries['question'].apply(pre_process)\n","queries['tokenized_q'] = queries.apply(lambda row: wordpunct_tokenize(row['q_processed']), axis=1)\n","queries['q_len'] = queries.apply(lambda row: len(row['tokenized_q']), axis=1)\n","\n","queries.head(5)\n","\n","avg_q_count = queries['q_len'].mean()\n","\n","print(avg_q_count)\n","\n","# collection2.to_csv(\"processed_ans.csv\", index=None)\n","# queries.to_csv(\"processed_q.csv\", index=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yUEw_LSzmp4","colab_type":"code","colab":{}},"source":["# word2index = {\"PAD\": 0}\n","# word2count = {}\n","\n","# idx = 1\n","\n","# for index, row in collection2.iterrows():\n","#     for word in row['tokenized_ans']:\n","#         if word not in word2index:\n","#             word2index[word] = idx\n","#             idx += 1\n","#             word2count[word] = 1\n","#         else:\n","#             word2count[word] += 1\n","            \n","# ans_vocab_size = len(word2index)\n","\n","# print(\"Answer vocab size: {}\".format(ans_vocab_size))\n","\n","# idx = len(word2index)\n","\n","# for index, row in queries.iterrows():\n","#     for word in row['tokenized_q']:\n","#         if word not in word2index:\n","#             word2index[word] = idx\n","#             idx += 1\n","#             word2count[word] = 1\n","#         else:\n","#             word2count[word] += 1\n","\n","# print(len(word2index))\n","\n","# q_vocab_size = len(word2index) - ans_vocab_size\n","# print(\"Question vocab size: {}\".format(q_vocab_size))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAadkfOAznRh","colab_type":"code","colab":{}},"source":["# # Reduce the size of the vocabuary\n","# word2idx = {\"PAD\": 0}\n","# word2c = {}\n","# idx = 1\n","\n","# for word, count in word2count.items():\n","#     if count > 3:\n","#         if word not in word2idx:\n","#             word2idx[word] = idx\n","#             idx += 1\n","#             word2c[word] = count\n","\n","# print(word2idx)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPX2NhQ2zrh0","colab_type":"code","colab":{}},"source":["# # Vocab size\n","# len(word2idx)\n","\n","# c = Counter(word2c)\n","# mc = c.most_common(30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZLQA08Dzvtz","colab_type":"code","colab":{}},"source":["# with open('vocab_full.pickle', 'wb') as handle:\n","#     pickle.dump(word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","# with open('vocab.pickle', 'wb') as handle:\n","#     pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# with open('vocab_count.pickle', 'wb') as handle:\n","#     pickle.dump(word2c, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# # Label to answer text\n","# docid_tokenized = collection2[['docid', 'tokenized_ans']]\n","\n","# docid_tokenized.head(5)\n","\n","# label_to_ans = {}\n","\n","# for index, row in docid_tokenized.iterrows():\n","#     label_to_ans[row['docid']] = row['tokenized_ans']\n","\n","# print(take(5, label_to_ans.items()))\n","\n","# with open('label_ans.pickle', 'wb') as handle:\n","#     pickle.dump(label_to_ans, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQzlrA_izwbg","colab_type":"code","colab":{}},"source":["# # Question to question text\n","# q_tokenized = queries[['qid', 'tokenized_q']]\n","\n","# qid_to_text = {}\n","\n","# for index, row in q_tokenized.iterrows():\n","#     qid_to_text[row['qid']] = row['tokenized_q']\n","\n","# print(take(5, qid_to_text.items()))\n","\n","# with open('qid_text.pickle', 'wb') as handle:\n","#     pickle.dump(qid_to_text, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lcHJVB50beB","colab_type":"code","colab":{}},"source":["# Answer Ranking for each question\n","doc_ranking = pd.read_csv(path + \"run_train_small.tsv\", sep=\"\\t\", header=None)\n","doc_ranking = doc_ranking.rename(columns={0: 'qid', 1: 'doc_id', 3:'rank'})\n","\n","# Create dict for query id and ranked candidates\n","# key: query ids, values: list of 1000 ranked candidates\n","qid_ranked_docs = {}\n","\n","with open(path + \"run_train_small.tsv\",'r') as f:\n","    for line in f:\n","        # [qid, doc_id, rank]\n","        line = line.strip().split('\\t')\n","        qid = int(line[0])\n","        doc_id = int(line[1])\n","        rank = int(line[2])\n","        \n","        if qid not in qid_ranked_docs:\n","            # Create a list of size 1000 for each query to store the candidates\n","            candidates = [0]*100\n","            qid_ranked_docs[qid] = candidates\n","        qid_ranked_docs[qid][rank-1] = doc_id\n","        \n","print(take(1, qid_ranked_docs.items()))\n","\n","save_pickle('qid_ranked_docs_100.pickle', qid_ranked_docs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIT1j0BnzMhp","colab_type":"code","colab":{}},"source":["# qid_rel = {}\n","\n","# for index, row in qid_docid.iterrows():\n","    \n","#     if row['qid'] not in qid_rel:\n","#         qid_rel[row['qid']] = []\n","#     qid_rel[row['qid']].append(row['docid'])\n","    \n","# with open('qid_rel.pickle', 'wb') as handle:\n","#     pickle.dump(qid_rel, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]}]}