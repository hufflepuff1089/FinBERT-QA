{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "process_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPRofcaDIT8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovy5Jis3IAz8",
        "colab_type": "code",
        "outputId": "be230473-5aa2-4955-f319-8b52aef195b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from utils import *\n",
        "from process_data import *\n",
        "\n",
        "path = \"drive/My Drive/Thesis/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qGaiNXuzYJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = load_answers_to_dict(path + \"data/raw/FiQA_train_doc_final.tsv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etUrlQ4Y0m8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collection = load_answers_to_df(path + \"data/raw/FiQA_train_doc_final.tsv\")\n",
        "queries = load_questions_to_df(path + \"data/raw/FiQA_train_question_final.tsv\")\n",
        "# Question ID and Answer ID pair\n",
        "qid_docid = load_qid_docid_to_df(path + \"data/raw/FiQA_train_question_doc_final.tsv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geR_D-06aAwe",
        "colab_type": "code",
        "outputId": "7ef72aed-e90c-4777-a75f-4ffda801dd31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"Number of answers: {}\".format(len(collection)))\n",
        "print(\"Number of questions: {}\".format(len(queries)))\n",
        "print(\"Number of QA pairs: {}\".format(len(qid_docid)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of answers: 57638\n",
            "Number of questions: 6648\n",
            "Number of QA pairs: 17110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WtTT2H9YTPwV"
      },
      "source": [
        "# **Clean data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAVwwt-ZTL5y",
        "colab_type": "code",
        "outputId": "779e55a9-1509-4ef5-e658-fc28f768c724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Cleaning data\n",
        "empty_docs, empty_id = get_empty_docs(collection)\n",
        "# Remove empty answers from collection of answers\n",
        "collection_cleaned = collection.drop(empty_id)\n",
        "# Remove empty answers from qa pairs\n",
        "qid_docid = qid_docid[~qid_docid['docid'].isin(empty_docs)]\n",
        "\n",
        "print(\"Number of answers after cleaning: {}\".format(len(collection_cleaned)))\n",
        "print(\"Number of QA pairs after cleaning: {}\".format(len(qid_docid)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of answers after cleaning: 57600\n",
            "Number of QA pairs after cleaning: 17072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cLPWIz0wUnRj"
      },
      "source": [
        "# **Prepare data for Anserini**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cRzxOucdnIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write collection df to file\n",
        "save_tsv(path + \"data/retrieval/collection_cleaned.tsv\", collection_cleaned)\n",
        "\n",
        "# Convert collection df to JSON file for Anserini's document indexer\n",
        "collection_to_json(path + \"data/retrieval/collection_json/docs.json\", path + \"data/retrieval/collection_cleaned.tsv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iH5R4JIR0DN",
        "colab_type": "text"
      },
      "source": [
        "# **Split question and answer pairs into train, test, and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNJN552rb57H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label, test_label, valid_label = split_label(qid_docid)\n",
        "\n",
        "# Save label\n",
        "save_pickle(path + \"data/retrieval/train/qid_rel_train.pickle\", train)\n",
        "save_pickle(path + \"data/retrieval/test/qid_rel_test.pickle\", test)\n",
        "save_pickle(path + \"data/retrieval/valid/qid_rel_valid.pickle\", valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewhrnwlUSN2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_questions, test_questions, valid_questions = split_question(train_label, test_label, valid_label, queries)\n",
        "\n",
        "# Save the questions dataset\n",
        "save_tsv(path + \"data/retrieval/train/train_questions\", train_questions)\n",
        "save_tsv(path + \"data/retrieval/test/test_questions\", test_questions)\n",
        "save_tsv(path + \"data/retrieval/valid/valid_questions\", valid_questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyByyuRLSdR7",
        "colab_type": "code",
        "outputId": "65f8001f-8f6a-4ad1-a2af-cc9ae294bef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Number of questions in each set\n",
        "print(\"Number of questions in the training set: {}\".format(len(train_questions)))\n",
        "print(\"Number of questions in the testing set: {}\".format(len(test_questions)))\n",
        "print(\"Number of questions in the validation set: {}\".format(len(valid_questions)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of questions in the training set: 5681\n",
            "Number of questions in the testing set: 333\n",
            "Number of questions in the validation set: 632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PirJ3iuHWj9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_questions(queries):\n",
        "    queries = queries.copy()\n",
        "    queries['q_processed'] = queries['question'].apply(pre_process)\n",
        "    queries['tokenized_q'] = queries.apply(lambda row: wordpunct_tokenize(row['q_processed']), axis=1)\n",
        "    queries['q_len'] = queries.apply(lambda row: len(row['tokenized_q']), axis=1)\n",
        "\n",
        "    return queries\n",
        "\n",
        "def pre_process(doc):\n",
        "    doc = str(doc)\n",
        "    x = re.sub('[…“”%!&\"@#()\\-\\*\\+,/:;<=>?@[\\]\\^_`{\\}~]', ' ', doc)\n",
        "    y = re.sub('[\\.\\']', \"\", x)\n",
        "    z = y.lower()\n",
        "    return z\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5bV3bU0zgJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Questions Tokenization\n",
        "# def process_questions(queries):\n",
        "#     queries = queries.copy()\n",
        "#     queries['q_processed'] = queries['question'].apply(pre_process)\n",
        "#     queries['tokenized_q'] = queries.apply(lambda row: wordpunct_tokenize(row['q_processed']), axis=1)\n",
        "#     queries['q_len'] = queries.apply(lambda row: len(row['tokenized_q']), axis=1)\n",
        "\n",
        "#     return queries\n",
        "\n",
        "# train_questions = process_questions(train_questions)\n",
        "# train_questions = train_questions[['qid', 'q_processed']]\n",
        "\n",
        "# test_questions = process_questions(test_questions)\n",
        "# test_questions = test_questions[['qid', 'q_processed']]\n",
        "\n",
        "# valid_questions = process_questions(valid_questions)\n",
        "# valid_questions = valid_questions[['qid', 'q_processed']]\n",
        "\n",
        "\n",
        "# # avg_q_count = queries['q_len'].mean()\n",
        "# # print(avg_q_count)\n",
        "\n",
        "# print(len(train_questions))\n",
        "# print(len(test_questions))\n",
        "# print(len(valid_questions))\n",
        "\n",
        "# with open(path + \"data/train_questions.tsv\",'w') as write_tsv:\n",
        "#     write_tsv.write(train_questions.to_csv(sep='\\t', index=False, header=False))\n",
        "\n",
        "# with open(path + \"data/test_questions.tsv\",'w') as write_tsv:\n",
        "#     write_tsv.write(test_questions.to_csv(sep='\\t', index=False, header=False))\n",
        "\n",
        "# with open(path + \"data/valid_questions.tsv\",'w') as write_tsv:\n",
        "#     write_tsv.write(valid_questions.to_csv(sep='\\t', index=False, header=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUIPocCogotl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Answers Tokenization\n",
        "# collection2['doc_processed'] = collection2['doc'].apply(pre_process)\n",
        "# collection2['tokenized_ans'] = collection2.apply(lambda row: wordpunct_tokenize(row['doc_processed']), axis=1)\n",
        "# collection2['ans_len'] = collection2.apply(lambda row: len(row['tokenized_ans']), axis=1)\n",
        "\n",
        "collection2.head(5)\n",
        "\n",
        "# len(collection2)\n",
        "\n",
        "# avg_ans_count = collection2['ans_len'].mean()\n",
        "\n",
        "# print(avg_ans_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Gd_18RJoHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open('empty_docs.pickle', 'wb') as handle:\n",
        "#     pickle.dump(empty_docs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yUEw_LSzmp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2index = {\"PAD\": 0}\n",
        "# word2count = {}\n",
        "\n",
        "# idx = 1\n",
        "\n",
        "# for index, row in collection2.iterrows():\n",
        "#     for word in row['tokenized_ans']:\n",
        "#         if word not in word2index:\n",
        "#             word2index[word] = idx\n",
        "#             idx += 1\n",
        "#             word2count[word] = 1\n",
        "#         else:\n",
        "#             word2count[word] += 1\n",
        "            \n",
        "# ans_vocab_size = len(word2index)\n",
        "\n",
        "# print(\"Answer vocab size: {}\".format(ans_vocab_size))\n",
        "\n",
        "# idx = len(word2index)\n",
        "\n",
        "# for index, row in queries.iterrows():\n",
        "#     for word in row['tokenized_q']:\n",
        "#         if word not in word2index:\n",
        "#             word2index[word] = idx\n",
        "#             idx += 1\n",
        "#             word2count[word] = 1\n",
        "#         else:\n",
        "#             word2count[word] += 1\n",
        "\n",
        "# print(len(word2index))\n",
        "\n",
        "# q_vocab_size = len(word2index) - ans_vocab_size\n",
        "# print(\"Question vocab size: {}\".format(q_vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAadkfOAznRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Reduce the size of the vocabuary\n",
        "# word2idx = {\"PAD\": 0}\n",
        "# word2c = {}\n",
        "# idx = 1\n",
        "\n",
        "# for word, count in word2count.items():\n",
        "#     if count > 3:\n",
        "#         if word not in word2idx:\n",
        "#             word2idx[word] = idx\n",
        "#             idx += 1\n",
        "#             word2c[word] = count\n",
        "\n",
        "# print(word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPX2NhQ2zrh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Vocab size\n",
        "# len(word2idx)\n",
        "\n",
        "# c = Counter(word2c)\n",
        "# mc = c.most_common(30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZLQA08Dzvtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open('vocab_full.pickle', 'wb') as handle:\n",
        "#     pickle.dump(word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "# with open('vocab.pickle', 'wb') as handle:\n",
        "#     pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('vocab_count.pickle', 'wb') as handle:\n",
        "#     pickle.dump(word2c, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# # Label to answer text\n",
        "# docid_tokenized = collection2[['docid', 'tokenized_ans']]\n",
        "\n",
        "# docid_tokenized.head(5)\n",
        "\n",
        "# label_to_ans = {}\n",
        "\n",
        "# for index, row in docid_tokenized.iterrows():\n",
        "#     label_to_ans[row['docid']] = row['tokenized_ans']\n",
        "\n",
        "# print(take(5, label_to_ans.items()))\n",
        "\n",
        "# with open('label_ans.pickle', 'wb') as handle:\n",
        "#     pickle.dump(label_to_ans, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQzlrA_izwbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Question to question text\n",
        "# q_tokenized = queries[['qid', 'tokenized_q']]\n",
        "\n",
        "# qid_to_text = {}\n",
        "\n",
        "# for index, row in q_tokenized.iterrows():\n",
        "#     qid_to_text[row['qid']] = row['tokenized_q']\n",
        "\n",
        "# print(take(5, qid_to_text.items()))\n",
        "\n",
        "# with open('qid_text.pickle', 'wb') as handle:\n",
        "#     pickle.dump(qid_to_text, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lcHJVB50beB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Answer Ranking for each question\n",
        "# doc_ranking = pd.read_csv(path + \"data-bert/cands_train_500.tsv\", sep=\"\\t\", header=None)\n",
        "# doc_ranking = doc_ranking.rename(columns={0: 'qid', 1: 'doc_id', 2:'rank'})\n",
        "\n",
        "# Create dict for query id and ranked candidates\n",
        "# key: query ids, values: list of 1000 ranked candidates\n",
        "qid_ranked_docs = {}\n",
        "\n",
        "with open(path + \"data-bert/cands_train_500.tsv\",'r') as f:\n",
        "    for line in f:\n",
        "        # [qid, doc_id, rank]\n",
        "        line = line.strip().split('\\t')\n",
        "        qid = int(line[0])\n",
        "        doc_id = int(line[1])\n",
        "        rank = int(line[2])\n",
        "        \n",
        "        if qid not in qid_ranked_docs:\n",
        "            # Create a list of size 1000 for each query to store the candidates\n",
        "            candidates = [0]*500\n",
        "            qid_ranked_docs[qid] = candidates\n",
        "        qid_ranked_docs[qid][rank-1] = doc_id\n",
        "        \n",
        "print(take(1, qid_ranked_docs.items()))\n",
        "\n",
        "save_pickle('qid_ranked_docs_100.pickle', qid_ranked_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIT1j0BnzMhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# qid_rel = {}\n",
        "\n",
        "# for index, row in qid_docid.iterrows():\n",
        "    \n",
        "#     if row['qid'] not in qid_rel:\n",
        "#         qid_rel[row['qid']] = []\n",
        "#     qid_rel[row['qid']].append(row['docid'])\n",
        "    \n",
        "# with open('qid_rel.pickle', 'wb') as handle:\n",
        "#     pickle.dump(qid_rel, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}