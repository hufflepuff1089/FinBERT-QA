{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of fiqa2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX2UXHqaWsoH",
        "colab_type": "code",
        "outputId": "2f9e780d-2166-4d2b-d979-250264528bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh0V2HoCmrer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NOakGS8WnB",
        "colab_type": "code",
        "outputId": "8fe468c5-401c-4803-d737-0f3ac24d1506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data \n",
        "import torchtext\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import csv\n",
        "from itertools import islice\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import regex as re\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import torch.utils.data as data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.1 GB\n",
            "Cached:    0.3 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwqmca0M1PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "    \n",
        "def pre_process(doc):\n",
        "    doc = str(doc)\n",
        "    x = re.sub('[…“”%!&\"@#()\\-\\*\\+,/:;<=>?@[\\]\\^_`{\\}~]', ' ', doc)\n",
        "    y = re.sub('[\\.\\']', \"\", x)\n",
        "    z = y.lower()\n",
        "    return z\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_pickle(path, data):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def timer(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Returns the minutes and seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    time = end_time - start_time\n",
        "    mins = int(time / 60)\n",
        "    secs = int(time - (mins * 60))\n",
        "\n",
        "    return mins, secs\n",
        "\n",
        "def pad_seq(seq, max_seq_len):\n",
        "    # Pad each seq to be the same length to process in batch.\n",
        "    # pad_token = 0\n",
        "    if len(seq) >= max_seq_len:\n",
        "        seq = seq[:max_seq_len]\n",
        "    else:\n",
        "        seq += [0]*(max_seq_len - len(seq))\n",
        "    return seq\n",
        "\n",
        "def vectorize(seq, vocab, max_seq_len):\n",
        "    # Map tokens in seq to idx\n",
        "    seq_idx = [vocab[token] for token in seq]\n",
        "    # Pad seq idx\n",
        "    padded_seq_idx = [pad_seq(seq_idx, max_seq_len)]\n",
        "    # padded_seq_idx = pad_seq(seq_idx, max_seq_len)\n",
        "\n",
        "    # return torch.tensor(padded_seq_idx)\n",
        "    return padded_seq_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoDZxXUnEES2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qid_docid = pd.read_csv(path + \"FiQA_train_question_doc_final.tsv\", sep=\"\\t\")\n",
        "qid_docid = qid_docid [['qid', 'docid']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNmKP8_M0x0q",
        "colab_type": "text"
      },
      "source": [
        "**Load pickle files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAYWDxGXyda4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict mapping of token to idx\n",
        "vocab = load_pickle(path + 'vocab_full.pickle')\n",
        "# dict mapping of docid to doc text\n",
        "docid_to_text = load_pickle(path + 'label_ans.pickle')\n",
        "# dict mapping of qid to question text\n",
        "qid_to_text = load_pickle(path + 'qid_text.pickle')\n",
        "# dict mapping of qid to relevant docs\n",
        "qid_rel = load_pickle(path + 'qid_rel.pickle')\n",
        "# dict mapping of qid to ranked candidates\n",
        "qid_ranked_docs = load_pickle(path+'qid_ranked_docs_100.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS9TviMG1BTm",
        "colab_type": "text"
      },
      "source": [
        "**Example data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Xm-zCjj83u",
        "colab_type": "code",
        "outputId": "057b50c0-c6af-4d0d-c8a6-2998aa9f63ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "toy_label = dict(itertools.islice(qid_rel.items(), 100))\n",
        "toy_cand = dict(itertools.islice(qid_ranked_docs.items(), 100))\n",
        "\n",
        "print(toy_label)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: [18850], 1: [14255], 2: [308938], 3: [296717, 100764, 314352, 146317], 4: [196463], 5: [69306], 6: [560251, 188530, 564488], 7: [411063], 8: [566392, 65404], 9: [509122, 184698], 11: [596427], 12: [192516, 338700, 158738], 13: [503678], 14: [398960], 15: [325273], 16: [60590], 17: [146657], 18: [88124], 19: [315086, 142623], 20: [447231], 21: [497642], 23: [550624, 32102], 25: [107584, 562777], 26: [285255, 350819], 27: [537326], 28: [250640], 29: [274832, 114494, 189642, 103662], 30: [551175, 434082, 336922, 19233], 31: [156554], 32: [279480, 69623, 84645], 33: [519798, 425387], 34: [599545], 35: [498681, 80913], 36: [275249, 368649], 37: [523564], 38: [85517, 195207, 357037, 233751], 41: [176229], 42: [272709, 327263, 331981], 43: [76662], 44: [385881], 45: [284610, 261220], 46: [91325], 47: [133299], 48: [108062, 401260, 329810, 512151], 49: [352927], 51: [107817, 257168, 75195], 52: [566417, 125111], 53: [84077, 562798, 102362, 323269, 119308, 184852, 119210, 176196], 54: [590775, 109546, 511651], 55: [324854, 579628, 237207], 56: [572690], 57: [203633, 391403, 77818, 226530], 59: [71601], 60: [381151], 61: [524134], 62: [34810], 63: [509617], 64: [391619], 65: [580624, 109203], 66: [397608, 540395, 405412], 67: [370815, 294864, 18792, 511571], 68: [19183], 69: [378484], 70: [327002], 71: [372052], 72: [549870, 302049], 74: [46680], 75: [297965], 76: [375357], 77: [551315], 78: [152407], 80: [252473], 81: [451207], 82: [500708], 83: [534277], 85: [431230], 86: [71569], 87: [357938, 537593], 88: [415946], 89: [413229, 590102, 268026, 248624, 508754, 64556], 90: [31793], 91: [523431], 92: [465787], 93: [292748], 94: [245447], 95: [586355], 96: [328863], 97: [578529, 77352], 98: [575929, 527522], 99: [474248, 193081, 341413, 136662, 153679], 102: [494264, 187073], 104: [575869, 523158], 105: [41356, 107564], 106: [76695], 107: [276408, 269646], 108: [396982], 109: [73427], 110: [520386, 220063, 410128, 459119, 482165, 397152], 111: [46092], 112: [153541]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7vU-XnlgjTB",
        "colab_type": "code",
        "outputId": "4678e59a-63aa-49f6-d1ab-3efdd8bb5ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_ans = {}\n",
        "\n",
        "for qid, pos_ans_lst in tqdm(toy_label.items()):\n",
        "    for i, cand_lst in toy_cand.items():\n",
        "        trimed_cand = [x for x in cand_lst if x not in pos_ans_lst]\n",
        "    neg_ans_lst = random.sample(trimed_cand, len(pos_ans_lst))\n",
        "    neg_ans[qid] = neg_ans_lst\n",
        "\n",
        "# neg_ans"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 302.07it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcSwoycjoL65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = qid_docid[:177]\n",
        "# test = qid_docid[:16]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sibXqTHtL_PE",
        "colab_type": "code",
        "outputId": "980ee0c2-033f-4240-b630-68f5438ed6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_set = []\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    for k, v in neg_ans.items():\n",
        "        if k == row['qid']:\n",
        "            tmp = []\n",
        "            tmp.append(row['qid'])\n",
        "            tmp.append(row['docid'])\n",
        "            tmp.append(v)\n",
        "            train_set.append(tmp)\n",
        "\n",
        "for idx, sample in enumerate(train_set):\n",
        "    if len(sample[2]) > 1:\n",
        "        sample[2] = random.choice(sample[2])\n",
        "    else:\n",
        "        sample[2] = sample[2][0]\n",
        "\n",
        "len(train_set)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwwJgdVHGcIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d2e04495-1ee9-4943-bc04-f4bdc94127ad"
      },
      "source": [
        "train_set, valid_set = train_test_split(train_set, test_size=0.1)\n",
        "\n",
        "print(\"Number of train data: {}\".format(len(train_set)))\n",
        "print(\"Number of validation data: {}\".format(len(valid_set)))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train data: 159\n",
            "Number of validation data: 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCUqSDFR8J1t",
        "colab_type": "text"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl3hClS13Gz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "n_epochs = 2\n",
        "batch_size = 8\n",
        "hidden_size = 141\n",
        "max_seq_len = 200\n",
        "k = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyaFgL7ANmyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb = torchtext.vocab.GloVe(\"6B\", dim=emb_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-VHSXrLSfF",
        "colab_type": "code",
        "outputId": "5392a3ac-81b6-4e76-c286-57956427f344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# dictionary mapping of word idx to glove vectors\n",
        "emb_weights = np.zeros((vocab_size, emb_dim))\n",
        "words_found = 0\n",
        "print(\"Embedding dim: {}\".format(emb_weights.shape))\n",
        "\n",
        "for token, idx in vocab.items():\n",
        "    # emb.stoi is a dict of token to idx mapping\n",
        "    if token in emb.stoi:\n",
        "        emb_weights[idx] = emb[token]\n",
        "        words_found += 1\n",
        "\n",
        "print(\"vocab size: \", vocab_size)\n",
        "print(words_found, \" words are found in GloVe\")\n",
        "\n",
        "# Convert numpy matrix to tensor\n",
        "emb_weights = torch.from_numpy(emb_weights).float()\n",
        "\n",
        "emb_weights.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding dim: (85034, 100)\n",
            "vocab size:  85034\n",
            "50456  words are found in GloVe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([85034, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8C6zB6MP4bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emb_layer(emb_weights):\n",
        "    vocab_size, emb_dim = emb_weights.shape\n",
        "    emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "    emb_layer.load_state_dict({'weight': emb_weights})\n",
        "\n",
        "    return emb_layer\n",
        "\n",
        "def loss_fn(pos_sim, neg_sim):\n",
        "    margin = 0.2\n",
        "\n",
        "    loss = margin - pos_sim + neg_sim\n",
        "    if loss.data[0] < 0:\n",
        "        loss.data[0] = 0\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxdQU4qJTV73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QA_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "\n",
        "        super(QA_LSTM, self).__init__()\n",
        "\n",
        "        # Shape - (max_seq_len, emb_dim)\n",
        "        self.embedding = create_emb_layer(emb_weights)\n",
        "\n",
        "        self.shared_lstm = nn.LSTM(emb_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, q, a):\n",
        "        # embedding\n",
        "        q = self.embedding(q) # (bs, L, E)\n",
        "        a = self.embedding(a) # (bs, L, E)\n",
        "\n",
        "        # LSTM\n",
        "        q, (hidden, cell) = self.shared_lstm(q) # (bs, L, 2H)\n",
        "        a, (hidden, cell) = self.shared_lstm(a) # (bs, L, 2H)\n",
        "\n",
        "        # Output shape (batch size, seq_len, num_direction * hidden_size)\n",
        "        # There are n of word level biLSTM representations for the seq where n is the number of seq len\n",
        "        # Use max pooling to generate the best representation\n",
        "        q = torch.max(q, 1)[0] \n",
        "        a = torch.max(a, 1)[0] # (bs, 2H)\n",
        "\n",
        "        return self.cos(q, a) # (bs,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HZNF3LQC8Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, q_lst, pos_ans, neg_ans):\n",
        "        'Initialization'\n",
        "        self.q_lst = q_lst\n",
        "        self.pos_ans_lst = pos_ans\n",
        "        self.neg_ans_lst = neg_ans\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.q_lst)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.q_lst[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        q = self.q_lst[index]\n",
        "        pos_ans = self.pos_ans_lst[index]\n",
        "        neg_ans = self.neg_ans_lst[index]\n",
        "\n",
        "        return q, pos_ans, neg_ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OP2Qcdd8xmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_set, optimizer, batch_size):\n",
        "\n",
        "    # Cumulated Training loss\n",
        "    training_loss = 0.0\n",
        "\n",
        "    q_lst = []\n",
        "    pos_lst = []\n",
        "    neg_lst = []\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        " \n",
        "    for i, seq in enumerate(train_set):\n",
        "\n",
        "        ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = vectorize(q_text, vocab, max_seq_len)\n",
        "\n",
        "        q_lst.append(q_vec)\n",
        "\n",
        "        pos_ans_text = docid_to_text[pos_ans]\n",
        "        pos_ans_vec = vectorize(pos_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        pos_lst.append(pos_ans_vec)\n",
        "\n",
        "        neg_ans_text = docid_to_text[neg_ans]\n",
        "        neg_ans_vec = vectorize(neg_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        neg_lst.append(neg_ans_vec)\n",
        "\n",
        "    q_lst = torch.tensor(q_lst)\n",
        "    pos_lst = torch.tensor(pos_lst)\n",
        "    neg_lst = torch.tensor(neg_lst)\n",
        "\n",
        "    train_data = Dataset(q_lst, pos_lst, neg_lst)\n",
        "\n",
        "    train_loader = data.DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "    for ques, pos_ans, neg_ans in tqdm(train_loader):\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for q in ques:\n",
        "            batch_q = q.to(device)\n",
        "\n",
        "        for p in pos_ans:\n",
        "            batch_pos = p.to(device)\n",
        "\n",
        "        for n in neg_ans:\n",
        "            batch_neg = n.to(device)\n",
        "            \n",
        "        # 2. Compute predictions\n",
        "        pos_sim = model(batch_q, batch_pos)    \n",
        "        neg_sim = model(batch_q, batch_neg)\n",
        "\n",
        "        # 3. Compute loss\n",
        "        loss = loss_fn(pos_sim, neg_sim)\n",
        "\n",
        "        # 4. Use loss to compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "            \n",
        "        training_loss += loss.item()\n",
        "            \n",
        "    return training_loss / len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwOnHJH28nUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, valid_set, batch_size):\n",
        "\n",
        "    # Cumulated Training loss\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    q_lst = []\n",
        "    pos_lst = []\n",
        "    neg_lst = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        " \n",
        "    for i, seq in enumerate(train_set):\n",
        "\n",
        "        ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = vectorize(q_text, vocab, max_seq_len)\n",
        "\n",
        "        q_lst.append(q_vec)\n",
        "\n",
        "        pos_ans_text = docid_to_text[pos_ans]\n",
        "        pos_ans_vec = vectorize(pos_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        pos_lst.append(pos_ans_vec)\n",
        "\n",
        "        neg_ans_text = docid_to_text[neg_ans]\n",
        "        neg_ans_vec = vectorize(neg_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        neg_lst.append(neg_ans_vec)\n",
        "\n",
        "    q_lst = torch.tensor(q_lst)\n",
        "    pos_lst = torch.tensor(pos_lst)\n",
        "    neg_lst = torch.tensor(neg_lst)\n",
        "\n",
        "    train_data = Dataset(q_lst, pos_lst, neg_lst)\n",
        "\n",
        "    train_loader = data.DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "        \n",
        "    # Don't calculate the gradients\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for ques, pos_ans, neg_ans in tqdm(train_loader):\n",
        "\n",
        "            for q in ques:\n",
        "                batch_q = q.to(device)\n",
        "\n",
        "            for p in pos_ans:\n",
        "                batch_pos = p.to(device)\n",
        "\n",
        "            for n in neg_ans:\n",
        "                batch_neg = n.to(device)\n",
        "                \n",
        "            pos_sim = model(batch_q, batch_pos)    \n",
        "            neg_sim = model(batch_q, batch_neg)\n",
        "\n",
        "            loss = loss_fn(pos_sim, neg_sim)\n",
        "                \n",
        "            valid_loss += loss.item()\n",
        "                \n",
        "        return valid_loss / len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xylTrRNW-Cyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "5bc298bc-a516-4280-8313-ed00947c0cdc"
      },
      "source": [
        "model = QA_LSTM(vocab_size, emb_dim, hidden_size)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
        "\n",
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss = train(model, train_set, optimizer, batch_size)\n",
        "    # Evaluate validation loss\n",
        "    valid_loss = validate(model, valid_set, batch_size)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # Save the parameters of the model\n",
        "        torch.save(model.state_dict(), 'model-test.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {}\".format(round(train_loss, 3)))\n",
        "    print(\"\\t Validation Loss: {}\\n\".format(round(valid_loss, 3)))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 22.88it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 48.59it/s]\n",
            " 15%|█▌        | 3/20 [00:00<00:00, 23.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 1:\n",
            "\t Train Loss: 0.18\n",
            "\t Validation Loss: 0.059\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 22.56it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 49.57it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 2:\n",
            "\t Train Loss: 0.015\n",
            "\t Validation Loss: 0.009\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-vFCHyGF92F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a14c81fa-57a9-4622-c4ea-91be00541915"
      },
      "source": [
        "test_set = []\n",
        "\n",
        "for qid, docid in toy_label.items():\n",
        "    for k, v in toy_cand.items():\n",
        "        if k == qid:\n",
        "            tmp = []\n",
        "            tmp.append(qid)\n",
        "            tmp.append(docid)\n",
        "            tmp.append(v)\n",
        "            test_set.append(tmp)\n",
        "\n",
        "print(test_set)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, [18850], [531578, 417981, 324911, 524879, 397608, 216077, 173212, 104464, 326261, 434846, 528838, 234436, 571062, 481692, 207449, 338700, 196374, 153377, 327002, 421301, 11538, 375748, 406418, 238271, 322893, 130631, 73427, 560087, 483385, 156554, 531442, 541809, 192843, 553328, 562777, 209224, 351672, 324513, 18850, 283505, 55200, 367754, 297841, 455984, 540395, 160340, 577284, 565935, 354716, 552845, 287474, 179144, 292748, 310612, 194308, 76618, 100764, 534997, 392484, 155490, 83059, 11132, 557186, 348787, 136071, 192516, 234743, 391619, 468741, 12729, 219313, 365558, 396056, 462831, 146657, 178942, 79411, 292919, 309909, 447231, 400230, 540325, 74688, 354511, 245447, 79397, 120500, 237207, 32072, 588509, 308472, 258155, 388042, 18934, 358631, 381151, 145148, 594531, 81599, 195207]], [1, [14255], [231279, 470066, 392484, 14255, 31117, 146657, 257168, 156063, 354716, 365456, 100280, 183612, 208989, 349672, 156444, 81599, 141738, 560776, 216783, 528838, 216077, 47260, 18850, 445298, 151645, 364938, 541809, 192516, 130631, 444899, 59000, 418999, 492456, 12729, 338700, 356884, 202645, 476980, 562802, 536849, 196374, 113876, 395139, 100764, 55200, 399882, 355897, 444273, 588253, 246461, 571062, 5587, 146388, 540395, 254158, 283505, 583956, 328853, 494000, 390435, 532888, 363495, 316932, 590775, 434846, 462831, 182168, 24421, 18934, 525149, 85622, 576985, 248761, 160340, 540325, 599876, 223624, 474795, 397920, 41793, 75005, 522671, 577284, 167494, 427469, 178501, 362069, 289620, 243822, 154931, 73427, 153541, 523564, 307779, 212661, 395011, 313361, 309909, 311285, 175889]], [2, [308938], [400230, 108734, 172306, 308938, 494783, 549437, 279480, 98636, 569342, 214518, 264631, 53155, 543348, 376898, 438975, 29372, 103680, 540285, 39006, 481817, 361889, 350642, 558611, 193592, 104043, 553380, 462585, 358837, 297013, 206140, 122182, 22719, 319265, 69623, 172567, 542213, 177946, 372787, 135196, 405848, 84645, 305470, 41012, 74842, 175522, 180539, 361580, 60020, 552216, 340620, 361037, 443795, 584258, 5219, 342212, 152827, 146317, 64556, 1873, 565355, 114768, 187734, 384968, 271116, 163904, 467059, 282103, 452540, 26011, 177363, 322395, 478807, 83346, 101993, 36880, 406109, 398856, 224102, 279759, 362035, 499604, 129965, 432514, 401111, 246049, 476233, 339545, 498927, 405440, 269575, 48976, 400291, 506065, 127353, 296717, 78597, 463568, 525851, 481459, 23340]], [3, [296717, 100764, 314352, 146317], [537593, 296717, 562777, 364378, 542213, 107584, 395139, 109203, 64556, 195207, 39006, 75195, 367754, 426343, 177946, 357037, 310103, 98636, 413229, 83346, 580624, 390368, 137572, 377152, 77248, 547301, 464449, 325677, 563025, 308938, 399882, 182989, 599142, 438975, 588211, 334902, 467059, 460008, 354314, 73427, 520922, 490326, 422908, 427032, 56044, 297900, 309909, 128465, 95397, 196374, 71569, 400230, 206140, 103662, 89311, 28974, 405996, 300121, 549437, 494826, 586007, 384192, 575869, 21598, 244185, 224102, 489959, 261856, 181616, 3336, 16187, 268257, 40854, 137138, 374443, 516608, 508754, 152027, 391463, 446870, 226568, 143728, 340857, 38394, 100764, 456636, 293628, 492178, 484414, 153377, 484884, 89161, 331384, 298046, 598834, 357520, 404800, 188167, 198895, 196321]], [4, [196463], [397608, 531578, 434846, 53755, 18850, 324911, 462831, 332161, 417981, 391619, 447231, 327002, 173212, 541809, 528838, 219351, 421301, 234743, 390435, 157630, 28504, 541682, 552845, 524879, 55666, 187085, 104464, 83695, 40257, 153377, 146388, 209224, 272137, 455984, 335226, 287857, 540395, 97719, 487196, 445652, 582269, 485318, 427884, 130631, 385929, 33544, 41793, 494323, 497642, 212004, 375748, 65313, 84034, 147837, 47015, 88967, 149210, 245447, 217596, 216783, 79453, 499335, 55791, 281040, 585332, 260146, 405356, 354716, 196463, 553328, 165645, 381151, 316074, 562777, 588509, 54333, 217363, 534997, 94960, 146657, 408434, 232199, 473835, 216077, 267945, 194308, 102954, 388078, 386305, 98727, 574417, 332916, 310612, 528564, 531442, 210187, 350151, 267859, 296345, 377019]], [5, [69306], [480717, 115274, 266163, 204747, 75073, 542022, 64899, 426528, 522798, 130598, 504056, 105448, 528206, 443707, 132547, 50444, 514383, 56329, 484979, 545323, 555430, 120077, 107584, 174296, 372229, 78117, 274400, 32345, 401004, 282947, 52028, 351405, 467444, 553540, 427506, 71338, 173794, 104485, 38963, 291717, 56747, 322995, 35754, 398365, 178356, 1519, 301082, 284351, 525334, 399762, 89812, 9032, 99476, 143499, 391841, 428471, 576288, 7774, 220682, 128533, 258326, 448816, 471950, 351439, 527416, 304951, 141960, 160275, 342871, 341602, 386038, 496670, 384994, 374492, 228054, 568700, 529839, 360716, 237514, 178722, 237338, 91305, 538000, 58426, 265765, 152404, 592309, 137434, 100438, 147684, 286843, 200605, 99745, 582134, 57402, 509261, 538023, 529407, 3550, 466673]], [6, [560251, 188530, 564488], [560251, 210300, 188530, 108734, 78139, 564488, 151506, 477720, 349669, 106319, 399366, 152279, 227926, 257841, 396679, 181855, 571265, 336792, 585282, 366477, 457013, 522358, 417785, 213242, 449001, 467936, 125497, 237039, 234632, 258439, 499098, 382442, 504384, 36193, 54638, 372036, 155131, 379732, 134419, 226984, 314988, 481339, 509075, 286843, 20116, 41214, 16585, 35875, 595121, 25906, 406623, 14461, 317651, 29397, 499483, 473538, 358445, 44593, 227132, 349764, 404852, 121465, 233544, 238682, 575408, 557870, 77016, 593554, 416683, 482244, 224421, 490427, 432067, 488994, 298108, 438869, 490529, 579007, 11719, 548465, 47373, 73741, 589, 388016, 416523, 565367, 533589, 477637, 347759, 62764, 115553, 263088, 110948, 229777, 111594, 76285, 371759, 185020, 156143, 517784]], [7, [411063], [227757, 402437, 257168, 141458, 334603, 225380, 128435, 338700, 468741, 384994, 260603, 19640, 362778, 523792, 393553, 145016, 125111, 297841, 219274, 588253, 130631, 246157, 156554, 291996, 589398, 542213, 76120, 473373, 115274, 577937, 385320, 262761, 276411, 529839, 259708, 290862, 391561, 538199, 532012, 203485, 483664, 181187, 483385, 220334, 129355, 388713, 8163, 375423, 237207, 339115, 297427, 158258, 535673, 253210, 50310, 228737, 289501, 534997, 476562, 426343, 62335, 549708, 279870, 64899, 19698, 420180, 144190, 300768, 108826, 480717, 454537, 198226, 287214, 407524, 414834, 42924, 115899, 390368, 230973, 30343, 597909, 367754, 376839, 465294, 59782, 274832, 133299, 181032, 543686, 462831, 256981, 202114, 79397, 527037, 477940, 91325, 81886, 8200, 525149, 429555]], [8, [566392, 65404], [65404, 12655, 456636, 173545, 86852, 212713, 589, 316359, 358743, 590102, 146317, 508754, 35534, 350508, 157712, 290508, 64556, 367754, 580624, 346852, 529459, 543812, 199508, 261856, 583694, 272279, 318108, 357612, 319786, 345389, 301833, 29372, 132678, 223645, 18647, 474266, 175196, 309171, 456773, 445739, 271102, 296717, 271974, 547301, 450925, 262485, 374258, 200248, 537593, 369445, 339648, 564553, 122182, 457013, 570004, 469194, 84645, 176017, 498631, 177946, 124191, 239632, 228396, 58511, 38394, 567201, 596549, 181616, 216200, 340857, 383863, 292989, 525803, 428671, 25397, 57373, 590837, 279480, 135196, 32092, 544174, 489199, 492178, 287540, 471872, 484535, 316074, 415737, 104492, 164801, 243732, 89211, 182866, 130350, 489959, 115968, 566607, 322456, 417133, 243602]], [9, [509122, 184698], [167494, 153377, 525149, 283505, 330, 313361, 562176, 132780, 540634, 25439, 32973, 408112, 51211, 381151, 28764, 451020, 196374, 278824, 466442, 248624, 466718, 172745, 309023, 84034, 50992, 119985, 250498, 55500, 588253, 546372, 207449, 427017, 547941, 586026, 523564, 219187, 95441, 538208, 516108, 449155, 154113, 273472, 35810, 145016, 402273, 127021, 509122, 160859, 357961, 385121, 141928, 581456, 11569, 269380, 113776, 300768, 220682, 175072, 479276, 511651, 114327, 418630, 272820, 277835, 482813, 14255, 260499, 68486, 389091, 298777, 201125, 446708, 211094, 356743, 424008, 84804, 124875, 88967, 570466, 492371, 465680, 489573, 406490, 258412, 293310, 581265, 100655, 244484, 562685, 134270, 286654, 11295, 439929, 525967, 332757, 17448, 19880, 316494, 40729, 269812]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9YIIOZt3waV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model, test_set, qid_rel, max_seq_len, k):\n",
        "    \n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "\n",
        "        ques, pos_ans, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = torch.tensor(vectorize(q_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "        cands_text = [docid_to_text[c] if c is not 0 else \"\" for c in cands]\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        for cand in cands_text:\n",
        "            a_vec = torch.tensor(vectorize(cand, vocab, max_seq_len)).to(device)\n",
        "            scores.append(model(q_vec, a_vec).item())\n",
        "\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        qid_pred_rank[ques] = ranked_ans\n",
        "\n",
        "    MRR, average_ndcg, precision = evaluate(qid_ranked_docs, qid_rel, k)\n",
        "\n",
        "    return MRR, average_ndcg, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Kd6zOOBkbc",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99aL2wiN4pFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "b9b6f898-0f70-49a0-a75a-76cccb05ffce"
      },
      "source": [
        "# Load the model with the best validation loss\n",
        "model.load_state_dict(torch.load('model-test.pt'))\n",
        "\n",
        "MRR, average_ndcg, precision = eval(model, test_set, qid_rel, max_seq_len, k=10)\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:10<00:00,  1.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 10 queries: 0.36504981770986045\n",
            "\n",
            "MRR@10 for 10 queries: 0.3104752234828942\n",
            "\n",
            "Average Precision@1: 0.23901925391095066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}