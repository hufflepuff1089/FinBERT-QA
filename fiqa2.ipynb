{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of fiqa2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX2UXHqaWsoH",
        "colab_type": "code",
        "outputId": "9cec1d4d-6df9-410f-f795-c5ac119a290b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh0V2HoCmrer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NOakGS8WnB",
        "colab_type": "code",
        "outputId": "a0b24cde-7572-43e0-88a6-087e90e297f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data \n",
        "import torchtext\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import csv\n",
        "from itertools import islice\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import regex as re\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import torch.utils.data as data\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.1 GB\n",
            "Cached:    0.3 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwqmca0M1PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "    \n",
        "def pre_process(doc):\n",
        "    doc = str(doc)\n",
        "    x = re.sub('[…“”%!&\"@#()\\-\\*\\+,/:;<=>?@[\\]\\^_`{\\}~]', ' ', doc)\n",
        "    y = re.sub('[\\.\\']', \"\", x)\n",
        "    z = y.lower()\n",
        "    return z\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_pickle(path, data):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def pad_seq(seq, max_seq_len):\n",
        "    # Pad each seq to be the same length to process in batch.\n",
        "    # pad_token = 0\n",
        "    if len(seq) >= max_seq_len:\n",
        "        seq = seq[:max_seq_len]\n",
        "    else:\n",
        "        seq += [0]*(max_seq_len - len(seq))\n",
        "    return seq\n",
        "\n",
        "def vectorize(seq, vocab, max_seq_len):\n",
        "    # Map tokens in seq to idx\n",
        "    seq_idx = [vocab[token] for token in seq]\n",
        "    # Pad seq idx\n",
        "    padded_seq_idx = [pad_seq(seq_idx, max_seq_len)]\n",
        "    # padded_seq_idx = pad_seq(seq_idx, max_seq_len)\n",
        "\n",
        "    # return torch.tensor(padded_seq_idx)\n",
        "    return padded_seq_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoDZxXUnEES2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qid_docid = pd.read_csv(path + \"FiQA_train_question_doc_final.tsv\", sep=\"\\t\")\n",
        "qid_docid = qid_docid [['qid', 'docid']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNmKP8_M0x0q",
        "colab_type": "text"
      },
      "source": [
        "**Load pickle files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAYWDxGXyda4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict mapping of token to idx\n",
        "vocab = load_pickle(path + 'vocab_full.pickle')\n",
        "# dict mapping of docid to doc text\n",
        "docid_to_text = load_pickle(path + 'label_ans.pickle')\n",
        "# dict mapping of qid to question text\n",
        "qid_to_text = load_pickle(path + 'qid_text.pickle')\n",
        "# dict mapping of qid to relevant docs\n",
        "qid_rel = load_pickle(path + 'qid_rel.pickle')\n",
        "# dict mapping of qid to ranked candidates\n",
        "qid_ranked_docs = load_pickle(path+'qid_ranked_docs_100.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS9TviMG1BTm",
        "colab_type": "text"
      },
      "source": [
        "**Example data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Xm-zCjj83u",
        "colab_type": "code",
        "outputId": "158676bb-8a08-4f09-e3fa-02d8eaea7082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "toy_label = dict(itertools.islice(qid_rel.items(), 10))\n",
        "toy_cand = dict(itertools.islice(qid_ranked_docs.items(), 10))\n",
        "\n",
        "print(toy_label)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: [18850], 1: [14255], 2: [308938], 3: [296717, 100764, 314352, 146317], 4: [196463], 5: [69306], 6: [560251, 188530, 564488], 7: [411063], 8: [566392, 65404], 9: [509122, 184698]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7vU-XnlgjTB",
        "colab_type": "code",
        "outputId": "1bbf4489-2352-479e-9420-25c88848f3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_ans = {}\n",
        "\n",
        "for qid, pos_ans_lst in tqdm(toy_label.items()):\n",
        "    for i, cand_lst in toy_cand.items():\n",
        "        trimed_cand = [x for x in cand_lst if x not in pos_ans_lst]\n",
        "    neg_ans_lst = random.sample(trimed_cand, len(pos_ans_lst))\n",
        "    neg_ans[qid] = neg_ans_lst\n",
        "\n",
        "# neg_ans"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 1227.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcSwoycjoL65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test = qid_docid[:177]\n",
        "test = qid_docid[:16]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sibXqTHtL_PE",
        "colab_type": "code",
        "outputId": "8a348d5c-5f28-47da-ac29-5e00761e3fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "train_set = []\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    for k, v in neg_ans.items():\n",
        "        if k == row['qid']:\n",
        "            tmp = []\n",
        "            tmp.append(row['qid'])\n",
        "            tmp.append(row['docid'])\n",
        "            tmp.append(v)\n",
        "            train_set.append(tmp)\n",
        "\n",
        "for idx, sample in enumerate(train_set):\n",
        "    if len(sample[2]) > 1:\n",
        "        sample[2] = random.choice(sample[2])\n",
        "    else:\n",
        "        sample[2] = sample[2][0]\n",
        "\n",
        "train_set"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 18850, 172745],\n",
              " [1, 14255, 449155],\n",
              " [2, 308938, 141928],\n",
              " [3, 296717, 141928],\n",
              " [3, 100764, 25439],\n",
              " [3, 314352, 55500],\n",
              " [3, 146317, 260499],\n",
              " [4, 196463, 298777],\n",
              " [5, 69306, 581265],\n",
              " [6, 560251, 466442],\n",
              " [6, 188530, 35810],\n",
              " [6, 564488, 154113],\n",
              " [7, 411063, 547941],\n",
              " [8, 566392, 424008],\n",
              " [8, 65404, 424008],\n",
              " [9, 509122, 427017]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwwJgdVHGcIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = {}\n",
        "\n",
        "# # for idx, sample in enumerate(train_set):\n",
        "# #     train[sample[0]]= [sample[1], sample[2]]\n",
        "\n",
        "\n",
        "# train_part = {}\n",
        "\n",
        "# for idx, sample in enumerate(train_set):\n",
        "#     if \"qid\" not in train_part:\n",
        "#         train_part[\"qid\"] = []\n",
        "#     else:\n",
        "#         train_part[\"qid\"].append(sample[0])\n",
        "\n",
        "# train_part"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KExX7v_v2kVZ",
        "colab_type": "text"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl3hClS13Gz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "n_epochs = 2\n",
        "batch_size = 256\n",
        "hidden_size = 141\n",
        "max_seq_len = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyaFgL7ANmyk",
        "colab_type": "code",
        "outputId": "34f9bceb-d588-4ced-9e83-46e9ed6ebcb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "emb = torchtext.vocab.GloVe(\"6B\", dim=emb_dim)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                           \n",
            " 99%|█████████▉| 397531/400000 [00:14<00:00, 25630.97it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-VHSXrLSfF",
        "colab_type": "code",
        "outputId": "d628a1eb-3266-4b80-c52a-2e1637a2aa0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# dictionary mapping of word idx to glove vectors\n",
        "emb_weights = np.zeros((vocab_size, emb_dim))\n",
        "words_found = 0\n",
        "print(\"Embedding dim: {}\".format(emb_weights.shape))\n",
        "\n",
        "for token, idx in vocab.items():\n",
        "    # emb.stoi is a dict of token to idx mapping\n",
        "    if token in emb.stoi:\n",
        "        emb_weights[idx] = emb[token]\n",
        "        words_found += 1\n",
        "\n",
        "print(\"vocab size: \", vocab_size)\n",
        "print(words_found, \" words are found in GloVe\")\n",
        "\n",
        "# Convert numpy matrix to tensor\n",
        "emb_weights = torch.from_numpy(emb_weights).float()\n",
        "\n",
        "emb_weights.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding dim: (85034, 100)\n",
            "vocab size:  85034\n",
            "50456  words are found in GloVe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([85034, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8C6zB6MP4bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emb_layer(emb_weights):\n",
        "    vocab_size, emb_dim = emb_weights.shape\n",
        "    emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "    emb_layer.load_state_dict({'weight': emb_weights})\n",
        "\n",
        "    return emb_layer\n",
        "\n",
        "def loss_fn(pos_sim, neg_sim):\n",
        "    margin = 0.2\n",
        "\n",
        "    loss = margin - pos_sim + neg_sim\n",
        "    if loss.data[0] < 0:\n",
        "        loss.data[0] = 0\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxdQU4qJTV73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QA_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "\n",
        "        super(QA_LSTM, self).__init__()\n",
        "\n",
        "        # Shape - (max_seq_len, emb_dim)\n",
        "        self.embedding = create_emb_layer(emb_weights)\n",
        "\n",
        "        self.shared_lstm = nn.LSTM(emb_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, q, a):\n",
        "        # embedding\n",
        "        q = self.embedding(q) # (bs, L, E)\n",
        "        a = self.embedding(a) # (bs, L, E)\n",
        "\n",
        "        # LSTM\n",
        "        q, (hidden, cell) = self.shared_lstm(q) # (bs, L, 2H)\n",
        "        a, (hidden, cell) = self.shared_lstm(a) # (bs, L, 2H)\n",
        "\n",
        "        # Output shape (batch size, seq_len, num_direction * hidden_size)\n",
        "        # There are n of word level biLSTM representations for the seq where n is the number of seq len\n",
        "        # Use max pooling to generate the best representation\n",
        "        q = torch.max(q, 1)[0] \n",
        "        a = torch.max(a, 1)[0] # (bs, 2H)\n",
        "\n",
        "        return self.cos(q, a) # (bs,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HZNF3LQC8Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, q_lst, pos_ans, neg_ans):\n",
        "        'Initialization'\n",
        "        self.q_lst = q_lst\n",
        "        self.pos_ans_lst = pos_ans\n",
        "        self.neg_ans_lst = neg_ans\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.q_lst)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.q_lst[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        q = self.q_lst[index]\n",
        "        pos_ans = self.pos_ans_lst[index]\n",
        "        neg_ans = self.neg_ans_lst[index]\n",
        "\n",
        "        return q, pos_ans, neg_ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDEj8cltbz5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "a5932dec-0629-4632-99d8-f4f3755b3917"
      },
      "source": [
        "def train(model, train_set, optimizer, n_epochs=2, batch_size=8):\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        # Cumulated Training loss\n",
        "        training_loss = 0.0\n",
        "\n",
        "        q_lst = []\n",
        "        pos_lst = []\n",
        "        neg_lst = []\n",
        " \n",
        "        for i, seq in enumerate(train_set):\n",
        "\n",
        "            ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "            q_text = qid_to_text[ques]\n",
        "            q_vec = vectorize(q_text, vocab, max_seq_len)\n",
        "\n",
        "            q_lst.append(q_vec)\n",
        "\n",
        "            pos_ans_text = docid_to_text[pos_ans]\n",
        "            pos_ans_vec = vectorize(pos_ans_text, vocab, max_seq_len)\n",
        "\n",
        "            pos_lst.append(pos_ans_vec)\n",
        "\n",
        "            neg_ans_text = docid_to_text[neg_ans]\n",
        "            neg_ans_vec = vectorize(neg_ans_text, vocab, max_seq_len)\n",
        "\n",
        "            neg_lst.append(neg_ans_vec)\n",
        "\n",
        "        q_lst = torch.tensor(q_lst)\n",
        "        pos_lst = torch.tensor(pos_lst)\n",
        "        neg_lst = torch.tensor(neg_lst)\n",
        "\n",
        "        train_data = Dataset(q_lst, pos_lst, neg_lst)\n",
        "\n",
        "        train_loader = data.DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "        for ques, pos_ans, neg_ans in tqdm(train_loader):\n",
        "            # 1. Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for q in ques:\n",
        "                batch_q = q.to(device)\n",
        "\n",
        "            for p in pos_ans:\n",
        "                batch_pos = p.to(device)\n",
        "\n",
        "            for n in neg_ans:\n",
        "                batch_neg = n.to(device)\n",
        "            \n",
        "            # 2. Compute predictions\n",
        "            pos_sim = model(batch_q, batch_pos)    \n",
        "            neg_sim = model(batch_q, batch_neg)\n",
        "\n",
        "            # 3. Compute loss\n",
        "            loss = loss_fn(pos_sim, neg_sim)\n",
        "\n",
        "            # 4. Use loss to compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # 5. Use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            \n",
        "            training_loss += loss.item()\n",
        "            \n",
        "        avg_loss = training_loss / len(train_loader)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Epoch: {}, loss: {}\".format(epoch+1, avg_loss))\n",
        "\n",
        "        # filename = '{}/Epoch-{}.model'.format('.', epoch)\n",
        "        # save_checkpoint({\n",
        "        #     'epoch': epoch + 1,\n",
        "        #     'state_dict': model.state_dict(),\n",
        "        #     'optimizer' : optimizer.state_dict(),\n",
        "        # }, filename=filename)\n",
        "        # test(model, test_data)\n",
        "\n",
        "model = QA_LSTM(vocab_size, emb_dim, hidden_size)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
        "\n",
        "train(model, train_set, optimizer)\n"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00, 20.33it/s]\u001b[A\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00, 20.47it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch: 1, loss: 0.19364887475967407\n",
            "\n",
            "\n",
            "Epoch: 2, loss: 0.03658381104469299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d1c1aa7b-8775-4486-b972-856fb67280a7",
        "id": "GGfu1r_dSTe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 16/16 [00:00<00:00, 5398.07it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-vFCHyGF92F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "1b91057e-a9f5-4184-df30-3b40c5354d57"
      },
      "source": [
        "test_set = []\n",
        "\n",
        "for qid, docid in toy_label.items():\n",
        "    for k, v in toy_cand.items():\n",
        "        if k == qid:\n",
        "            tmp = []\n",
        "            tmp.append(qid)\n",
        "            tmp.append(docid)\n",
        "            tmp.append(v)\n",
        "            test_set.append(tmp)\n",
        "\n",
        "print(test_set)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, [18850], [531578, 417981, 324911, 524879, 397608, 216077, 173212, 104464, 326261, 434846, 528838, 234436, 571062, 481692, 207449, 338700, 196374, 153377, 327002, 421301, 11538, 375748, 406418, 238271, 322893, 130631, 73427, 560087, 483385, 156554, 531442, 541809, 192843, 553328, 562777, 209224, 351672, 324513, 18850, 283505, 55200, 367754, 297841, 455984, 540395, 160340, 577284, 565935, 354716, 552845, 287474, 179144, 292748, 310612, 194308, 76618, 100764, 534997, 392484, 155490, 83059, 11132, 557186, 348787, 136071, 192516, 234743, 391619, 468741, 12729, 219313, 365558, 396056, 462831, 146657, 178942, 79411, 292919, 309909, 447231, 400230, 540325, 74688, 354511, 245447, 79397, 120500, 237207, 32072, 588509, 308472, 258155, 388042, 18934, 358631, 381151, 145148, 594531, 81599, 195207]], [1, [14255], [231279, 470066, 392484, 14255, 31117, 146657, 257168, 156063, 354716, 365456, 100280, 183612, 208989, 349672, 156444, 81599, 141738, 560776, 216783, 528838, 216077, 47260, 18850, 445298, 151645, 364938, 541809, 192516, 130631, 444899, 59000, 418999, 492456, 12729, 338700, 356884, 202645, 476980, 562802, 536849, 196374, 113876, 395139, 100764, 55200, 399882, 355897, 444273, 588253, 246461, 571062, 5587, 146388, 540395, 254158, 283505, 583956, 328853, 494000, 390435, 532888, 363495, 316932, 590775, 434846, 462831, 182168, 24421, 18934, 525149, 85622, 576985, 248761, 160340, 540325, 599876, 223624, 474795, 397920, 41793, 75005, 522671, 577284, 167494, 427469, 178501, 362069, 289620, 243822, 154931, 73427, 153541, 523564, 307779, 212661, 395011, 313361, 309909, 311285, 175889]], [2, [308938], [400230, 108734, 172306, 308938, 494783, 549437, 279480, 98636, 569342, 214518, 264631, 53155, 543348, 376898, 438975, 29372, 103680, 540285, 39006, 481817, 361889, 350642, 558611, 193592, 104043, 553380, 462585, 358837, 297013, 206140, 122182, 22719, 319265, 69623, 172567, 542213, 177946, 372787, 135196, 405848, 84645, 305470, 41012, 74842, 175522, 180539, 361580, 60020, 552216, 340620, 361037, 443795, 584258, 5219, 342212, 152827, 146317, 64556, 1873, 565355, 114768, 187734, 384968, 271116, 163904, 467059, 282103, 452540, 26011, 177363, 322395, 478807, 83346, 101993, 36880, 406109, 398856, 224102, 279759, 362035, 499604, 129965, 432514, 401111, 246049, 476233, 339545, 498927, 405440, 269575, 48976, 400291, 506065, 127353, 296717, 78597, 463568, 525851, 481459, 23340]], [3, [296717, 100764, 314352, 146317], [537593, 296717, 562777, 364378, 542213, 107584, 395139, 109203, 64556, 195207, 39006, 75195, 367754, 426343, 177946, 357037, 310103, 98636, 413229, 83346, 580624, 390368, 137572, 377152, 77248, 547301, 464449, 325677, 563025, 308938, 399882, 182989, 599142, 438975, 588211, 334902, 467059, 460008, 354314, 73427, 520922, 490326, 422908, 427032, 56044, 297900, 309909, 128465, 95397, 196374, 71569, 400230, 206140, 103662, 89311, 28974, 405996, 300121, 549437, 494826, 586007, 384192, 575869, 21598, 244185, 224102, 489959, 261856, 181616, 3336, 16187, 268257, 40854, 137138, 374443, 516608, 508754, 152027, 391463, 446870, 226568, 143728, 340857, 38394, 100764, 456636, 293628, 492178, 484414, 153377, 484884, 89161, 331384, 298046, 598834, 357520, 404800, 188167, 198895, 196321]], [4, [196463], [397608, 531578, 434846, 53755, 18850, 324911, 462831, 332161, 417981, 391619, 447231, 327002, 173212, 541809, 528838, 219351, 421301, 234743, 390435, 157630, 28504, 541682, 552845, 524879, 55666, 187085, 104464, 83695, 40257, 153377, 146388, 209224, 272137, 455984, 335226, 287857, 540395, 97719, 487196, 445652, 582269, 485318, 427884, 130631, 385929, 33544, 41793, 494323, 497642, 212004, 375748, 65313, 84034, 147837, 47015, 88967, 149210, 245447, 217596, 216783, 79453, 499335, 55791, 281040, 585332, 260146, 405356, 354716, 196463, 553328, 165645, 381151, 316074, 562777, 588509, 54333, 217363, 534997, 94960, 146657, 408434, 232199, 473835, 216077, 267945, 194308, 102954, 388078, 386305, 98727, 574417, 332916, 310612, 528564, 531442, 210187, 350151, 267859, 296345, 377019]], [5, [69306], [480717, 115274, 266163, 204747, 75073, 542022, 64899, 426528, 522798, 130598, 504056, 105448, 528206, 443707, 132547, 50444, 514383, 56329, 484979, 545323, 555430, 120077, 107584, 174296, 372229, 78117, 274400, 32345, 401004, 282947, 52028, 351405, 467444, 553540, 427506, 71338, 173794, 104485, 38963, 291717, 56747, 322995, 35754, 398365, 178356, 1519, 301082, 284351, 525334, 399762, 89812, 9032, 99476, 143499, 391841, 428471, 576288, 7774, 220682, 128533, 258326, 448816, 471950, 351439, 527416, 304951, 141960, 160275, 342871, 341602, 386038, 496670, 384994, 374492, 228054, 568700, 529839, 360716, 237514, 178722, 237338, 91305, 538000, 58426, 265765, 152404, 592309, 137434, 100438, 147684, 286843, 200605, 99745, 582134, 57402, 509261, 538023, 529407, 3550, 466673]], [6, [560251, 188530, 564488], [560251, 210300, 188530, 108734, 78139, 564488, 151506, 477720, 349669, 106319, 399366, 152279, 227926, 257841, 396679, 181855, 571265, 336792, 585282, 366477, 457013, 522358, 417785, 213242, 449001, 467936, 125497, 237039, 234632, 258439, 499098, 382442, 504384, 36193, 54638, 372036, 155131, 379732, 134419, 226984, 314988, 481339, 509075, 286843, 20116, 41214, 16585, 35875, 595121, 25906, 406623, 14461, 317651, 29397, 499483, 473538, 358445, 44593, 227132, 349764, 404852, 121465, 233544, 238682, 575408, 557870, 77016, 593554, 416683, 482244, 224421, 490427, 432067, 488994, 298108, 438869, 490529, 579007, 11719, 548465, 47373, 73741, 589, 388016, 416523, 565367, 533589, 477637, 347759, 62764, 115553, 263088, 110948, 229777, 111594, 76285, 371759, 185020, 156143, 517784]], [7, [411063], [227757, 402437, 257168, 141458, 334603, 225380, 128435, 338700, 468741, 384994, 260603, 19640, 362778, 523792, 393553, 145016, 125111, 297841, 219274, 588253, 130631, 246157, 156554, 291996, 589398, 542213, 76120, 473373, 115274, 577937, 385320, 262761, 276411, 529839, 259708, 290862, 391561, 538199, 532012, 203485, 483664, 181187, 483385, 220334, 129355, 388713, 8163, 375423, 237207, 339115, 297427, 158258, 535673, 253210, 50310, 228737, 289501, 534997, 476562, 426343, 62335, 549708, 279870, 64899, 19698, 420180, 144190, 300768, 108826, 480717, 454537, 198226, 287214, 407524, 414834, 42924, 115899, 390368, 230973, 30343, 597909, 367754, 376839, 465294, 59782, 274832, 133299, 181032, 543686, 462831, 256981, 202114, 79397, 527037, 477940, 91325, 81886, 8200, 525149, 429555]], [8, [566392, 65404], [65404, 12655, 456636, 173545, 86852, 212713, 589, 316359, 358743, 590102, 146317, 508754, 35534, 350508, 157712, 290508, 64556, 367754, 580624, 346852, 529459, 543812, 199508, 261856, 583694, 272279, 318108, 357612, 319786, 345389, 301833, 29372, 132678, 223645, 18647, 474266, 175196, 309171, 456773, 445739, 271102, 296717, 271974, 547301, 450925, 262485, 374258, 200248, 537593, 369445, 339648, 564553, 122182, 457013, 570004, 469194, 84645, 176017, 498631, 177946, 124191, 239632, 228396, 58511, 38394, 567201, 596549, 181616, 216200, 340857, 383863, 292989, 525803, 428671, 25397, 57373, 590837, 279480, 135196, 32092, 544174, 489199, 492178, 287540, 471872, 484535, 316074, 415737, 104492, 164801, 243732, 89211, 182866, 130350, 489959, 115968, 566607, 322456, 417133, 243602]], [9, [509122, 184698], [167494, 153377, 525149, 283505, 330, 313361, 562176, 132780, 540634, 25439, 32973, 408112, 51211, 381151, 28764, 451020, 196374, 278824, 466442, 248624, 466718, 172745, 309023, 84034, 50992, 119985, 250498, 55500, 588253, 546372, 207449, 427017, 547941, 586026, 523564, 219187, 95441, 538208, 516108, 449155, 154113, 273472, 35810, 145016, 402273, 127021, 509122, 160859, 357961, 385121, 141928, 581456, 11569, 269380, 113776, 300768, 220682, 175072, 479276, 511651, 114327, 418630, 272820, 277835, 482813, 14255, 260499, 68486, 389091, 298777, 201125, 446708, 211094, 356743, 424008, 84804, 124875, 88967, 570466, 492371, 465680, 489573, 406490, 258412, 293310, 581265, 100655, 244484, 562685, 134270, 286654, 11295, 439929, 525967, 332757, 17448, 19880, 316494, 40729, 269812]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9YIIOZt3waV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_set, qid_rel, k):\n",
        "    max_sent_len = 200\n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "\n",
        "        ques, pos_ans, cands = seq[0], [seq[1]], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = vectorize(q_text, vocab, max_seq_len).to(device)\n",
        "\n",
        "        cands_text = [docid_to_text[c] if c is not 0 else \"\" for c in cands]\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        for cand in cands_text:\n",
        "            a_vec = vectorize(cand, vocab, max_seq_len).to(device)\n",
        "            scores.append(model(q_vec, a_vec).item())\n",
        "\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        qid_pred_rank[ques] = ranked_ans\n",
        "\n",
        "    MRR, average_ndcg, precision = evaluate(qid_ranked_docs, qid_rel, k)\n",
        "\n",
        "    return MRR, average_ndcg, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV5oqOPjRGQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b9853293-12d0-4f9a-c01a-9622f39dcf05"
      },
      "source": [
        "MRR, average_ndcg, precision = test(model, test_set, qid_rel, k=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:01<00:09,  1.01s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:02<00:08,  1.01s/it]\u001b[A\n",
            " 99%|█████████▉| 397531/400000 [00:29<00:00, 25630.97it/s]\n",
            " 40%|████      | 4/10 [00:04<00:06,  1.04s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:05<00:05,  1.06s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:06<00:04,  1.04s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:07<00:03,  1.03s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:08<00:02,  1.02s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:09<00:01,  1.01s/it]\u001b[A\n",
            "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh3vug3kqoKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "7344a97e-308b-44fa-b31d-752af42dda0e"
      },
      "source": [
        "num_q = len(qid_rel)\n",
        "k = 10\n",
        "\n",
        "print(\"Average nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average nDCG@10 for 6648 queries: 0.36504981770986045\n",
            "\n",
            "MRR@10 for 6648 queries: 0.3104752234828942\n",
            "\n",
            "Average Precision@1: 0.23901925391095066\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}