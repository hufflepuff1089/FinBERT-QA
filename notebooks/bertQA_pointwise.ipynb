{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertQA-pointwise.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OKxNW3iprhg7"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXHdHsEUxNd",
        "colab_type": "code",
        "outputId": "8f6d440b-ffd7-4d84-a9de-29abff39914c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hWS0v0sx7J",
        "colab_type": "code",
        "outputId": "9f05442c-0fed-4667-e1b7-b9faccd6a50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb81509ea90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3dEiY-MzxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhPBExB2bjG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGukeI_cCPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *\n",
        "from utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iCcUYUvb6-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary - key: qid, value: list of positive docid\n",
        "train_qid_rel = load_pickle(path + \"new-data/qid_rel_train.pickle\")\n",
        "test_qid_rel = load_pickle(path + \"new-data/qid_rel_test.pickle\")\n",
        "valid_qid_rel = load_pickle(path + \"new-data/qid_rel_valid.pickle\")\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list containing [qid, positive docid, negative docid]\n",
        "# train_set = load_pickle(path + 'new-data/data_50/train_set_50.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_50/valid_set_50.pickle')\n",
        "train_set = load_pickle(path + 'new-data/data_25/train_set_25.pickle')\n",
        "valid_set = load_pickle(path + 'new-data/data_25/valid_set_25.pickle')\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list contraining [qid, list of pos docid, list of candidate docid]\n",
        "# Contains candidates with all pos docids\n",
        "test_set = load_pickle(path + 'new-data/data_50/test_set_50.pickle')\n",
        "# Contains candidates retrieved by BM25\n",
        "# May be missing pos docids in candidates\n",
        "test_set_full = load_pickle(path + 'new-data/data_50/test_set_full_50.pickle')\n",
        "\n",
        "# Dictionary mapping docid and qid to raw text\n",
        "docid_to_text = load_pickle(path + 'new-data/docid_to_text.pickle')\n",
        "qid_to_text = load_pickle(path + 'new-data/qid_to_text.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_sM2Lrb794",
        "colab_type": "code",
        "outputId": "611b256f-cc71-4ae2-a325-f738a8973b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 142025\n",
            "Number of validation samples: 15800\n",
            "Number of test samples: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRSvaM8BEvH8",
        "colab_type": "code",
        "outputId": "69882a22-2618-422f-e7ae-ddf12cc8f4cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Example of the training set [qid, pos docid, neg docid]\n",
        "print(train_set[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 18850, 378523], [0, 18850, 403025], [0, 18850, 173088], [0, 18850, 142631], [0, 18850, 59638], [0, 18850, 592891], [0, 18850, 53244], [0, 18850, 481339], [0, 18850, 22916], [0, 18850, 8891]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npVgMDNewIW",
        "colab_type": "code",
        "outputId": "4c71ae3f-d039-4b40-8ff4-3d8656ddbaac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcnub5RAVxJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sequence_df(dataset):\n",
        "    \"\"\"\n",
        "    Converts training and validation data into a df with relevancy labels\n",
        "    and map the qid and docid to text.\n",
        "    \n",
        "    Returns data_df: df with columns qid, docid, label, question (text), answer (text)\n",
        "    ---------------\n",
        "    dataset: train or validation set in the form of list of lists\n",
        "    \"\"\"\n",
        "    # Load list into a dataframe\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df.rename(columns={0: 'qid', 1: 'pos', 2:'neg'})\n",
        "    # Construct new df with positive docids\n",
        "    df_pos = df[['qid', 'pos']]\n",
        "    df_pos = df_pos.rename(columns={'pos': 'docid'})\n",
        "    # Add new column and assign positive label\n",
        "    df_pos['label'] = df_pos.apply(lambda x: 1, axis=1)\n",
        "    df_pos = df_pos.drop_duplicates()\n",
        "\n",
        "    # Construct new df with negative docids\n",
        "    df_neg = df[['qid', 'neg']]\n",
        "    df_neg = df_neg.rename(columns={'neg': 'docid'})\n",
        "    # Add new column and assign negative label\n",
        "    df_neg['label'] = df_neg.apply(lambda x: 0, axis=1)\n",
        "\n",
        "    # Concatenate the positive and negative df\n",
        "    data_df = pd.concat([df_pos, df_neg]).sort_values(by=['qid'])\n",
        "\n",
        "    # Map id to text\n",
        "    data_df['question'] = data_df['qid'].apply(lambda x: qid_to_text[x])\n",
        "    data_df['ans_cand'] = data_df['docid'].apply(lambda x: docid_to_text[x])\n",
        "\n",
        "    return data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB4xGQLQwSXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input(questions, answers, max_seq_len):\n",
        "    \"\"\"\n",
        "    Returns input objects for training:\n",
        "        input_ids: List of lists\n",
        "                Each element contains a list of padded/clipped numericalized\n",
        "                tokens of the sequences including [CLS] and [SEP] tokens\n",
        "                e.g. [[101, 2054, 2003, 102, 2449, 1029, 102], ...]\n",
        "        token_type_ids: List of lists\n",
        "                Each element contains a list of segment token indices to \n",
        "                indicate first and second portions of the inputs. \n",
        "                0 corresponds to a question token, 1 corresponds an answer token\n",
        "                e.g. [[0, 0, 0, 0, 1, 1, 1], ...]\n",
        "        att_masks: List of lists\n",
        "                Each element contains a list of mask values\n",
        "                Mask to avoid performing attention on padding token indices. \n",
        "                1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n",
        "                e.g. [[1, 1, 1, 1, 1, 1, 1], ...]\n",
        "    -----------------\n",
        "    questions: List of strings\n",
        "            Each element contains a question string\n",
        "    answers: List of strings\n",
        "            Each element contains an asnwer string\n",
        "    max_seq_len: int\n",
        "            Maximum sequence length\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    token_type_ids = []\n",
        "    att_masks = []\n",
        "\n",
        "    for i in tqdm(range(len(questions))):\n",
        "        a = questions[i]\n",
        "        b = answers[i]\n",
        "\n",
        "        # Tokenize the questions and answers, apply padding, and trim the vectors\n",
        "        # to the max_seq_len\n",
        "        encoded_seq = tokenizer.encode_plus(a, b, \n",
        "                                            max_length=max_seq_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "        input_id = encoded_seq['input_ids']\n",
        "        token_type_id = encoded_seq['token_type_ids']\n",
        "        att_mask = encoded_seq['attention_mask']\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Input id dimension incorrect!\"\n",
        "        assert len(token_type_id) == max_seq_len, \"Token type id dimension incorrect!\"\n",
        "        assert len(att_mask) == max_seq_len, \"Attention mask dimension incorrect!\"\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        att_masks.append(att_mask)\n",
        "\n",
        "    return input_ids, token_type_ids, att_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_kGUNI4XK5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    # Get the column with the higher probability\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKxNW3iprhg7",
        "colab_type": "text"
      },
      "source": [
        "## **Pointwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMmcIxyY3MJ5",
        "colab_type": "code",
        "outputId": "6654fdb7-fe10-4754-f5bf-1705297767ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Create train/validation data\n",
        "trainset = get_sequence_df(train_set)\n",
        "train_questions = trainset.question.values\n",
        "train_answers = trainset.ans_cand.values\n",
        "train_labels = trainset.label.values\n",
        "\n",
        "train_input, train_type_id, train_att_mask = get_input(train_questions, train_answers, 256)\n",
        "\n",
        "validset = get_sequence_df(valid_set)\n",
        "valid_questions = validset.question.values\n",
        "valid_answers = validset.ans_cand.values\n",
        "valid_labels = validset.label.values\n",
        "\n",
        "valid_input, valid_type_id, valid_att_mask = get_input(valid_questions, valid_answers, 256)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_labels.pickle', train_labels)\n",
        "# save_pickle(path+'/pointwise-data/valid_labels.pickle', valid_labels)\n",
        "\n",
        "save_pickle(path+'/pointwise-data/train_input_256_large.pickle', train_input)\n",
        "save_pickle(path+'/pointwise-data/valid_input_256_large.pickle', valid_input)\n",
        "\n",
        "save_pickle(path+'/pointwise-data/train_type_id_256_large.pickle', train_type_id)\n",
        "save_pickle(path+'/pointwise-data/valid_type_id_256_large.pickle', valid_att_mask)\n",
        "\n",
        "save_pickle(path+'/pointwise-data/train_mask_256_large.pickle', train_att_mask)\n",
        "save_pickle(path+'/pointwise-data/valid_mask_256_large.pickle', valid_att_mask)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 298662/298662 [23:08<00:00, 215.10it/s]\n",
            "100%|██████████| 33195/33195 [02:00<00:00, 275.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRYz-N36gRKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train/validation data\n",
        "train_label = load_pickle(path+'/pointwise-data/train_labels.pickle')\n",
        "valid_label = load_pickle(path+'/pointwise-data/valid_labels.pickle')\n",
        "\n",
        "## bert-base tokenizer\n",
        "# train_input = load_pickle(path+'/pointwise-data/train_input_256.pickle')\n",
        "# valid_input = load_pickle(path+'/pointwise-data/valid_input_256.pickle')\n",
        "# train_type_id = load_pickle(path+'/pointwise-data/train_type_id_256.pickle')\n",
        "# valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_256.pickle')\n",
        "# train_att_mask = load_pickle(path+'/pointwise-data/train_mask_256.pickle')\n",
        "# valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_256.pickle')\n",
        "\n",
        "# bert-large tokenizer\n",
        "train_input = load_pickle(path+'/pointwise-data/train_input_256_large.pickle')\n",
        "valid_input = load_pickle(path+'/pointwise-data/valid_input_256_large.pickle')\n",
        "train_type_id = load_pickle(path+'/pointwise-data/train_type_id_256_large.pickle')\n",
        "valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_256_large.pickle')\n",
        "train_att_mask = load_pickle(path+'/pointwise-data/train_mask_256_large.pickle')\n",
        "valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_256_large.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_NnAZcnvby_",
        "colab_type": "code",
        "outputId": "962da956-1f42-4973-91a4-550f608db35e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(\"Number of training samples: {}\".format(len(train_input)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_input)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 298662\n",
            "Number of validation samples: 33195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwJauE6rFae-",
        "colab_type": "code",
        "outputId": "dcf9ce3e-9228-48a5-bb72-a4aa6b1fda0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "print(\"Example of the train_input\\n\")\n",
        "print(train_input[0])\n",
        "\n",
        "print(\"\\nExample of the train_type_id\\n\")\n",
        "print(train_type_id[0])\n",
        "\n",
        "print(\"\\nExample of the train_att_mask\\n\")\n",
        "print(train_att_mask[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of the train_input\n",
            "\n",
            "[101, 2054, 2003, 2641, 1037, 2449, 10961, 2006, 1037, 2449, 4440, 1029, 102, 1996, 25760, 8606, 20246, 2000, 1996, 3395, 1012, 1999, 2236, 1996, 2190, 1045, 2064, 2360, 2003, 2115, 2449, 10961, 2089, 2022, 2139, 8566, 6593, 7028, 1012, 2021, 2009, 9041, 2006, 1996, 6214, 1998, 2054, 2009, 2003, 2017, 2215, 2000, 2139, 8566, 6593, 1012, 3604, 26457, 2040, 3604, 2185, 2013, 2188, 2006, 2449, 2089, 2139, 8566, 6593, 3141, 11727, 1010, 2164, 1996, 3465, 1997, 4285, 2037, 7688, 1010, 1996, 3465, 1997, 26859, 1998, 12278, 1998, 2060, 6623, 1998, 4072, 11727, 1012, 26457, 2024, 2641, 1523, 7118, 2185, 2013, 2188, 1524, 2065, 2037, 5704, 5478, 2068, 2000, 2022, 2185, 2013, 2188, 12381, 2936, 2084, 2019, 6623, 2154, 1521, 1055, 2147, 1998, 2027, 2342, 2000, 3637, 2030, 2717, 2000, 3113, 1996, 7670, 1997, 2037, 2147, 1012, 1996, 5025, 3465, 1997, 12278, 1998, 5043, 2389, 11727, 2089, 2022, 2139, 29510, 2030, 1996, 26980, 2089, 2224, 1037, 3115, 7954, 21447, 1998, 4359, 2501, 4363, 5918, 1012, 7539, 1997, 1996, 4118, 2109, 1010, 7954, 2139, 16256, 2015, 2024, 3227, 3132, 2000, 2753, 3867, 2004, 3090, 3041, 1012, 2069, 5025, 5366, 2005, 26859, 2089, 2022, 3555, 2004, 2019, 10961, 1998, 28258, 2442, 2022, 2921, 2005, 12653, 1012, 11727, 2442, 2022, 9608, 1998, 6413, 1025, 2139, 16256, 2015, 2005, 27856, 11727, 2024, 2025, 3499, 3085, 1012, 2062, 2592, 2003, 2800, 1999, 4772, 4805, 2509, 1010, 3604, 1010, 4024, 1010, 5592, 1010, 1998, 2482, 11727, 1012, 4024, 11727, 2005, 14036, 7846, 1010, 6304, 2030, 5126, 2089, 2022, 2139, 29510, 2065, 2027, 102]\n",
            "\n",
            "Example of the train_type_id\n",
            "\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Example of the train_att_mask\n",
            "\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAk07EggdjRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_labels = train_labels[:10000]\n",
        "# train_input = train_input[:10000]\n",
        "# train_type_id = train_type_id[:10000]\n",
        "# train_att_mask = train_att_mask[:10000]\n",
        "\n",
        "# valid_labels = valid_labels[:1000]\n",
        "# valid_input = valid_input[:1000]\n",
        "# valid_type_id = valid_type_id[:1000]\n",
        "# valid_att_mask = valid_att_mask[:1000]\n",
        "\n",
        "# print(len(train_input))\n",
        "# print(len(valid_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy1wj92aI4hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors\n",
        "train_labels = torch.tensor(train_label)\n",
        "validation_labels = torch.tensor(valid_label)\n",
        "\n",
        "train_inputs = torch.tensor(train_input)\n",
        "validation_inputs = torch.tensor(valid_input)\n",
        "\n",
        "train_type_ids = torch.tensor(train_type_id)\n",
        "validation_type_ids = torch.tensor(valid_type_id)\n",
        "\n",
        "train_masks = torch.tensor(train_att_mask)\n",
        "validation_masks = torch.tensor(valid_att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P4Lb1dzK3Ko",
        "colab_type": "code",
        "outputId": "415a917f-c76c-44d5-e251-7ac17a380d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Create DataLoader to train in bacthes\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_type_ids, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_type_ids, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "print(\"Size of the DataLoader for the training set: {}\".format(len(train_dataloader)))\n",
        "print(\"Size of the DataLoader for the validation set: {}\".format(len(validation_dataloader)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the DataLoader for the training set: 37333\n",
            "Size of the DataLoader for the validation set: 4150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDc45R4mjGDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "    # Store the average loss after each epoch for plotting\n",
        "    loss_values = []\n",
        "\n",
        "    # Reset the total loss each epoch\n",
        "    total_loss = 0\n",
        "    train_accuracy = 0\n",
        "    # Track the number of batches\n",
        "    num_steps = 0\n",
        "\n",
        "    # Set model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # batch contains four PyTorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: token_type_ids\n",
        "        #   [2]: attention masks\n",
        "        #   [3]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_token_type_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Forward pass\n",
        "        # The model will return the loss and the logits\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids = b_token_type_ids, \n",
        "                    attention_mask = b_input_mask, \n",
        "                    labels = b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        tmp_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        train_accuracy += tmp_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        num_steps += 1\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    acc = _accuracy/nb_eval_steps\n",
        "\n",
        "    return avg_train_loss, acc, loss_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNhQobcGke2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, validation_dataloader):\n",
        "\n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    eval_accuracy = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # For each batch of the validation data\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from the dataloader\n",
        "        b_input_ids, b_token_type_ids, b_input_masks, b_labels = batch\n",
        "        \n",
        "        # Don't to compute or store gradients\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids = b_token_type_ids, \n",
        "                            attention_mask = b_input_masks,\n",
        "                            labels= b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "    avg_loss = total_loss / len(validation_dataloader) \n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxFYsHtADVla"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_zn-HiYolL",
        "colab_type": "code",
        "outputId": "23ef346a-ecfc-4408-aef7-b7702e8efa21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top\n",
        "\n",
        "# model_path = \"/content/drive/My Drive/FiQA/model/fin_model\"\n",
        "# model = BertForSequenceClassification.from_pretrained(model_path, cache_dir=None, num_labels=2)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=None, num_labels=2)\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", cache_dir=None, num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7wlYPSlZcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-6, eps = 1e-8)\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3lU_QLEl4lY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss, train_acc, loss_values = train(model, train_dataloader, optimizer, scheduler)\n",
        "    # Evaluate validation loss\n",
        "    valid_loss, valid_acc = validate(model, validation_dataloader)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), path + 'model/' + str(epoch+1)+'_pointwise_25.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHBg5FXKOYHU"
      },
      "source": [
        "## **Evalulation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGbsz18-Urm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rank(model, test_set, qid_rel, max_seq_len):\n",
        "    \"\"\"\n",
        "    Returns a dictionary - key: qid, value: list of ranked candidates\n",
        "    -------------------\n",
        "    model - PyTorch model\n",
        "    test_set - List of lists:\n",
        "            Each element is a list contraining \n",
        "            [qid, list of pos docid, list of candidate docid]\n",
        "    qid_rel: Dictionary\n",
        "            key: qid, value: list of relevant answer id\n",
        "    max_seq_len: int\n",
        "            Maximum sequence length\n",
        "    \"\"\"\n",
        "\n",
        "    # Initiate empty dictionary\n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # For each element in the test set\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "        \n",
        "        # question id, list of rel answers, list of candidates\n",
        "        qid, label, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        # Convert list to numpy array\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        # Empty list for the probability scores of relevancy\n",
        "        scores = []\n",
        "\n",
        "        # For each answer in the candidates\n",
        "        for docid in cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "\n",
        "            # Create inputs for the model\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text, \n",
        "                                            max_length=max_seq_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "            # Numericalized, padded, clipped seq with special tokens\n",
        "            input_ids = torch.tensor([encoded_seq['input_ids']]).to(device)\n",
        "            # Specify question seq and answer seq\n",
        "            token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(device)\n",
        "            # Sepecify which position is part of the seq which is padded\n",
        "            att_mask = torch.tensor([encoded_seq['attention_mask']]).to(device)\n",
        "\n",
        "            # Don't calculate gradients\n",
        "            with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions for each QA pair\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=att_mask)\n",
        "\n",
        "            # Get the predictions\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Apply activation function\n",
        "            pred = softmax(logits, dim=1)\n",
        "            # pred = torch.sigmoid(logits)\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "\n",
        "            # Append relevant scores to list (where label = 1)\n",
        "            scores.append(pred[:,1][0])\n",
        "\n",
        "        print(scores)\n",
        "\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        # Get the list of docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[qid] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOR-OM-qbb3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 2))\n",
        "toy_test = test_set[:2]\n",
        "# toy_test = [[14, [398960], [84963, 14255, 398960]],\n",
        "#             [68, [19183], [107584, 562777, 19183]],\n",
        "#             [70, [327002], [107584, 327002, 19183]]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa0KvT4cbbo",
        "colab_type": "code",
        "outputId": "74e67074-6efa-4fcf-c93b-005a158dfd3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "model.load_state_dict(torch.load(path+'model/1_pairwise_25.pt'))\n",
        "\n",
        "# qid_pred_rank = get_rank(model, test_set, test_qid_rel, max_seq_len=512)\n",
        "qid_pred_rank = get_rank(model, toy_test, toy_test_label, max_seq_len=256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:07<00:07,  7.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.9999527, 0.999949, 0.18187124, 0.999884, 0.000103418584, 0.99862635, 0.9999441, 0.99994326, 0.9999306, 0.999944, 0.9999107, 0.011065468, 0.99990654, 0.9999043, 0.9996063, 0.99993455, 0.999925, 0.9999478, 0.9997577, 0.9284304, 0.124180555, 0.9999361, 0.9197168, 0.9999304, 0.999887, 0.99994993, 0.9999486, 0.00038539234, 0.999936, 0.9999198, 0.98532575, 0.9999027, 0.9998845, 0.99889565, 0.99994695, 0.89547914, 0.9347405, 0.99992275, 0.004050158, 0.0004175653, 0.70191604, 0.9997336, 0.61686075, 0.9996581, 0.99994326, 0.992488, 0.9998895, 0.99988997, 0.0010666391, 0.8459604, 0.9999151, 0.9999553, 0.99987435, 0.99994063, 0.9998692, 0.9999106, 0.77538884, 0.99957675, 0.9996093, 0.9997527, 0.99994993, 0.9999403, 0.8784555, 0.99993014, 0.99838126, 0.9999491, 0.9998518, 0.9999471, 0.9999548, 0.999882, 0.99994826, 0.99995434, 0.99989235, 0.7867769, 0.99981743, 0.74969816, 0.9998274, 0.9997148, 0.00039994254, 0.99994624, 0.087119706, 0.9999229, 0.99973744, 0.99985087, 0.9978452, 0.99994147, 0.9999567, 0.99995446, 0.3475442, 0.011162113, 0.99994624, 0.99995387, 0.99993813, 0.99994767, 0.99995327, 0.99994874, 0.00017699118, 0.99989116, 0.016093124, 0.9999008, 0.9999318, 0.00049407786, 0.9999527, 0.9999449, 0.9996556, 0.99983716, 0.99986494, 0.9975672, 0.999941, 0.99005985, 0.18401062, 0.99994326, 0.9980015, 0.9998871, 0.9998574, 0.99993575, 0.9986897, 0.9999249, 0.9999485, 0.9999355, 0.9997359, 0.99984944, 0.9998853, 0.99994767, 0.027540471, 0.9999496, 0.9999428, 0.0062411474, 0.99810624, 0.9999138, 0.99985135, 0.9999485, 0.9944001, 0.9902857, 0.9999536, 0.9999492, 0.9993875, 0.99985325, 0.99992776, 0.9998654, 0.9999423, 0.99989057, 0.9999386, 0.015173371, 0.999948, 0.99982893, 0.17032662, 0.99995434, 0.99993205, 0.0018434654, 0.999895, 0.9994753, 0.99990904, 0.9999176, 0.999912, 0.9999566, 0.9999424, 0.9998024, 0.99776256, 0.99987614, 0.8275018, 0.0009889285, 0.99995875, 0.009319318, 0.00028031893, 0.9999293, 0.9999342, 0.9999312, 0.99992895, 0.99993265, 0.9996897, 0.99992347, 0.99995065, 0.9998603, 0.3786432, 0.9993813, 0.25336707, 0.9998099, 0.014688242, 0.9985942, 0.9999542, 0.9999542, 0.9999472, 0.022485012, 0.00012552446, 0.03372591, 0.9999417, 0.99994004, 0.9999496, 0.99898344, 0.99995446, 0.9998878, 0.99994624, 0.9990663, 0.9999398, 0.8716433, 0.9999192, 0.9994031, 0.9999435, 0.9972427, 0.99993575, 0.013907222, 0.999949, 0.99994445, 0.9999393, 0.9999417, 0.98887306, 0.0022712164, 0.9999548, 0.99956363, 0.9998759, 0.99740976, 0.9999056, 0.9999441, 0.99992514, 0.99994004, 0.0041667507, 0.9999466, 0.99989283, 0.9998449, 0.9999473, 0.9999479, 0.9999361, 0.99994373, 0.999951, 0.9999492, 0.999928, 0.9999114, 0.9993136, 0.9999167, 0.99994147, 0.9999398, 0.9999479, 0.00053685583, 0.99995315, 0.99992406, 0.99993765, 0.99994373, 0.9999522, 0.02693599, 0.9999324, 0.99994695, 0.99995327, 0.6568741, 0.9999479, 0.99993694, 0.9999291, 0.99978, 0.9999385, 0.99988484, 0.9998721, 0.6906214, 0.18763976, 0.99995494, 0.99993515, 0.9846363, 0.29567248, 0.9999591, 0.99960643, 0.00019369536, 0.99994755, 0.00022218659, 0.00017739246, 0.99995637, 0.9995105, 0.4746182, 0.9999169, 0.99993384, 0.99992144, 0.99993694, 0.9673961, 0.9999119, 0.9999467, 0.951224, 0.5392592, 0.9999517, 0.42235917, 0.99952865, 0.20740855, 0.9999107, 0.9967524, 0.9999422, 0.89208066, 0.9993143, 0.9999354, 0.99988127, 0.99985313, 0.9998691, 0.9998375, 0.9999554, 0.99993074, 0.0066825054, 0.999918, 0.9572586, 0.999861, 0.9999349, 0.9997602, 0.99993336, 0.99993813, 0.99991465, 0.9999131, 0.9993851, 0.024105452, 0.99974746, 0.9998603, 0.5560292, 0.999954, 0.9999461, 0.99995625, 0.9999536, 0.9999547, 0.9999367, 0.99995184, 0.9999585, 0.99587005, 0.30289543, 0.9999498, 0.9995384, 0.9997918, 0.9998723, 0.99380255, 0.99962723, 0.9999447, 0.005616155, 0.99982077, 0.07931997, 0.9998528, 0.99989283, 0.9999558, 0.9999473, 0.9999546, 0.9999571, 0.999954, 0.99763, 0.9991522, 0.99994516, 0.99991536, 0.9999467, 0.9997539, 0.99988747, 0.9999492, 0.9999466, 0.18748252, 0.9999337, 0.99994195, 0.9911942, 0.9999306, 0.99995613, 0.9999131, 0.9994217, 0.9999372, 0.9993955, 0.99994457, 0.99987924, 0.9976736, 0.000103669576, 0.99989116, 0.9999354, 0.999944, 0.99994004, 0.999941, 0.99994326, 0.9999536, 0.99986565, 0.99994516, 0.99993634, 0.9999474, 0.007200234, 0.99989676, 0.9715897, 0.9999536, 0.99994826, 0.99994683, 0.9998982, 0.9999306, 0.999918, 0.5604524, 0.9998903, 0.99984777, 0.9990293, 0.99995327, 0.9999566, 0.99961615, 0.9999529, 0.9999527, 0.5317129, 0.99992764, 0.9998934, 0.9999528, 0.004281145, 0.99995136, 0.9999063, 0.032023247, 0.9995689, 0.9999559, 0.9998319, 0.99993396, 0.99995244, 0.9999509, 0.999819, 0.9994667, 0.99994373, 0.9999566, 0.99993753, 0.9996741, 0.0009729735, 0.99994826, 0.99994195, 0.87127954, 0.9999554, 0.9998698, 0.9999274, 0.008103746, 0.9999225, 0.0023392676, 0.999956, 0.9950093, 0.9995968, 0.99985564, 0.18005766, 0.99995387, 0.00021571707, 0.9998616, 0.99995327, 0.03440732, 0.9999453, 0.004972663, 0.00013354399, 0.99993026, 0.9999472, 0.99994326, 0.9998454, 0.9999212, 0.89780915, 0.99995065, 0.99744177, 0.99994504, 0.010077396, 0.9998449, 0.014185263, 0.9999517, 0.99942845, 0.9998652, 0.9999254, 0.72560793, 0.99962544, 0.99986136, 0.45438176, 0.0017595548, 0.27680638, 0.9999517, 0.9998981, 0.9999356, 0.9997354, 0.0013328773, 0.99993575, 0.9999416, 0.99995315, 0.9999403, 0.9999523, 0.9999449, 0.99995375, 0.99992764, 0.9999577, 0.99994063, 0.999749, 0.05377649, 0.99992406, 0.0014594522, 0.93989396, 0.98956245, 0.999881, 0.8421778, 0.9999479, 0.9999459, 0.9992349, 0.9999336, 0.99994516, 0.99953306, 0.999948, 0.7785077, 0.99995136, 0.99994504, 0.99993134, 0.99994874, 0.35641298, 0.99995136, 0.9999293, 0.99698585, 0.9999496, 0.9999535, 0.9998964, 0.99969745, 0.040776797, 0.99957055, 0.99747914, 0.9999075, 0.99995494, 0.9937273, 0.99993443]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 2/2 [00:15<00:00,  7.49s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.9999287, 0.0072370716, 0.99994373, 0.9999317, 0.9999299, 0.99995506, 0.99957365, 0.9999552, 0.00057716575, 0.9998795, 0.9999534, 0.99996006, 0.9999405, 0.9999261, 0.99993515, 0.7964893, 0.99969685, 0.9999058, 0.9999336, 0.8458747, 0.9999441, 0.9999472, 0.99995685, 0.99995244, 0.9999547, 0.99401265, 0.9999547, 0.99992085, 0.9999304, 0.9998442, 0.99995935, 0.99789006, 0.99994314, 0.999928, 0.9999423, 0.99995923, 0.99995387, 0.9999374, 0.9999492, 0.9999448, 0.999943, 0.99992156, 0.99994683, 0.9996908, 0.99995124, 0.9999485, 0.9999329, 0.00013012807, 0.99995685, 0.0009288644, 0.99994457, 0.9998565, 0.022961553, 0.99992335, 0.99994254, 0.9998714, 0.9999542, 0.9999484, 0.9996592, 0.99994934, 0.9999589, 0.99993706, 0.9998497, 0.99993575, 0.9999453, 0.99995613, 0.9999505, 0.9998099, 0.011141944, 0.99994516, 0.9999552, 0.9999478, 0.99994814, 0.9999567, 0.99995184, 0.99995375, 0.9999583, 0.00016959306, 0.9999527, 0.9999472, 0.9999584, 0.9999287, 0.99984765, 0.9999304, 0.00031214024, 0.9999368, 0.9997166, 0.999962, 0.9999609, 0.9964545, 0.9999554, 0.999954, 0.9999422, 0.99995005, 0.9998473, 0.9999571, 0.9999081, 0.9999417, 0.9999341, 0.9991947, 0.99995506, 0.99995875, 0.00054983335, 0.99945396, 0.99920964, 0.99994147, 0.99995744, 0.99995697, 0.9695939, 0.99993753, 0.99981076, 0.9999602, 0.9999548, 0.99989724, 0.9999529, 0.9999455, 0.99995446, 0.9999478, 0.9999577, 0.99995327, 0.99969625, 0.99995875, 0.9999503, 0.99995494, 0.99986744, 0.99988616, 0.9999577, 0.99995434, 0.99995136, 0.9999541, 0.99988925, 0.9999541, 0.9999331, 0.9999571, 0.9999423, 0.99995315, 0.99995065, 0.6186225, 0.9999492, 0.99993825, 0.9999366, 0.00050550397, 0.9999498, 0.99995613, 0.99994874, 0.9999552, 0.9999348, 0.9999484, 0.999918, 0.9999404, 0.9999368, 0.9999428, 0.99995744, 0.9999509, 0.99995816, 0.99836737, 0.9999342, 0.99994195, 0.4308989, 0.9999449, 0.9999455, 0.99995196, 0.0044896863, 0.9999498, 0.9998845, 0.9999509, 0.99994695, 0.99995995, 0.99983335, 0.99995124, 0.99994385, 0.99929214, 0.99632144, 0.99995303, 0.999788, 0.9999056, 0.99995553, 0.99994993, 0.9999553, 0.99995804, 0.99924695, 0.999959, 0.1295024, 0.99995637, 0.99996006, 0.99993455, 0.9959189, 0.99995935, 0.9999249, 0.99995875, 0.9998727, 0.9999466, 0.99995065, 0.9995265, 0.99994266, 0.9999479, 0.99996126, 0.9999496, 0.0943796, 0.999943, 0.02653428, 0.9999347, 0.9999294, 0.0010781545, 0.9954803, 0.99993527, 0.9999559, 0.9999393, 0.9801108, 0.9999511, 0.99995923, 0.9999491, 0.9999101, 0.9998698, 0.99995744, 0.999948, 0.9999591, 0.99995816, 0.99995434, 0.99994946, 0.99995935, 0.9999181, 0.9999465, 0.9999167, 0.9999504, 0.99993885, 0.99933475, 0.99995685, 0.9999484, 0.99993837, 0.9999567, 0.9999577, 0.99994683, 0.999678, 0.9991768, 0.99995637, 0.99995315, 0.9999542, 0.9999498, 0.9998908, 0.999961, 0.9999567, 0.99993694, 0.999948, 0.9999567, 0.9999435, 0.99994075, 0.9965064, 0.9999497, 0.9999399, 0.9999547, 0.9999535, 0.99993336, 0.99994516, 0.999956, 0.99994814, 0.99995077, 0.9999591, 0.99994385, 0.999959, 0.9999491, 0.99991393, 0.99995863, 0.9999566, 0.9999559, 0.9999573, 0.99995434, 0.99995315, 0.99995816, 0.9999542, 0.99992657, 0.99996054, 0.9999423, 0.99994683, 0.9999473, 0.99993265, 0.99995863, 0.9999447, 0.99992645, 0.99989116, 0.9955265, 0.9999441, 0.9998646, 0.9999478, 0.99995065, 0.9999474, 0.9999472, 0.99990034, 0.9997894, 0.9999459, 0.00043835904, 0.9999478, 0.9999552, 0.9997185, 0.9999534, 0.9999316, 0.9999516, 0.9998381, 0.99940956, 0.9999571, 0.99994636, 0.9999567, 0.9999262, 0.9999486, 0.9999455, 0.9999472, 0.99992895, 0.99966204, 0.9999268, 0.99994695, 0.99995136, 0.9999535, 0.99993265, 0.9999578, 0.9999194, 0.99994826, 0.0003821713, 0.99991703, 0.9999577, 0.97868335, 0.9999496, 0.9999138, 0.99995995, 0.00011005535, 0.99994445, 0.9999063, 0.9999447, 0.9998913, 0.9998833, 0.9999435, 0.9999529, 0.999933, 0.99988544, 0.99990547, 0.99986565, 0.9999244, 0.99995136, 0.9999181, 0.9999051, 0.9999591, 0.9999559, 0.99798524, 0.9999534, 0.99990344, 0.010515641, 0.12829946, 0.99992657, 0.9999316, 0.99995613, 0.9999536, 0.99995255, 0.99993753, 0.99995077, 0.99994206, 0.99994826, 0.99996006, 0.9997167, 0.00018124709, 0.994429, 0.99901485, 0.9998078, 0.9999522, 0.9999596, 0.9999424, 0.99954164, 0.9999528, 0.99994504, 0.9999529, 0.99948776, 0.9999409, 0.9998603, 0.99992657, 0.99993026, 0.99995685, 0.9999467, 0.99993837, 0.00022353312, 0.999956, 0.99994624, 0.99992085, 0.99994683, 0.9999467, 0.99985623, 0.99995387, 0.999928, 0.99995744, 0.9999095, 0.99994814, 0.99994206, 0.9999237, 0.99995637, 0.9999578, 0.9999486, 0.99993765, 0.9999554, 0.99995756, 0.999956, 0.99995244, 0.9999567, 0.9999579, 0.99994826, 0.026717095, 0.99996006, 0.9999565, 0.9998155, 0.9999393, 0.99995553, 0.9999596, 0.99995565, 0.9999615, 0.99995375, 0.022707924, 0.99993956, 0.99995863, 0.90225667, 0.9999471, 0.9999584, 0.9672542, 0.99995196, 0.99994874, 0.99995685, 0.99995375, 0.0039665424, 0.9999579, 0.01787288, 0.99994886, 0.99995255, 0.9999329, 0.9999558, 0.99995613, 0.9999542, 0.9999548, 0.9999509, 0.9999571, 0.99995553, 0.9999577, 0.99994767, 0.99995005, 0.9999529, 0.9999399, 0.9999198, 0.99993277, 0.00049240876, 0.9999474, 0.99995863, 0.9999558, 0.99993443, 0.99994946, 0.13373764, 0.9996989, 0.9999366, 0.999933, 0.9999416, 0.9999504, 0.9999467, 0.99995375, 0.99982566, 0.00014307762, 0.998466, 0.99990964, 0.9999497, 0.9999423, 0.9998921, 0.99995494, 0.9999602, 0.99990237, 0.99995875, 0.9999529, 0.9999496, 0.9999502, 0.99959606, 0.9999372, 0.9999546, 0.9999472, 0.99992406, 0.9999554, 0.9996315, 0.99993443, 0.999956, 0.9998332, 0.9999441, 0.9999584, 0.99995923, 0.99995327, 0.99975616, 0.9999417, 0.99994683, 0.9999455, 0.99991775, 0.9999002, 0.0024171418, 0.99991655, 0.9999331, 0.9999535, 0.99995697, 0.99988747, 0.004801053, 0.99995685, 0.99780124, 0.00042007858]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTc2SQVhuv2Q",
        "colab_type": "code",
        "outputId": "56742277-2f16-417b-f5d6-01d1b24ead89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "k = 10\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "# MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, test_qid_rel, k)\n",
        "MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, toy_test_label, k)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 333 queries: 0.4510568402073561\n",
            "\n",
            "MRR@10 for 333 queries: 0.44166666666666665\n",
            "\n",
            "Average Precision@1: 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcfbXb5eBcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'rank/rank_bert_256_full.pickle', qid_pred_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}