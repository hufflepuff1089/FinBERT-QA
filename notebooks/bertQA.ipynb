{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertQA_50.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x5arM3DfYyTO"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXHdHsEUxNd",
        "colab_type": "code",
        "outputId": "adf9e196-4ccb-41eb-d213-eb1fb748fe77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hWS0v0sx7J",
        "colab_type": "code",
        "outputId": "09643ee8-bfa1-4068-b8b7-4d5df1ef5552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f05e169b470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3dEiY-MzxU",
        "colab_type": "code",
        "outputId": "9c367f5a-88bb-4b2d-a428-72ea4374369b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, BertConfig"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhPBExB2bjG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/fiqa/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGukeI_cCPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *\n",
        "from utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diWEgRaLGoyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_cands(cands_path):\n",
        "\n",
        "    qid_ranked_docs = {}\n",
        "\n",
        "    with open(cands_path,'r') as f:\n",
        "        for line in f:\n",
        "            \n",
        "            line = line.strip().split('\\t')\n",
        "            qid = int(line[0])\n",
        "            doc_id = int(line[1])\n",
        "            rank = int(line[2])\n",
        "\n",
        "            if qid not in qid_ranked_docs:\n",
        "                # Create a list for each query to store the candidates\n",
        "                candidates = [0]*50\n",
        "                qid_ranked_docs[qid] = candidates\n",
        "            qid_ranked_docs[qid][rank-1] = doc_id\n",
        "\n",
        "    return qid_ranked_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9kxwJskHG1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(qid_rel, cands):\n",
        "    test_set = []\n",
        "\n",
        "    for qid, docid in qid_rel.items():\n",
        "        \n",
        "        for ques, cand in cands.items():\n",
        "            if 0 not in cand:\n",
        "                cand_ans = cand\n",
        "                if ques == qid:\n",
        "                    tmp = []\n",
        "                    tmp.append(qid)\n",
        "                    tmp.append(docid)\n",
        "                    tmp.append(cand_ans)\n",
        "                    test_set.append(tmp)\n",
        "\n",
        "    for row in test_set:\n",
        "        assert len(row[2]) == 50, \"Dataset size is incorrect!\"\n",
        "    \n",
        "    return test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iCcUYUvb6-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary - key: qid, value: list of positive docid\n",
        "train_qid_rel = load_pickle(path + \"data/qid_rel_train.pickle\")\n",
        "test_qid_rel = load_pickle(path + \"data/qid_rel_test.pickle\")\n",
        "valid_qid_rel = load_pickle(path + \"data/qid_rel_valid.pickle\")\n",
        "\n",
        "# Dictionary mapping docid and qid to raw text\n",
        "docid_to_text = load_pickle(path + 'data/docid_to_text.pickle')\n",
        "qid_to_text = load_pickle(path + 'data/qid_to_text.pickle')\n",
        "\n",
        "# train_cands = load_cands(path + 'data/cands_train_100.tsv')\n",
        "# valid_cands = load_cands(path + 'data/cands_valid_100.tsv')\n",
        "# test_cands = load_cands(path + 'data/cands_test_100.tsv')\n",
        "\n",
        "# train_cands = load_cands(path + 'data/cands_train_50.tsv')\n",
        "# valid_cands = load_cands(path + 'data/cands_valid_50.tsv')\n",
        "# test_cands = load_cands(path + 'data/cands_test_50.tsv')\n",
        "\n",
        "# train_set = create_dataset(train_qid_rel, train_cands)\n",
        "# valid_set = create_dataset(valid_qid_rel, valid_cands)\n",
        "# test_set = create_dataset(test_qid_rel, test_cands)\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list contraining [qid, list of pos docid, list of candidate docid]\n",
        "# Contains candidates with all pos docids\n",
        "train_set = load_pickle(path + 'data/train_set_50.pickle')\n",
        "valid_set = load_pickle(path + 'data/valid_set_50.pickle')\n",
        "test_set = load_pickle(path + 'data/test_set_50.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6lI80kbTo-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_pickle(path+'data/test_set_50.pickle', test_set)\n",
        "# save_pickle(path+'data/valid_set_50.pickle', valid_set)\n",
        "save_pickle(path+'data/train_set_50.pickle', train_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdWsesbTVMt0",
        "colab_type": "code",
        "outputId": "b5f67e69-701d-4d25-e937-04c7f3d8a32e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 5676\n",
            "Number of validation samples: 631\n",
            "Number of test samples: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npVgMDNewIW",
        "colab_type": "code",
        "outputId": "6a087268-c595-46f0-85c3-61334d16ebe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRSvaM8BEvH8",
        "colab_type": "code",
        "outputId": "01fe8dfb-ffd0-426a-92ef-4397e75a7250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Example of the training set [qid, pos docid, neg docid]\n",
        "print(train_set[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, [18850], [531578, 417981, 324911, 524879, 397608, 216077, 173212, 434846, 104464, 326261, 528838, 234436, 571062, 196374, 481692, 207449, 338700, 153377, 406418, 327002, 421301, 11538, 375748, 238271, 322893, 130631, 483385, 73427, 560087, 531442, 156554, 541809, 562777, 192843, 553328, 283505, 209224, 351672, 324513, 18850, 55200, 540395, 297841, 367754, 455984, 160340, 577284, 287474, 565935, 354716]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x5arM3DfYyTO"
      },
      "source": [
        "## **Pairwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1x8kg7BBwYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pairwise_input_data(dataset, max_seq_len):\n",
        "    pos_input_ids = []\n",
        "    neg_input_ids = []\n",
        "\n",
        "    pos_type_ids = []\n",
        "    neg_type_ids = []\n",
        "    \n",
        "    pos_masks = []\n",
        "    neg_masks = []\n",
        "\n",
        "    pos_labels = []\n",
        "    neg_labels = []\n",
        "\n",
        "    for i, seq in enumerate(tqdm(dataset)):\n",
        "        qid, ans_labels, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        filtered_cands = list(set(cands)-set(ans_labels))\n",
        "\n",
        "        pos_docid = random.choice(ans_labels)\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        for neg_docid in filtered_cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            pos_ans_text = docid_to_text[pos_docid]\n",
        "            neg_ans_text = docid_to_text[neg_docid]\n",
        "\n",
        "            pos_encoded_seq = tokenizer.encode_plus(q_text, pos_ans_text, \n",
        "                                                max_length=max_seq_len, \n",
        "                                                pad_to_max_length=True, \n",
        "                                                return_token_type_ids=True,\n",
        "                                                return_attention_mask = True)\n",
        "            \n",
        "            neg_encoded_seq = tokenizer.encode_plus(q_text, neg_ans_text, \n",
        "                                                max_length=max_seq_len, \n",
        "                                                pad_to_max_length=True, \n",
        "                                                return_token_type_ids=True,\n",
        "                                                return_attention_mask = True)\n",
        "\n",
        "            pos_input_id = pos_encoded_seq['input_ids']\n",
        "            pos_type_id = pos_encoded_seq['token_type_ids']\n",
        "            pos_mask = pos_encoded_seq['attention_mask']\n",
        "\n",
        "            neg_input_id = neg_encoded_seq['input_ids']\n",
        "            neg_type_id = neg_encoded_seq['token_type_ids']\n",
        "            neg_mask = neg_encoded_seq['attention_mask']\n",
        "\n",
        "            pos_input_ids.append(pos_input_id)\n",
        "            pos_type_ids.append(pos_type_id)\n",
        "            pos_masks.append(pos_mask)\n",
        "            pos_labels.append(1)\n",
        "\n",
        "            neg_input_ids.append(neg_input_id)\n",
        "            neg_type_ids.append(neg_type_id)\n",
        "            neg_masks.append(neg_mask)\n",
        "            neg_labels.append(0)\n",
        "\n",
        "    return pos_input_ids, pos_type_ids, pos_masks, pos_labels, neg_input_ids, neg_type_ids, neg_masks, neg_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcYbXyudDgQn",
        "colab_type": "code",
        "outputId": "855f4104-f7d6-4110-ea2d-52718dd29c30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_pos_input, train_pos_type_id, train_pos_att_mask, train_pos_labels, \\\n",
        "train_neg_input, train_neg_type_id, train_neg_att_mask, train_neg_labels  = get_pairwise_input_data(train_set, 128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5676/5676 [37:18<00:00,  2.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8j7FG2OHRlh",
        "colab_type": "code",
        "outputId": "329a68d0-2163-4cd0-e9a2-cd46cff692d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "valid_pos_input, valid_pos_type_id, valid_pos_att_mask, valid_pos_labels, \\\n",
        "valid_neg_input, valid_neg_type_id, valid_neg_att_mask, valid_neg_labels = get_pairwise_input_data(valid_set, 128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 631/631 [04:01<00:00,  2.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHGawog0Z461",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'pairwise-data/train_pos_labels_50.pickle', train_pos_labels)\n",
        "save_pickle(path+'pairwise-data/train_neg_labels_50.pickle', train_neg_labels)\n",
        "save_pickle(path+'pairwise-data/valid_pos_labels_50.pickle', valid_pos_labels)\n",
        "save_pickle(path+'pairwise-data/valid_neg_labels_50.pickle', valid_neg_labels)\n",
        "\n",
        "save_pickle(path+'pairwise-data/train_pos_input_128_50.pickle', train_pos_input)\n",
        "save_pickle(path+'pairwise-data/train_neg_input_128_50.pickle', train_neg_input)\n",
        "save_pickle(path+'pairwise-data/valid_pos_input_128_50.pickle', valid_pos_input)\n",
        "save_pickle(path+'pairwise-data/valid_neg_input_128_50.pickle', valid_neg_input)\n",
        "\n",
        "save_pickle(path+'pairwise-data/train_pos_type_id_128_50.pickle', train_pos_type_id)\n",
        "save_pickle(path+'pairwise-data/train_neg_type_id_128_50.pickle', train_neg_type_id)\n",
        "save_pickle(path+'pairwise-data/valid_pos_type_id_128_50.pickle', valid_pos_type_id)\n",
        "save_pickle(path+'pairwise-data/valid_neg_type_id_128_50.pickle', valid_neg_type_id)\n",
        "\n",
        "save_pickle(path+'pairwise-data/train_pos_mask_128_50.pickle', train_pos_att_mask)\n",
        "save_pickle(path+'pairwise-data/train_neg_mask_128_50.pickle', train_neg_att_mask)\n",
        "save_pickle(path+'pairwise-data/valid_pos_mask_128_50.pickle', valid_pos_att_mask)\n",
        "save_pickle(path+'pairwise-data/valid_neg_mask_128_50.pickle', valid_neg_att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdFPxvp-ahTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pos_labels = load_pickle(path+'pairwise-data/train_pos_labels_50.pickle')\n",
        "train_neg_labels = load_pickle(path+'pairwise-data/train_neg_labels_50.pickle')\n",
        "valid_pos_labels = load_pickle(path+'pairwise-data/valid_pos_labels_50.pickle')\n",
        "valid_neg_labels = load_pickle(path+'pairwise-data/valid_neg_labels_50.pickle')\n",
        "\n",
        "train_pos_input = load_pickle(path+'pairwise-data/train_pos_input_128_50.pickle')\n",
        "train_neg_input = load_pickle(path+'pairwise-data/train_neg_input_128_50.pickle')\n",
        "valid_pos_input = load_pickle(path+'pairwise-data/valid_pos_input_128_50.pickle')\n",
        "valid_neg_input = load_pickle(path+'pairwise-data/valid_neg_input_128_50.pickle')\n",
        "\n",
        "train_pos_type_id = load_pickle(path+'pairwise-data/train_pos_type_id_128_50.pickle')\n",
        "train_neg_type_id = load_pickle(path+'pairwise-data/train_neg_type_id_128_50.pickle')\n",
        "valid_pos_type_id = load_pickle(path+'pairwise-data/valid_pos_type_id_128_50.pickle')\n",
        "valid_neg_type_id = load_pickle(path+'pairwise-data/valid_neg_type_id_128_50.pickle')\n",
        "\n",
        "train_pos_mask = load_pickle(path+'pairwise-data/train_pos_mask_128_50.pickle')\n",
        "train_neg_mask = load_pickle(path+'pairwise-data/train_neg_mask_128_50.pickle')\n",
        "valid_pos_mask = load_pickle(path+'pairwise-data/valid_pos_mask_128_50.pickle')\n",
        "valid_neg_mask = load_pickle(path+'pairwise-data/valid_neg_mask_128_50.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7q6wCjj3XdB",
        "colab_type": "code",
        "outputId": "88935056-f38d-4352-91c9-65b1ba727388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_pos_input))\n",
        "print(len(valid_pos_input))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "277827\n",
            "30874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEL7jdoecvbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_pos_labels = train_pos_labels[:100]\n",
        "# train_neg_labels = train_neg_labels[:100]\n",
        "# train_pos_input = train_pos_input[:100]\n",
        "# train_neg_input = train_neg_input[:100]\n",
        "# train_pos_type_id = train_pos_type_id[:100]\n",
        "# train_neg_type_id = train_neg_type_id[:100]\n",
        "# train_pos_mask = train_pos_mask[:100]\n",
        "# train_neg_mask = train_neg_mask[:100]\n",
        "\n",
        "# valid_pos_labels = valid_pos_labels[:10]\n",
        "# valid_neg_labels = valid_neg_labels[:10]\n",
        "# valid_pos_input = valid_pos_input[:10]\n",
        "# valid_neg_input = valid_neg_input[:10]\n",
        "# valid_pos_type_id = valid_pos_type_id[:10]\n",
        "# valid_neg_type_id = valid_neg_type_id[:10]\n",
        "# valid_pos_mask = valid_pos_mask[:10]\n",
        "# valid_neg_mask = valid_neg_mask[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6c9oM8jauOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert lists to PyTorch tensors\n",
        "train_pos_inputs = torch.tensor(train_pos_input)\n",
        "train_neg_inputs = torch.tensor(train_neg_input)\n",
        "valid_pos_inputs = torch.tensor(valid_pos_input)\n",
        "valid_neg_inputs = torch.tensor(valid_neg_input)\n",
        "\n",
        "train_pos_labels = torch.tensor(train_pos_labels)\n",
        "train_neg_labels = torch.tensor(train_neg_labels)\n",
        "valid_pos_labels = torch.tensor(valid_pos_labels)\n",
        "valid_neg_labels = torch.tensor(valid_neg_labels)\n",
        "\n",
        "train_pos_type_ids = torch.tensor(train_pos_type_id)\n",
        "train_neg_type_ids = torch.tensor(train_neg_type_id)\n",
        "valid_pos_type_ids = torch.tensor(valid_pos_type_id)\n",
        "valid_neg_type_ids = torch.tensor(valid_neg_type_id)\n",
        "\n",
        "train_pos_masks = torch.tensor(train_pos_mask)\n",
        "train_neg_masks = torch.tensor(train_neg_mask)\n",
        "valid_pos_masks = torch.tensor(valid_pos_mask)\n",
        "valid_neg_masks = torch.tensor(valid_neg_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sBUeEuDau3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create DataLoaders to train the model in batches\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_pos_inputs, train_pos_type_ids, train_pos_masks, train_pos_labels, train_neg_inputs, train_neg_type_ids, train_neg_masks, train_neg_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(valid_pos_inputs, valid_pos_type_ids, valid_pos_masks, valid_pos_labels, valid_neg_inputs, valid_neg_type_ids, valid_neg_masks, valid_neg_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46AZOrGeawlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairwise_loss(pos_scores, neg_scores):\n",
        "    \"\"\"\n",
        "    Pairwise learning approach introduced in https://arxiv.org/pdf/1905.07588.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    cross_entropy_loss = -torch.log(pos_scores) - torch.log(1 - neg_scores)\n",
        "\n",
        "    margin = 1\n",
        "\n",
        "    hinge_loss = torch.max(torch.tensor(0, dtype=torch.float).to(device), margin - pos_scores + neg_scores)\n",
        "\n",
        "    loss = (0.5 * cross_entropy_loss + 0.5 * hinge_loss)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ofMf9vDazyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_pairwise(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "    # Reset the loss and accuracy for each epoch\n",
        "    total_loss = 0\n",
        "    nb_train_steps = 0\n",
        "    train_accuracy = 0\n",
        "\n",
        "    # Set model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # batch contains eight PyTorch tensors:\n",
        "        pos_input = batch[0].to(device)\n",
        "        pos_type_id = batch[1].to(device)\n",
        "        pos_mask = batch[2].to(device)\n",
        "        pos_labels = batch[3].to(device)\n",
        "\n",
        "        neg_input = batch[4].to(device)\n",
        "        neg_type_id = batch[5].to(device)\n",
        "        neg_mask = batch[6].to(device)\n",
        "        neg_labels = batch[7].to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Compute predictinos for postive and negative QA pairs\n",
        "        pos_outputs = model(pos_input, token_type_ids=pos_type_id, attention_mask=pos_mask, labels=pos_labels)\n",
        "        neg_outputs = model(neg_input, token_type_ids=neg_type_id, attention_mask=neg_mask, labels=neg_labels)\n",
        "\n",
        "        # Get the logits from the model for positive and negative QA pairs\n",
        "        pos_logits = pos_outputs[1]\n",
        "        neg_logits = neg_outputs[1]\n",
        "\n",
        "        # Get the column of the relevant scores and apply activation function\n",
        "        pos_scores = softmax(pos_logits, dim=1)[:,1]\n",
        "        neg_scores = softmax(neg_logits, dim=1)[:,1]\n",
        "        \n",
        "        # Compute pairwise loss and get the mean of each batch\n",
        "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        p_logits = pos_logits.detach().cpu().numpy()\n",
        "        p_labels = pos_labels.to('cpu').numpy()\n",
        "        n_logits = neg_logits.detach().cpu().numpy()\n",
        "        n_labels = neg_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for each batch\n",
        "        tmp_pos_accuracy = flat_accuracy(p_logits, p_labels)\n",
        "        tmp_neg_accuracy = flat_accuracy(n_logits, n_labels)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        train_accuracy += tmp_pos_accuracy\n",
        "        train_accuracy += tmp_neg_accuracy\n",
        "        \n",
        "        # Track the number of batches (2 for pos and neg accuracies)\n",
        "        nb_train_steps += 2\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    # Compute accuracy for each epoch\n",
        "    acc = train_accuracy/nb_train_steps\n",
        "\n",
        "    return avg_train_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wZBD9-BdFEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_pairwise(model, validation_dataloader):\n",
        "\n",
        "    # Set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    eval_accuracy = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        pos_input, pos_type_id, pos_mask, pos_labels, neg_input, neg_type_id, neg_mask, neg_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Compute predictinos for postive and negative QA pairs\n",
        "            pos_outputs = model(pos_input, token_type_ids=pos_type_id, attention_mask=pos_mask, labels=pos_labels)\n",
        "            neg_outputs = model(neg_input, token_type_ids=neg_type_id, attention_mask=neg_mask, labels=neg_labels)\n",
        "\n",
        "            # Get logits\n",
        "            pos_logits = pos_outputs[1]\n",
        "            neg_logits = neg_outputs[1]\n",
        "\n",
        "            # Apply activation function\n",
        "            pos_scores = softmax(pos_logits, dim=1)[:,1]\n",
        "            neg_scores = softmax(neg_logits, dim=1)[:,1]\n",
        "        \n",
        "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        p_logits = pos_logits.detach().cpu().numpy()\n",
        "        p_labels = pos_labels.to('cpu').numpy()\n",
        "        n_logits = neg_logits.detach().cpu().numpy()\n",
        "        n_labels = neg_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_pos_accuracy = flat_accuracy(p_logits, p_labels)\n",
        "        tmp_neg_accuracy = flat_accuracy(n_logits, n_labels)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_pos_accuracy\n",
        "        eval_accuracy += tmp_neg_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 2\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(validation_dataloader)\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxFYsHtADVla"
      },
      "source": [
        "## **Pointwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkOOxhr1KCeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input_data(dataset, max_seq_len):\n",
        "    input_ids = []\n",
        "    token_type_ids = []\n",
        "    att_masks = []\n",
        "    labels = []\n",
        "\n",
        "    for i, seq in enumerate(tqdm(dataset)):\n",
        "        qid, ans_labels, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        for docid in cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text, \n",
        "                                                max_length=max_seq_len, \n",
        "                                                pad_to_max_length=True, \n",
        "                                                return_token_type_ids=True,\n",
        "                                                return_attention_mask = True)\n",
        "\n",
        "            input_id = encoded_seq['input_ids']\n",
        "            token_type_id = encoded_seq['token_type_ids']\n",
        "            att_mask = encoded_seq['attention_mask']\n",
        "\n",
        "            if docid in ans_labels:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "\n",
        "            assert len(input_id) == max_seq_len, \"Input id dimension incorrect!\"\n",
        "            assert len(token_type_id) == max_seq_len, \"Token type id dimension incorrect!\"\n",
        "            assert len(att_mask) == max_seq_len, \"Attention mask dimension incorrect!\"\n",
        "\n",
        "            input_ids.append(input_id)\n",
        "            token_type_ids.append(token_type_id)\n",
        "            att_masks.append(att_mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    return input_ids, token_type_ids, att_masks, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfTcRrN6J4m7",
        "colab_type": "code",
        "outputId": "cf126bd0-33e3-438f-df5f-251663086d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_input, train_type_id, train_att_mask, train_label = get_input_data(train_set, 128)\n",
        "valid_input, valid_type_id, valid_att_mask, valid_label = get_input_data(valid_set, 128)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5676/5676 [20:37<00:00,  4.59it/s]\n",
            "100%|██████████| 631/631 [02:16<00:00,  4.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMBJFDBnXUr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'/pointwise-data/train_input_128_lm.pickle', train_input)\n",
        "save_pickle(path+'/pointwise-data/train_type_id_128_lm.pickle', train_type_id)\n",
        "save_pickle(path+'/pointwise-data/train_mask_128_lm.pickle', train_att_mask)\n",
        "save_pickle(path+'/pointwise-data/train_labels_128_lm.pickle', train_label)\n",
        "\n",
        "save_pickle(path+'/pointwise-data/valid_input_128_lm.pickle', valid_input)\n",
        "save_pickle(path+'/pointwise-data/valid_type_id_128_lm.pickle', valid_type_id)\n",
        "save_pickle(path+'/pointwise-data/valid_mask_128_lm.pickle', valid_att_mask)\n",
        "save_pickle(path+'/pointwise-data/valid_labels_128_lm.pickle', valid_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abSdoj111348",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = load_pickle(path+'/pointwise-data/train_input_128_50.pickle')\n",
        "train_type_id = load_pickle(path+'/pointwise-data/train_type_id_128_50.pickle')\n",
        "train_att_mask = load_pickle(path+'/pointwise-data/train_mask_128_50.pickle')\n",
        "train_label = load_pickle(path+'/pointwise-data/train_labels_128_50.pickle')\n",
        "\n",
        "valid_input = load_pickle(path+'/pointwise-data/valid_input_128_50.pickle')\n",
        "valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_128_50.pickle')\n",
        "valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_128_50.pickle')\n",
        "valid_label = load_pickle(path+'/pointwise-data/valid_labels_128_50.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NdmYES0rGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = train_input[:100]\n",
        "train_type_id = train_type_id[:100]\n",
        "train_att_mask = train_att_mask[:100]\n",
        "train_label = train_label[:100]\n",
        "\n",
        "valid_input = valid_input[:10]\n",
        "valid_type_id = valid_type_id[:10]\n",
        "valid_att_mask = valid_att_mask[:10]\n",
        "valid_label = valid_label[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKcl56lbMmQ0",
        "colab_type": "code",
        "outputId": "8093bc35-3ca9-4e1b-b10c-9f649f863c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_input))\n",
        "print(len(valid_input))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "283800\n",
            "31550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiCn2P_3MkN2",
        "colab_type": "code",
        "outputId": "85e0599a-4264-499b-fb67-ecf209de1af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Convert all inputs and labels into torch tensors\n",
        "train_inputs = torch.tensor(train_input)\n",
        "train_type_ids = torch.tensor(train_type_id)\n",
        "train_masks = torch.tensor(train_att_mask)\n",
        "train_labels = torch.tensor(train_label)\n",
        "\n",
        "validation_inputs = torch.tensor(valid_input)\n",
        "validation_type_ids = torch.tensor(valid_type_id)\n",
        "validation_masks = torch.tensor(valid_att_mask)\n",
        "validation_labels = torch.tensor(valid_label)\n",
        "\n",
        "# Create DataLoader to train in bacthes\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_type_ids, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_type_ids, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "print(\"Size of the DataLoader for the training set: {}\".format(len(train_dataloader)))\n",
        "print(\"Size of the DataLoader for the validation set: {}\".format(len(validation_dataloader)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the DataLoader for the training set: 8869\n",
            "Size of the DataLoader for the validation set: 986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KKtZ8j6Mzw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "    # Reset the total loss each epoch\n",
        "    total_loss = 0\n",
        "    train_accuracy = 0\n",
        "    # Track the number of batches\n",
        "    num_steps = 0\n",
        "\n",
        "    # Set model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # batch contains four PyTorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: token_type_ids\n",
        "        #   [2]: attention masks\n",
        "        #   [3]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_token_type_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Forward pass\n",
        "        # The model will return the loss and the logits\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids = b_token_type_ids, \n",
        "                    attention_mask = b_input_mask, \n",
        "                    labels = b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        tmp_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        train_accuracy += tmp_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        num_steps += 1\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    acc = train_accuracy/num_steps\n",
        "\n",
        "    return avg_train_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN-iWQHQM24D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, validation_dataloader):\n",
        "\n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    eval_accuracy = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # For each batch of the validation data\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from the dataloader\n",
        "        b_input_ids, b_token_type_ids, b_input_masks, b_labels = batch\n",
        "        \n",
        "        # Don't to compute or store gradients\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids = b_token_type_ids, \n",
        "                            attention_mask = b_input_masks,\n",
        "                            labels= b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "    avg_loss = total_loss / len(validation_dataloader)\n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "viv1NrZkLr8K"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSMphfqC4ldr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXs37WMe2MFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "bcc91024-0add-48ca-8eff-e6d7412b32b6"
      },
      "source": [
        "!git clone https://github.com/yuanbit/transformers.git"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/84)\u001b[K\rremote: Counting objects:   2% (2/84)\u001b[K\rremote: Counting objects:   3% (3/84)\u001b[K\rremote: Counting objects:   4% (4/84)\u001b[K\rremote: Counting objects:   5% (5/84)\u001b[K\rremote: Counting objects:   7% (6/84)\u001b[K\rremote: Counting objects:   8% (7/84)\u001b[K\rremote: Counting objects:   9% (8/84)\u001b[K\rremote: Counting objects:  10% (9/84)\u001b[K\rremote: Counting objects:  11% (10/84)\u001b[K\rremote: Counting objects:  13% (11/84)\u001b[K\rremote: Counting objects:  14% (12/84)\u001b[K\rremote: Counting objects:  15% (13/84)\u001b[K\rremote: Counting objects:  16% (14/84)\u001b[K\rremote: Counting objects:  17% (15/84)\u001b[K\rremote: Counting objects:  19% (16/84)\u001b[K\rremote: Counting objects:  20% (17/84)\u001b[K\rremote: Counting objects:  21% (18/84)\u001b[K\rremote: Counting objects:  22% (19/84)\u001b[K\rremote: Counting objects:  23% (20/84)\u001b[K\rremote: Counting objects:  25% (21/84)\u001b[K\rremote: Counting objects:  26% (22/84)\u001b[K\rremote: Counting objects:  27% (23/84)\u001b[K\rremote: Counting objects:  28% (24/84)\u001b[K\rremote: Counting objects:  29% (25/84)\u001b[K\rremote: Counting objects:  30% (26/84)\u001b[K\rremote: Counting objects:  32% (27/84)\u001b[K\rremote: Counting objects:  33% (28/84)\u001b[K\rremote: Counting objects:  34% (29/84)\u001b[K\rremote: Counting objects:  35% (30/84)\u001b[K\rremote: Counting objects:  36% (31/84)\u001b[K\rremote: Counting objects:  38% (32/84)\u001b[K\rremote: Counting objects:  39% (33/84)\u001b[K\rremote: Counting objects:  40% (34/84)\u001b[K\rremote: Counting objects:  41% (35/84)\u001b[K\rremote: Counting objects:  42% (36/84)\u001b[K\rremote: Counting objects:  44% (37/84)\u001b[K\rremote: Counting objects:  45% (38/84)\u001b[K\rremote: Counting objects:  46% (39/84)\u001b[K\rremote: Counting objects:  47% (40/84)\u001b[K\rremote: Counting objects:  48% (41/84)\u001b[K\rremote: Counting objects:  50% (42/84)\u001b[K\rremote: Counting objects:  51% (43/84)\u001b[K\rremote: Counting objects:  52% (44/84)\u001b[K\rremote: Counting objects:  53% (45/84)\u001b[K\rremote: Counting objects:  54% (46/84)\u001b[K\rremote: Counting objects:  55% (47/84)\u001b[K\rremote: Counting objects:  57% (48/84)\u001b[K\rremote: Counting objects:  58% (49/84)\u001b[K\rremote: Counting objects:  59% (50/84)\u001b[K\rremote: Counting objects:  60% (51/84)\u001b[K\rremote: Counting objects:  61% (52/84)\u001b[K\rremote: Counting objects:  63% (53/84)\u001b[K\rremote: Counting objects:  64% (54/84)\rremote: Counting objects:  65% (55/84)\u001b[K\rremote: Counting objects:  66% (56/84)\u001b[K\rremote: Counting objects:  67% (57/84)\u001b[K\rremote: Counting objects:  69% (58/84)\u001b[K\rremote: Counting objects:  70% (59/84)\u001b[K\rremote: Counting objects:  71% (60/84)\u001b[K\rremote: Counting objects:  72% (61/84)\u001b[K\rremote: Counting objects:  73% (62/84)\u001b[K\rremote: Counting objects:  75% (63/84)\u001b[K\rremote: Counting objects:  76% (64/84)\u001b[K\rremote: Counting objects:  77% (65/84)\u001b[K\rremote: Counting objects:  78% (66/84)\u001b[K\rremote: Counting objects:  79% (67/84)\u001b[K\rremote: Counting objects:  80% (68/84)\u001b[K\rremote: Counting objects:  82% (69/84)\u001b[K\rremote: Counting objects:  83% (70/84)\u001b[K\rremote: Counting objects:  84% (71/84)\u001b[K\rremote: Counting objects:  85% (72/84)\u001b[K\rremote: Counting objects:  86% (73/84)\u001b[K\rremote: Counting objects:  88% (74/84)\u001b[K\rremote: Counting objects:  89% (75/84)\u001b[K\rremote: Counting objects:  90% (76/84)\u001b[K\rremote: Counting objects:  91% (77/84)\u001b[K\rremote: Counting objects:  92% (78/84)\u001b[K\rremote: Counting objects:  94% (79/84)\u001b[K\rremote: Counting objects:  95% (80/84)\u001b[K\rremote: Counting objects:  96% (81/84)\u001b[K\rremote: Counting objects:  97% (82/84)\u001b[K\rremote: Counting objects:  98% (83/84)\u001b[K\rremote: Counting objects: 100% (84/84)\u001b[K\rremote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 23062 (delta 46), reused 62 (delta 26), pack-reused 22978\u001b[K\n",
            "Receiving objects: 100% (23062/23062), 13.35 MiB | 30.45 MiB/s, done.\n",
            "Resolving deltas: 100% (16519/16519), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yS9veg2qDVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmd = '''python /content/transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py \\\n",
        "        --tf_checkpoint_path /content/drive/'My Drive'/fiqa/bert-ms-macro/bert_model.ckpt \\\n",
        "        --bert_config_file /content/drive/'My Drive'/fiqa/bert-ms-macro/bert_config.json \\\n",
        "        --pytorch_dump_path /content/drive/'My Drive'/fiqa/bert-ms-macro/pytorch_model.bin'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWi2XrF6snnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!{cmd}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_zn-HiYolL",
        "colab_type": "code",
        "outputId": "8bbfbc16-d827-4bfc-bc76-f4777e0990cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top\n",
        "\n",
        "model_path = path + \"bert_ms\"\n",
        "# model_path = path + \"model/fin_model\"\n",
        "# model_path = path + \"bert-lm/lm_model_bert\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, cache_dir=None, num_labels=2)\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=None, num_labels=2)\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", cache_dir=None, num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7wlYPSlZcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 3e-6, weight_decay=0.01)\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 10000,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6iuWZ0S1gWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    # Get the column with the higher probability\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2fZJST0_L2Q7",
        "outputId": "bcbf9975-253f-45c3-f71d-190afeaf84cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss_values = []\n",
        "    train_acc_values = []\n",
        "\n",
        "    valid_loss_values = []\n",
        "    valid_acc_values = []\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, scheduler)\n",
        "    # train_loss, train_acc = train_pairwise(model, train_dataloader, optimizer, scheduler)\n",
        "    train_loss_values.append(train_loss)\n",
        "    train_acc_values.append(train_acc)\n",
        "\n",
        "    # Evaluate validation loss\n",
        "    valid_loss, valid_acc = validate(model, validation_dataloader)\n",
        "    # valid_loss, valid_acc = validate_pairwise(model, validation_dataloader)\n",
        "    valid_loss_values.append(valid_loss)\n",
        "    valid_acc_values.append(valid_acc)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), path + 'model/' + str(epoch+1)+'_ms50_128_32_3e6.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8869/8869 [57:13<00:00,  2.58it/s]\n",
            "100%|██████████| 986/986 [01:57<00:00,  8.43it/s]\n",
            "  0%|          | 0/8869 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 1:\n",
            "\t Train Loss: 0.09 | Train Accuracy: 97.67%\n",
            "\t Validation Loss: 0.083 | Validation Accuracy: 98.02%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 1564/8869 [10:04<47:11,  2.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ba1176b14054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Evaluate training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# train_loss, train_acc = train_pairwise(model, train_dataloader, optimizer, scheduler)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_loss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-265fd4902bf9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Move logits and labels to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHBg5FXKOYHU"
      },
      "source": [
        "## **Evalulation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGbsz18-Urm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rank(model, test_set, qid_rel, max_seq_len):\n",
        "    \"\"\"\n",
        "    Returns a dictionary - key: qid, value: list of ranked candidates\n",
        "    -------------------\n",
        "    model - PyTorch model\n",
        "    test_set - List of lists:\n",
        "            Each element is a list contraining \n",
        "            [qid, list of pos docid, list of candidate docid]\n",
        "    qid_rel: Dictionary\n",
        "            key: qid, value: list of relevant answer id\n",
        "    max_seq_len: int\n",
        "            Maximum sequence length\n",
        "    \"\"\"\n",
        "\n",
        "    # Initiate empty dictionary\n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # For each element in the test set\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "        \n",
        "        # question id, list of rel answers, list of candidates\n",
        "        qid, label, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        # Convert list to numpy array\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        # Empty list for the probability scores of relevancy\n",
        "        scores = []\n",
        "\n",
        "        # For each answer in the candidates\n",
        "        for docid in cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "\n",
        "            # Create inputs for the model\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text, \n",
        "                                            max_length=max_seq_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "            # Numericalized, padded, clipped seq with special tokens\n",
        "            input_ids = torch.tensor([encoded_seq['input_ids']]).to(device)\n",
        "            # Specify question seq and answer seq\n",
        "            token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(device)\n",
        "            # Sepecify which position is part of the seq which is padded\n",
        "            att_mask = torch.tensor([encoded_seq['attention_mask']]).to(device)\n",
        "\n",
        "            # Don't calculate gradients\n",
        "            with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions for each QA pair\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=att_mask)\n",
        "\n",
        "            # Get the predictions\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Apply activation function\n",
        "            pred = softmax(logits, dim=1)\n",
        "            # pred = torch.sigmoid(logits)\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "\n",
        "            # Append relevant scores to list (where label = 1)\n",
        "            scores.append(pred[:,1][0])\n",
        "\n",
        "        print(scores)\n",
        "\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        # Get the list of docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[qid] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOR-OM-qbb3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 3))\n",
        "toy_test = test_set[:3]\n",
        "# toy_test = [[14, [398960], [84963, 14255, 398960]],\n",
        "#             [68, [19183], [107584, 562777, 19183]],\n",
        "#             [70, [327002], [107584, 327002, 19183]]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa0KvT4cbbo",
        "colab_type": "code",
        "outputId": "456083b7-6246-4de3-d6f9-30a7250b279e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "model.load_state_dict(torch.load(path+'model/1_ms.pt'))\n",
        "\n",
        "# qid_pred_rank = get_rank(model, test_set, test_qid_rel, max_seq_len=128)\n",
        "qid_pred_rank = get_rank(model, toy_test, toy_test_label, max_seq_len=128)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [00:00<00:01,  1.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.0014460683, 0.99522877, 0.0018765935, 0.0011903158, 0.00060061767, 0.0011571081, 0.00032464223, 0.0007156715, 0.000780765, 0.029762005, 0.6243521, 0.0008354524, 0.0029766026, 0.00046705105, 0.00051491166, 0.000679071, 0.0006529388, 0.0048139566, 0.00050552905, 0.000544052, 0.0003895729, 0.005932562, 0.0016115921, 0.00028404934, 0.00034913508, 0.00043809923, 0.00038465037, 0.041316748, 0.00048760607, 0.00077471527, 0.0015235193, 0.000366235, 0.0003260585, 0.0012150598, 0.00036388932, 0.00029957006, 0.0015998115, 0.004005428, 0.0106254835, 0.0003295103, 0.00040285027, 0.0011463206, 0.00054562127, 0.00030715158, 0.0002946553, 0.000911881, 0.0004176608, 0.00035140006, 0.00038602372, 0.0029754876]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.97196925, 0.008280156, 0.004412411, 0.010010536, 0.0022449808, 0.091228954, 0.0058784983, 0.0029067171, 0.019332435, 0.007762578, 0.017799087, 0.0023705198, 0.0006583438, 0.0014078738, 0.005538806, 0.0008397903, 0.0009882883, 0.00054391613, 0.8258419, 0.012428946, 0.0005125336, 0.012678809, 0.0037025707, 0.0004955662, 0.00047348518, 0.002777003, 0.03767127, 0.94463646, 0.008658173, 0.00093420746, 0.011033851, 0.0022116548, 0.0012917854, 0.10312926, 0.033742648, 0.0021721853, 0.009774332, 0.00066910643, 0.0019171506, 0.00030864775, 0.0026087724, 0.00042558849, 0.0004427174, 0.0013152718, 0.00048310545, 0.00039837882, 0.001021676, 0.088001154, 0.0006918193, 0.94146526]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  1.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.90717447, 0.37946194, 0.012527585, 0.9392928, 0.40293965, 0.91837025, 0.0017553003, 0.0044712364, 0.3090517, 0.0038262026, 0.0003283907, 0.0012507847, 0.00038227387, 0.001462036, 0.004381163, 0.00070745416, 0.004524845, 0.00042225176, 0.0006246477, 0.015367881, 0.0006733387, 0.010245406, 0.00033109298, 0.0006216366, 0.0019443405, 0.00072143757, 0.0024006648, 0.003094416, 0.0030023514, 0.0030128702, 0.00062265474, 0.026119946, 0.0027926606, 0.00094714965, 0.0015739623, 0.014195795, 0.020892315, 0.0023928597, 0.00917006, 0.1569114, 0.01235481, 0.00040886493, 0.038152963, 0.0006537785, 0.0010073651, 0.000592088, 0.0015151189, 0.00070623093, 0.0004678558, 0.104139544]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTc2SQVhuv2Q",
        "colab_type": "code",
        "outputId": "b1e21de4-2790-4fb6-d5d4-0a1ff1571d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "k = 10\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "# MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, test_qid_rel, k)\n",
        "MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, toy_test_label, k)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 333 queries: 0.45206906236934075\n",
            "\n",
            "MRR@10 for 333 queries: 0.38095238095238093\n",
            "\n",
            "Average Precision@1: 0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcfbXb5eBcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'rank/rank_1_pointwise50_128_32_3e6.pickle', qid_pred_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}