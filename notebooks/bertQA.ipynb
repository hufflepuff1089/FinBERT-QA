{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertQA_pairwise.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x5arM3DfYyTO"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXHdHsEUxNd",
        "colab_type": "code",
        "outputId": "ba7269c8-08fc-4aaf-87f1-2c854157d0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hWS0v0sx7J",
        "colab_type": "code",
        "outputId": "38a03875-0af2-43f3-93f9-831942ef0ab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe6961502f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3dEiY-MzxU",
        "colab_type": "code",
        "outputId": "e98cc04f-80f1-4283-b4a0-302e98282f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.23)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.23)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhPBExB2bjG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGukeI_cCPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *\n",
        "from utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iCcUYUvb6-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary - key: qid, value: list of positive docid\n",
        "train_qid_rel = load_pickle(path + \"new-data/qid_rel_train.pickle\")\n",
        "test_qid_rel = load_pickle(path + \"new-data/qid_rel_test.pickle\")\n",
        "valid_qid_rel = load_pickle(path + \"new-data/qid_rel_valid.pickle\")\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list containing [qid, positive docid, negative docid]\n",
        "train_set = load_pickle(path + 'new-data/data_50/train_set_50.pickle')\n",
        "valid_set = load_pickle(path + 'new-data/data_50/valid_set_50.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/data_25/train_set_25.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_25/valid_set_25.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/data_10/train_set_10.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_10/valid_set_10.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/train_set.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/valid_set.pickle')\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list contraining [qid, list of pos docid, list of candidate docid]\n",
        "# Contains candidates with all pos docids\n",
        "test_set = load_pickle(path + 'new-data/data_50/test_set_50.pickle')\n",
        "# Contains candidates retrieved by BM25\n",
        "# May be missing pos docids in candidates\n",
        "test_set_full = load_pickle(path + 'new-data/data_50/test_set_full_50.pickle')\n",
        "\n",
        "# Dictionary mapping docid and qid to raw text\n",
        "docid_to_text = load_pickle(path + 'new-data/docid_to_text.pickle')\n",
        "qid_to_text = load_pickle(path + 'new-data/qid_to_text.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_sM2Lrb794",
        "colab_type": "code",
        "outputId": "ae4928a6-17f2-400a-d498-04291bbb2695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 142025\n",
            "Number of validation samples: 15800\n",
            "Number of test samples: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRSvaM8BEvH8",
        "colab_type": "code",
        "outputId": "2632e633-4f17-4888-8866-717909851ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Example of the training set [qid, pos docid, neg docid]\n",
        "print(train_set[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 18850, 378523], [0, 18850, 403025], [0, 18850, 173088], [0, 18850, 142631], [0, 18850, 59638], [0, 18850, 592891], [0, 18850, 53244], [0, 18850, 481339], [0, 18850, 22916], [0, 18850, 8891]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npVgMDNewIW",
        "colab_type": "code",
        "outputId": "8d38b3b9-d874-4af1-86fe-839ea4c2f702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB4xGQLQwSXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input(questions, answers, max_seq_len):\n",
        "    \"\"\"\n",
        "    Returns input objects for training:\n",
        "        input_ids: List of lists\n",
        "                Each element contains a list of padded/clipped numericalized\n",
        "                tokens of the sequences including [CLS] and [SEP] tokens\n",
        "                e.g. [[101, 2054, 2003, 102, 2449, 1029, 102], ...]\n",
        "        token_type_ids: List of lists\n",
        "                Each element contains a list of segment token indices to \n",
        "                indicate first and second portions of the inputs. \n",
        "                0 corresponds to a question token, 1 corresponds an answer token\n",
        "                e.g. [[0, 0, 0, 0, 1, 1, 1], ...]\n",
        "        att_masks: List of lists\n",
        "                Each element contains a list of mask values\n",
        "                Mask to avoid performing attention on padding token indices. \n",
        "                1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n",
        "                e.g. [[1, 1, 1, 1, 1, 1, 1], ...]\n",
        "    -----------------\n",
        "    questions: List of strings\n",
        "            Each element contains a question string\n",
        "    answers: List of strings\n",
        "            Each element contains an asnwer string\n",
        "    max_seq_len: int\n",
        "            Maximum sequence length\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    token_type_ids = []\n",
        "    att_masks = []\n",
        "\n",
        "    for i in tqdm(range(len(questions))):\n",
        "        a = questions[i]\n",
        "        b = answers[i]\n",
        "\n",
        "        # Tokenize the questions and answers, apply padding, and trim the vectors\n",
        "        # to the max_seq_len\n",
        "        encoded_seq = tokenizer.encode_plus(a, b, \n",
        "                                            max_length=max_seq_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "        input_id = encoded_seq['input_ids']\n",
        "        token_type_id = encoded_seq['token_type_ids']\n",
        "        att_mask = encoded_seq['attention_mask']\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Input id dimension incorrect!\"\n",
        "        assert len(token_type_id) == max_seq_len, \"Token type id dimension incorrect!\"\n",
        "        assert len(att_mask) == max_seq_len, \"Attention mask dimension incorrect!\"\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        att_masks.append(att_mask)\n",
        "\n",
        "    return input_ids, token_type_ids, att_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_kGUNI4XK5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    # Get the column with the higher probability\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x5arM3DfYyTO"
      },
      "source": [
        "## **Pairwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1C8XAC3Yuj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_pairwise_sequence_df(dataset):\n",
        "#     \"\"\"\n",
        "#     Converts training and validation data into a df with relevancy labels\n",
        "#     and map the qid and docid to text.\n",
        "    \n",
        "#     Returns data_df: df with columns qid, pos docid,\n",
        "#             neg docid, pos label, neg_label, question (text), \n",
        "#             pos answer (text), neg answer (text)\n",
        "#     ---------------\n",
        "#     dataset: train or validation set in the form of list of lists\n",
        "#     \"\"\"\n",
        "#     df = pd.DataFrame(dataset)\n",
        "#     df = df.rename(columns={0: 'qid', 1: 'pos_id', 2:'neg_id'})\n",
        "#     df['pos_label'] = df.apply(lambda x: 1, axis=1)\n",
        "#     df['neg_label'] = df.apply(lambda x: 0, axis=1)\n",
        "\n",
        "#     df['question'] = df['qid'].apply(lambda x: qid_to_text[x])\n",
        "#     df['pos_ans'] = df['pos_id'].apply(lambda x: docid_to_text[x])\n",
        "#     df['neg_ans'] = df['neg_id'].apply(lambda x: docid_to_text[x])\n",
        "\n",
        "#     return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYRQH1J2Zzxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trainset = get_pairwise_sequence_df(train_set)\n",
        "# train_questions = trainset.question.values\n",
        "# train_pos_answers = trainset.pos_ans.values\n",
        "# train_neg_answers = trainset.neg_ans.values\n",
        "\n",
        "# train_pos_labels = trainset.pos_label.values\n",
        "# train_neg_labels = trainset.neg_label.values\n",
        "\n",
        "# train_pos_input, train_pos_type_id, train_pos_att_mask = get_input(train_questions, train_pos_answers, 256)\n",
        "# train_neg_input, train_neg_type_id, train_neg_att_mask = get_input(train_questions, train_neg_answers, 256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DsejCaRZ9dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validset = get_pairwise_sequence_df(valid_set)\n",
        "# valid_questions = validset.question.values\n",
        "# valid_pos_answers = validset.pos_ans.values\n",
        "# valid_neg_answers = validset.neg_ans.values\n",
        "\n",
        "# valid_pos_labels = validset.pos_label.values\n",
        "# valid_neg_labels = validset.neg_label.values\n",
        "\n",
        "# valid_pos_input, valid_pos_type_id, valid_pos_att_mask = get_input(valid_questions, valid_pos_answers, 256)\n",
        "# valid_neg_input, valid_neg_type_id, valid_neg_att_mask = get_input(valid_questions, valid_neg_answers, 256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHGawog0Z461",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_pickle(path+'/data-bert/train_pos_labels_small.pickle', train_pos_labels)\n",
        "# save_pickle(path+'/data-bert/train_neg_labels_small.pickle', train_neg_labels)\n",
        "# save_pickle(path+'/data-bert/valid_pos_labels_small.pickle', valid_pos_labels)\n",
        "# save_pickle(path+'/data-bert/valid_neg_labels_small.pickle', valid_neg_labels)\n",
        "\n",
        "# save_pickle(path+'/data-bert/train_pos_input_256_small.pickle', train_pos_input)\n",
        "# save_pickle(path+'/data-bert/train_neg_input_256_small.pickle', train_neg_input)\n",
        "# save_pickle(path+'/data-bert/valid_pos_input_256_small.pickle', valid_pos_input)\n",
        "# save_pickle(path+'/data-bert/valid_neg_input_256_small.pickle', valid_neg_input)\n",
        "\n",
        "# save_pickle(path+'/data-bert/train_pos_type_id_256_small.pickle', train_pos_type_id)\n",
        "# save_pickle(path+'/data-bert/train_neg_type_id_256_small.pickle', train_neg_type_id)\n",
        "# save_pickle(path+'/data-bert/valid_pos_type_id_256_small.pickle', valid_pos_type_id)\n",
        "# save_pickle(path+'/data-bert/valid_neg_type_id_256_small.pickle', valid_neg_type_id)\n",
        "\n",
        "# save_pickle(path+'/data-bert/train_pos_mask_256_small.pickle', train_pos_att_mask)\n",
        "# save_pickle(path+'/data-bert/train_neg_mask_256_small.pickle', train_neg_att_mask)\n",
        "# save_pickle(path+'/data-bert/valid_pos_mask_256_small.pickle', valid_pos_att_mask)\n",
        "# save_pickle(path+'/data-bert/valid_neg_mask_256_small.pickle', valid_neg_att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdFPxvp-ahTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_pos_labels = load_pickle(path+'/data-bert/train_pos_labels_small.pickle')\n",
        "# train_neg_labels = load_pickle(path+'/data-bert/train_neg_labels_small.pickle')\n",
        "# valid_pos_labels = load_pickle(path+'/data-bert/valid_pos_labels_small.pickle')\n",
        "# valid_neg_labels = load_pickle(path+'/data-bert/valid_neg_labels_small.pickle')\n",
        "\n",
        "# train_pos_input = load_pickle(path+'/data-bert/train_pos_input_256_small.pickle')\n",
        "# train_neg_input = load_pickle(path+'/data-bert/train_neg_input_256_small.pickle')\n",
        "# valid_pos_input = load_pickle(path+'/data-bert/valid_pos_input_256_small.pickle')\n",
        "# valid_neg_input = load_pickle(path+'/data-bert/valid_neg_input_256_small.pickle')\n",
        "\n",
        "# train_pos_type_id = load_pickle(path+'/data-bert/train_pos_type_id_256_small.pickle')\n",
        "# train_neg_type_id = load_pickle(path+'/data-bert/train_neg_type_id_256_small.pickle')\n",
        "# valid_pos_type_id = load_pickle(path+'/data-bert/valid_pos_type_id_256_small.pickle')\n",
        "# valid_neg_type_id = load_pickle(path+'/data-bert/valid_neg_type_id_256_small.pickle')\n",
        "\n",
        "# train_pos_mask = load_pickle(path+'/data-bert/train_pos_mask_256_small.pickle')\n",
        "# train_neg_mask = load_pickle(path+'/data-bert/train_neg_mask_256_small.pickle')\n",
        "# valid_pos_mask = load_pickle(path+'/data-bert/valid_pos_mask_256_small.pickle')\n",
        "# valid_neg_mask = load_pickle(path+'/data-bert/valid_neg_mask_256_small.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7q6wCjj3XdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(len(train_pos_input))\n",
        "# print(len(valid_pos_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEL7jdoecvbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_pos_labels = train_pos_labels[:100]\n",
        "# train_neg_labels = train_neg_labels[:100]\n",
        "# train_pos_input = train_pos_input[:100]\n",
        "# train_neg_input = train_neg_input[:100]\n",
        "# train_pos_type_id = train_pos_type_id[:100]\n",
        "# train_neg_type_id = train_neg_type_id[:100]\n",
        "# train_pos_mask = train_pos_mask[:100]\n",
        "# train_neg_mask = train_neg_mask[:100]\n",
        "\n",
        "# valid_pos_labels = valid_pos_labels[:10]\n",
        "# valid_neg_labels = valid_neg_labels[:10]\n",
        "# valid_pos_input = valid_pos_input[:10]\n",
        "# valid_neg_input = valid_neg_input[:10]\n",
        "# valid_pos_type_id = valid_pos_type_id[:10]\n",
        "# valid_neg_type_id = valid_neg_type_id[:10]\n",
        "# valid_pos_mask = valid_pos_mask[:10]\n",
        "# valid_neg_mask = valid_neg_mask[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6c9oM8jauOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Convert lists to PyTorch tensors\n",
        "# train_pos_inputs = torch.tensor(train_pos_input)\n",
        "# train_neg_inputs = torch.tensor(train_neg_input)\n",
        "# valid_pos_inputs = torch.tensor(valid_pos_input)\n",
        "# valid_neg_inputs = torch.tensor(valid_neg_input)\n",
        "\n",
        "# train_pos_labels = torch.tensor(train_pos_labels)\n",
        "# train_neg_labels = torch.tensor(train_neg_labels)\n",
        "# valid_pos_labels = torch.tensor(valid_pos_labels)\n",
        "# valid_neg_labels = torch.tensor(valid_neg_labels)\n",
        "\n",
        "# train_pos_type_ids = torch.tensor(train_pos_type_id)\n",
        "# train_neg_type_ids = torch.tensor(train_neg_type_id)\n",
        "# valid_pos_type_ids = torch.tensor(valid_pos_type_id)\n",
        "# valid_neg_type_ids = torch.tensor(valid_neg_type_id)\n",
        "\n",
        "# train_pos_masks = torch.tensor(train_pos_mask)\n",
        "# train_neg_masks = torch.tensor(train_neg_mask)\n",
        "# valid_pos_masks = torch.tensor(valid_pos_mask)\n",
        "# valid_neg_masks = torch.tensor(valid_neg_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sBUeEuDau3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Create DataLoaders to train the model in batches\n",
        "\n",
        "# batch_size = 16\n",
        "\n",
        "# # Create the DataLoader for our training set.\n",
        "# train_data = TensorDataset(train_pos_inputs, train_pos_type_ids, train_pos_masks, train_pos_labels, train_neg_inputs, train_neg_type_ids, train_neg_masks, train_neg_labels)\n",
        "# train_sampler = RandomSampler(train_data)\n",
        "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# # Create the DataLoader for our validation set.\n",
        "# validation_data = TensorDataset(valid_pos_inputs, valid_pos_type_ids, valid_pos_masks, valid_pos_labels, valid_neg_inputs, valid_neg_type_ids, valid_neg_masks, valid_neg_labels)\n",
        "# validation_sampler = SequentialSampler(validation_data)\n",
        "# validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46AZOrGeawlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def pairwise_loss(pos_scores, neg_scores):\n",
        "#     \"\"\"\n",
        "#     Pairwise learning approach introduced in https://arxiv.org/pdf/1905.07588.pdf\n",
        "#     \"\"\"\n",
        "\n",
        "#     cross_entropy_loss = -torch.log(pos_scores) - torch.log(1 - neg_scores)\n",
        "\n",
        "#     margin = 0.5\n",
        "\n",
        "#     hinge_loss = torch.max(torch.tensor(0, dtype=torch.float).to(device), margin - pos_scores + neg_scores)\n",
        "\n",
        "#     loss = (0.5 * cross_entropy_loss + 0.5 * hinge_loss)\n",
        "\n",
        "#     return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ofMf9vDazyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def train_pairwise(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "#     # Store the average loss after each epoch so we can plot them.\n",
        "#     loss_values = []\n",
        "\n",
        "#     # Reset the loss and accuracy for each epoch\n",
        "#     total_loss = 0\n",
        "#     nb_train_steps = 0\n",
        "#     train_accuracy = 0\n",
        "\n",
        "#     # Set model in training mode\n",
        "#     model.train()\n",
        "\n",
        "#     # For each batch of training data...\n",
        "#     for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "#         # batch contains eight PyTorch tensors:\n",
        "#         pos_input = batch[0].to(device)\n",
        "#         pos_type_id = batch[1].to(device)\n",
        "#         pos_mask = batch[2].to(device)\n",
        "#         pos_labels = batch[3].to(device)\n",
        "\n",
        "#         neg_input = batch[4].to(device)\n",
        "#         neg_type_id = batch[5].to(device)\n",
        "#         neg_mask = batch[6].to(device)\n",
        "#         neg_labels = batch[7].to(device)\n",
        "\n",
        "#         # Zero gradients\n",
        "#         model.zero_grad()\n",
        "\n",
        "#         # Compute predictinos for postive and negative QA pairs\n",
        "#         pos_outputs = model(pos_input, token_type_ids=pos_type_id, attention_mask=pos_mask, labels=pos_labels)\n",
        "#         neg_outputs = model(neg_input, token_type_ids=neg_type_id, attention_mask=neg_mask, labels=neg_labels)\n",
        "\n",
        "#         # Get the logits from the model for positive and negative QA pairs\n",
        "#         pos_logits = pos_outputs[1]\n",
        "#         neg_logits = neg_outputs[1]\n",
        "\n",
        "#         # Get the column of the relevant scores and apply activation function\n",
        "#         pos_scores = softmax(pos_logits, dim=1)[:,1]\n",
        "#         neg_scores = softmax(neg_logits, dim=1)[:,1]\n",
        "        \n",
        "#         # Compute pairwise loss and get the mean of each batch\n",
        "#         loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "#         # Move logits and labels to CPU\n",
        "#         p_logits = pos_logits.detach().cpu().numpy()\n",
        "#         p_labels = pos_labels.to('cpu').numpy()\n",
        "#         n_logits = neg_logits.detach().cpu().numpy()\n",
        "#         n_labels = neg_labels.to('cpu').numpy()\n",
        "\n",
        "#         # Calculate the accuracy for each batch\n",
        "#         tmp_pos_accuracy = flat_accuracy(p_logits, p_labels)\n",
        "#         tmp_neg_accuracy = flat_accuracy(n_logits, n_labels)\n",
        "\n",
        "#         # Accumulate the total accuracy.\n",
        "#         train_accuracy += tmp_pos_accuracy\n",
        "#         train_accuracy += tmp_neg_accuracy\n",
        "        \n",
        "#         # Track the number of batches (2 for pos and neg accuracies)\n",
        "#         nb_train_steps += 2\n",
        "\n",
        "#         # Accumulate the training loss over all of the batches\n",
        "#         total_loss += loss.item()\n",
        "    \n",
        "#         # Perform a backward pass to calculate the gradients.\n",
        "#         loss.backward()\n",
        "\n",
        "#         # Clip the norm of the gradients to 1.0.\n",
        "#         # This is to help prevent the \"exploding gradients\" problem.\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "#         # Update parameters and take a step using the computed gradient.\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Update scheduler\n",
        "#         scheduler.step()\n",
        "\n",
        "#     # Calculate the average loss over the training data.\n",
        "#     avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "#     # Store the loss value for plotting the learning curve.\n",
        "#     loss_values.append(avg_train_loss)\n",
        "\n",
        "#     # Compute accuracy for each epoch\n",
        "#     acc = train_accuracy/nb_train_steps\n",
        "\n",
        "#     return avg_train_loss, acc, loss_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wZBD9-BdFEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def validate_pairwise(model, validation_dataloader):\n",
        "\n",
        "#     # Set model in evaluation mode\n",
        "#     model.eval()\n",
        "\n",
        "#     # Tracking variables \n",
        "#     total_loss = 0\n",
        "#     nb_eval_steps = 0\n",
        "#     eval_accuracy = 0\n",
        "\n",
        "#     # Evaluate data for one epoch\n",
        "#     for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "#         # Add batch to GPU\n",
        "#         batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "#         # Unpack the inputs from our dataloader\n",
        "#         pos_input, pos_type_id, pos_mask, pos_labels, neg_input, neg_type_id, neg_mask, neg_labels = batch\n",
        "        \n",
        "#         # Telling the model not to compute or store gradients, saving memory and\n",
        "#         # speeding up validation\n",
        "#         with torch.no_grad():\n",
        "#             # Compute predictinos for postive and negative QA pairs\n",
        "#             pos_outputs = model(pos_input, token_type_ids=pos_type_id, attention_mask=pos_mask, labels=pos_labels)\n",
        "#             neg_outputs = model(neg_input, token_type_ids=neg_type_id, attention_mask=neg_mask, labels=neg_labels)\n",
        "\n",
        "#             # Get logits\n",
        "#             pos_logits = pos_outputs[1]\n",
        "#             neg_logits = neg_outputs[1]\n",
        "\n",
        "#             # Apply activation function\n",
        "#             pos_scores = softmax(pos_logits, dim=1)[:,1]\n",
        "#             neg_scores = softmax(neg_logits, dim=1)[:,1]\n",
        "        \n",
        "#         loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "#         # Move logits and labels to CPU\n",
        "#         p_logits = pos_logits.detach().cpu().numpy()\n",
        "#         p_labels = pos_labels.to('cpu').numpy()\n",
        "#         n_logits = neg_logits.detach().cpu().numpy()\n",
        "#         n_labels = neg_labels.to('cpu').numpy()\n",
        "\n",
        "#         # Calculate the accuracy for this batch of test sentences.\n",
        "#         tmp_pos_accuracy = flat_accuracy(p_logits, p_labels)\n",
        "#         tmp_neg_accuracy = flat_accuracy(n_logits, n_labels)\n",
        "\n",
        "#         # Accumulate the total accuracy.\n",
        "#         eval_accuracy += tmp_pos_accuracy\n",
        "#         eval_accuracy += tmp_neg_accuracy\n",
        "\n",
        "#         # Track the number of batches\n",
        "#         nb_eval_steps += 2\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     avg_loss = total_loss / len(validation_dataloader)\n",
        "#     acc = eval_accuracy/nb_eval_steps\n",
        "\n",
        "#     return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxFYsHtADVla"
      },
      "source": [
        "## **Pointwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NIXDVVCUFMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sequence_df(dataset):\n",
        "    \"\"\"\n",
        "    Converts training and validation data into a df with relevancy labels\n",
        "    and map the qid and docid to text.\n",
        "    \n",
        "    Returns data_df: df with columns qid, docid, label, question (text), answer (text)\n",
        "    ---------------\n",
        "    dataset: train or validation set in the form of list of lists\n",
        "    \"\"\"\n",
        "    # Load list into a dataframe\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df.rename(columns={0: 'qid', 1: 'pos', 2:'neg'})\n",
        "    # Construct new df with positive docids\n",
        "    df_pos = df[['qid', 'pos']]\n",
        "    df_pos = df_pos.rename(columns={'pos': 'docid'})\n",
        "    # Add new column and assign positive label\n",
        "    df_pos['label'] = df_pos.apply(lambda x: 1, axis=1)\n",
        "    df_pos = df_pos.drop_duplicates()\n",
        "\n",
        "    # Construct new df with negative docids\n",
        "    df_neg = df[['qid', 'neg']]\n",
        "    df_neg = df_neg.rename(columns={'neg': 'docid'})\n",
        "    # Add new column and assign negative label\n",
        "    df_neg['label'] = df_neg.apply(lambda x: 0, axis=1)\n",
        "\n",
        "    # Concatenate the positive and negative df\n",
        "    data_df = pd.concat([df_pos, df_neg]).sort_values(by=['qid'])\n",
        "\n",
        "    # Map id to text\n",
        "    data_df['question'] = data_df['qid'].apply(lambda x: qid_to_text[x])\n",
        "    data_df['ans_cand'] = data_df['docid'].apply(lambda x: docid_to_text[x])\n",
        "\n",
        "    return data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEgEpLIAUNZK",
        "colab_type": "code",
        "outputId": "f9e4d851-cf88-47f9-ec29-2b8165c9f956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# # Create train/validation data\n",
        "# trainset = get_sequence_df(train_set)\n",
        "# train_questions = trainset.question.values\n",
        "# train_answers = trainset.ans_cand.values\n",
        "# train_labels = trainset.label.values\n",
        "\n",
        "# train_input, train_type_id, train_att_mask = get_input(train_questions, train_answers, 128)\n",
        "\n",
        "# validset = get_sequence_df(valid_set)\n",
        "# valid_questions = validset.question.values\n",
        "# valid_answers = validset.ans_cand.values\n",
        "# valid_labels = validset.label.values\n",
        "\n",
        "# valid_input, valid_type_id, valid_att_mask = get_input(valid_questions, valid_answers, 128)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_labels_128_50.pickle', train_labels)\n",
        "# save_pickle(path+'/pointwise-data/valid_labels_128_50.pickle', valid_labels)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_input_128_50.pickle', train_input)\n",
        "# save_pickle(path+'/pointwise-data/valid_input_128_50.pickle', valid_input)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_type_id_128_50.pickle', train_type_id)\n",
        "# save_pickle(path+'/pointwise-data/valid_type_id_128_50.pickle', valid_att_mask)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_mask_128_50.pickle', train_att_mask)\n",
        "# save_pickle(path+'/pointwise-data/valid_mask_128_50.pickle', valid_att_mask)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 298662/298662 [29:18<00:00, 169.81it/s]\n",
            "100%|██████████| 33195/33195 [02:35<00:00, 213.79it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ftXkM6Lu8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load train/validation data\n",
        "train_label = load_pickle(path+'/pointwise-data/train_labels_128_50.pickle')\n",
        "valid_label = load_pickle(path+'/pointwise-data/valid_labels_128_50.pickle')\n",
        "\n",
        "# bert-base tokenizer\n",
        "train_input = load_pickle(path+'/pointwise-data/train_input_128_50.pickle')\n",
        "valid_input = load_pickle(path+'/pointwise-data/valid_input_128_50.pickle')\n",
        "train_type_id = load_pickle(path+'/pointwise-data/train_type_id_128_50.pickle')\n",
        "valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_128_50.pickle')\n",
        "train_att_mask = load_pickle(path+'/pointwise-data/train_mask_128_50.pickle')\n",
        "valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_128_50.pickle')\n",
        "\n",
        "# train_input = load_pickle(path+'/pointwise-data/train_input_256_10.pickle')\n",
        "# valid_input = load_pickle(path+'/pointwise-data/valid_input_256_10.pickle')\n",
        "# train_type_id = load_pickle(path+'/pointwise-data/train_type_id_256_10.pickle')\n",
        "# valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_256_10.pickle')\n",
        "# train_att_mask = load_pickle(path+'/pointwise-data/train_mask_256_10.pickle')\n",
        "# valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_256_10.pickle')\n",
        "\n",
        "\n",
        "# bert-large tokenizer\n",
        "# train_input = load_pickle(path+'/pointwise-data/train_input_256_large.pickle')\n",
        "# valid_input = load_pickle(path+'/pointwise-data/valid_input_256_large.pickle')\n",
        "# train_type_id = load_pickle(path+'/pointwise-data/train_type_id_256_large.pickle')\n",
        "# valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_256_large.pickle')\n",
        "# train_att_mask = load_pickle(path+'/pointwise-data/train_mask_256_large.pickle')\n",
        "# valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_256_large.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKcl56lbMmQ0",
        "colab_type": "code",
        "outputId": "55f0f07e-b6a3-4b3e-e548-139c5dbe88f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_input))\n",
        "print(len(valid_input))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "298662\n",
            "33195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiCn2P_3MkN2",
        "colab_type": "code",
        "outputId": "15430905-9c9c-4aab-c1a8-0fd7e0e65359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Convert all inputs and labels into torch tensors\n",
        "train_labels = torch.tensor(train_label)\n",
        "validation_labels = torch.tensor(valid_label)\n",
        "\n",
        "train_inputs = torch.tensor(train_input)\n",
        "validation_inputs = torch.tensor(valid_input)\n",
        "\n",
        "train_type_ids = torch.tensor(train_type_id)\n",
        "validation_type_ids = torch.tensor(valid_type_id)\n",
        "\n",
        "train_masks = torch.tensor(train_att_mask)\n",
        "validation_masks = torch.tensor(valid_att_mask)\n",
        "\n",
        "# Create DataLoader to train in bacthes\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_type_ids, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_type_ids, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "print(\"Size of the DataLoader for the training set: {}\".format(len(train_dataloader)))\n",
        "print(\"Size of the DataLoader for the validation set: {}\".format(len(validation_dataloader)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the DataLoader for the training set: 1167\n",
            "Size of the DataLoader for the validation set: 130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KKtZ8j6Mzw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "    # Reset the total loss each epoch\n",
        "    total_loss = 0\n",
        "    train_accuracy = 0\n",
        "    # Track the number of batches\n",
        "    num_steps = 0\n",
        "\n",
        "    # Set model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # batch contains four PyTorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: token_type_ids\n",
        "        #   [2]: attention masks\n",
        "        #   [3]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_token_type_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Forward pass\n",
        "        # The model will return the loss and the logits\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids = b_token_type_ids, \n",
        "                    attention_mask = b_input_mask, \n",
        "                    labels = b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        tmp_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        train_accuracy += tmp_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        num_steps += 1\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    acc = train_accuracy/num_steps\n",
        "\n",
        "    return avg_train_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN-iWQHQM24D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, validation_dataloader):\n",
        "\n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    eval_accuracy = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # For each batch of the validation data\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from the dataloader\n",
        "        b_input_ids, b_token_type_ids, b_input_masks, b_labels = batch\n",
        "        \n",
        "        # Don't to compute or store gradients\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids = b_token_type_ids, \n",
        "                            attention_mask = b_input_masks,\n",
        "                            labels= b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "    avg_loss = total_loss / len(validation_dataloader)\n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "viv1NrZkLr8K"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_zn-HiYolL",
        "colab_type": "code",
        "outputId": "0bb1dbfd-e48d-41ce-e2bc-6a3d3219073c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top\n",
        "\n",
        "# model_path = \"/content/drive/My Drive/FiQA/model/fin_model\"\n",
        "# model = BertForSequenceClassification.from_pretrained(model_path, cache_dir=None, num_labels=2)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=None, num_labels=2)\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", cache_dir=None, num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7wlYPSlZcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-7, eps = 1e-8)\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 6\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3lU_QLEl4lY",
        "colab_type": "code",
        "outputId": "edc646b6-c2fc-4f09-d761-0cbca2c89e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss_values = []\n",
        "    train_acc_values = []\n",
        "\n",
        "    valid_loss_values = []\n",
        "    valid_acc_values = []\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, scheduler)\n",
        "    # train_loss, train_acc = train_pairwise(model, train_dataloader, optimizer, scheduler)\n",
        "    train_loss_values.append(train_loss)\n",
        "    train_acc_values.append(train_acc)\n",
        "\n",
        "    # Evaluate validation loss\n",
        "    valid_loss, valid_acc = validate(model, validation_dataloader)\n",
        "    # valid_loss, valid_acc = validate_pairwise(model, validation_dataloader)\n",
        "    valid_loss_values.append(valid_loss)\n",
        "    valid_acc_values.append(valid_acc)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    # torch.save(model.state_dict(), path + 'model/' + str(epoch+1)+'_pointwise_128_50_2e7.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1167 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-0445c1627ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Evaluate training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# train_loss, train_acc = train_pairwise(model, train_dataloader, optimizer, scheduler)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_loss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-265fd4902bf9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_token_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     labels = b_labels)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m   1174\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m         )\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         )\n\u001b[1;32m    792\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mmixed_key_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mmixed_value_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 15.90 GiB total capacity; 14.98 GiB already allocated; 77.75 MiB free; 15.12 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHBg5FXKOYHU"
      },
      "source": [
        "## **Evalulation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGbsz18-Urm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rank(model, test_set, qid_rel, max_seq_len):\n",
        "    \"\"\"\n",
        "    Returns a dictionary - key: qid, value: list of ranked candidates\n",
        "    -------------------\n",
        "    model - PyTorch model\n",
        "    test_set - List of lists:\n",
        "            Each element is a list contraining \n",
        "            [qid, list of pos docid, list of candidate docid]\n",
        "    qid_rel: Dictionary\n",
        "            key: qid, value: list of relevant answer id\n",
        "    max_seq_len: int\n",
        "            Maximum sequence length\n",
        "    \"\"\"\n",
        "\n",
        "    # Initiate empty dictionary\n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # For each element in the test set\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "        \n",
        "        # question id, list of rel answers, list of candidates\n",
        "        qid, label, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        # Convert list to numpy array\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        # Empty list for the probability scores of relevancy\n",
        "        scores = []\n",
        "\n",
        "        # For each answer in the candidates\n",
        "        for docid in cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "\n",
        "            # Create inputs for the model\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text, \n",
        "                                            max_length=max_seq_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "            # Numericalized, padded, clipped seq with special tokens\n",
        "            input_ids = torch.tensor([encoded_seq['input_ids']]).to(device)\n",
        "            # Specify question seq and answer seq\n",
        "            token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(device)\n",
        "            # Sepecify which position is part of the seq which is padded\n",
        "            att_mask = torch.tensor([encoded_seq['attention_mask']]).to(device)\n",
        "\n",
        "            # Don't calculate gradients\n",
        "            with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions for each QA pair\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=att_mask)\n",
        "\n",
        "            # Get the predictions\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Apply activation function\n",
        "            pred = softmax(logits, dim=1)\n",
        "            # pred = torch.sigmoid(logits)\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "\n",
        "            # Append relevant scores to list (where label = 1)\n",
        "            scores.append(pred[:,1][0])\n",
        "\n",
        "        print(scores)\n",
        "\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        # Get the list of docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[qid] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOR-OM-qbb3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 3))\n",
        "toy_test = test_set[:3]\n",
        "# toy_test = [[14, [398960], [84963, 14255, 398960]],\n",
        "#             [68, [19183], [107584, 562777, 19183]],\n",
        "#             [70, [327002], [107584, 327002, 19183]]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa0KvT4cbbo",
        "colab_type": "code",
        "outputId": "ef659d58-8fda-4ab2-f7ea-3b69d3a7c1bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "model.load_state_dict(torch.load(path+'model/2_pointwise_128_50_2e7.pt'))\n",
        "\n",
        "# qid_pred_rank = get_rank(model, test_set, test_qid_rel, max_seq_len=256)\n",
        "qid_pred_rank = get_rank(model, toy_test, toy_test_label, max_seq_len=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [00:09<00:18,  9.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.8286438, 0.72305965, 0.011733182, 0.66405743, 0.0017544054, 0.6940986, 0.61294454, 0.81399083, 0.77299845, 0.7044675, 0.14875504, 0.41618046, 0.007451423, 0.013855331, 0.5748976, 0.7867979, 0.45340636, 0.76467717, 0.27105865, 0.008734003, 0.034240425, 0.7437239, 0.69202936, 0.8479345, 0.2737445, 0.8497187, 0.6374513, 0.0065103024, 0.45250896, 0.80903476, 0.0055502118, 0.68835175, 0.6404603, 0.71484506, 0.82673657, 0.3671019, 0.045627806, 0.554348, 0.00242398, 0.011698064, 0.2303327, 0.18343966, 0.019879868, 0.7884072, 0.6814348, 0.22711255, 0.23317909, 0.5559166, 0.007007909, 0.005729991, 0.6709125, 0.82528657, 0.06396706, 0.8109578, 0.34104696, 0.33541587, 0.40592748, 0.0077982196, 0.6434781, 0.04711554, 0.8567093, 0.013972649, 0.01646979, 0.8189694, 0.029761527, 0.8610558, 0.008540276, 0.38938007, 0.84518677, 0.71175677, 0.83031505, 0.06795997, 0.5186739, 0.01461953, 0.6108138, 0.17782144, 0.033869997, 0.018984089, 0.045874123, 0.7540156, 0.004031275, 0.85207224, 0.76301336, 0.48091507, 0.0027953226, 0.75667495, 0.85966116, 0.797876, 0.009780116, 0.63298905, 0.013259049, 0.8506257, 0.8310544, 0.78919685, 0.82005495, 0.42484748, 0.0031775634, 0.46348056, 0.012793952, 0.7723314, 0.8375717, 0.0023475909, 0.8226602, 0.6670097, 0.23284793, 0.31391695, 0.84070355, 0.17730898, 0.5950034, 0.013237855, 0.037551224, 0.12634003, 0.090013765, 0.6872879, 0.23769802, 0.041324906, 0.010746541, 0.6365979, 0.8562386, 0.70603037, 0.034506094, 0.27435276, 0.70345825, 0.4872091, 0.617315, 0.76212656, 0.80164033, 0.088420115, 0.52116615, 0.7609593, 0.4469513, 0.027635043, 0.04668574, 0.35001513, 0.8485718, 0.81466913, 0.07522402, 0.04335872, 0.034885492, 0.75551414, 0.72386646, 0.61175483, 0.76081485, 0.016821949, 0.71303046, 0.507996, 0.028337242, 0.8148494, 0.82181233, 0.008865597, 0.75390846, 0.28948686, 0.7239992, 0.6518179, 0.02212644, 0.41902575, 0.83857435, 0.59880257, 0.00472572, 0.0047166855, 0.1195907, 0.0045630783, 0.80470693, 0.0076422063, 0.007506988, 0.77374035, 0.7083211, 0.80233675, 0.6345944, 0.28036758, 0.0045312555, 0.7992065, 0.8290249, 0.04228945, 0.24303763, 0.34711057, 0.0041404436, 0.48191187, 0.054295816, 0.0246377, 0.8342756, 0.8337904, 0.74566174, 0.011445072, 0.0021237158, 0.023368426, 0.8103986, 0.7417439, 0.8389306, 0.17826702, 0.8539915, 0.6294638, 0.8610708, 0.008112552, 0.7405238, 0.008342352, 0.14507987, 0.22871077, 0.86001796, 0.43034393, 0.12826933, 0.0031454829, 0.77293664, 0.6555398, 0.7536455, 0.56538016, 0.25679684, 0.0042493176, 0.8222554, 0.06674006, 0.8178762, 0.36640623, 0.32019418, 0.014478479, 0.8452809, 0.7227181, 0.06943245, 0.44278783, 0.79475915, 0.01524792, 0.7875234, 0.6684467, 0.5948456, 0.83471143, 0.83919764, 0.8463777, 0.73651123, 0.70656675, 0.038368233, 0.33778083, 0.66154516, 0.77117145, 0.829481, 0.0017379567, 0.19420531, 0.7550315, 0.8593679, 0.8045633, 0.8441787, 0.047010563, 0.7385929, 0.7307176, 0.7994029, 0.008001582, 0.58733505, 0.824978, 0.6246192, 0.0068329386, 0.6083547, 0.03830994, 0.6086448, 0.014162642, 0.047447544, 0.83999074, 0.83642393, 0.017084932, 0.059614886, 0.86155784, 0.8143015, 0.008560969, 0.59357774, 0.002279389, 0.0020373266, 0.8282988, 0.08867753, 0.00801379, 0.62939394, 0.73623335, 0.0053160433, 0.8206041, 0.1345721, 0.6726325, 0.46335706, 0.32927254, 0.0037018303, 0.83672094, 0.026663741, 0.1961149, 0.03211584, 0.6600189, 0.6760929, 0.7840764, 0.0047470583, 0.040087756, 0.09128711, 0.01378729, 0.21612439, 0.7153259, 0.03743743, 0.77970904, 0.5886801, 0.6452701, 0.7903615, 0.015247655, 0.39388192, 0.8209336, 0.02239665, 0.7875829, 0.3549444, 0.77063465, 0.74660677, 0.07080787, 0.007588344, 0.55115825, 0.1694848, 0.29036224, 0.8603634, 0.8544152, 0.7937025, 0.80469453, 0.8522177, 0.70471233, 0.7733397, 0.86414766, 0.0098856045, 0.026096774, 0.8090828, 0.100763656, 0.0041105915, 0.09480523, 0.008419624, 0.028935086, 0.69721997, 0.011392549, 0.0059239715, 0.01896964, 0.69885796, 0.1441621, 0.5438743, 0.5881047, 0.7169667, 0.8455238, 0.80744934, 0.14784409, 0.026197432, 0.8572699, 0.7554121, 0.74234486, 0.21531074, 0.16810068, 0.5478031, 0.8259227, 0.001311829, 0.3488653, 0.7822471, 0.041215744, 0.71352786, 0.83467597, 0.7453231, 0.14322594, 0.0069577796, 0.6037738, 0.8371115, 0.26789856, 0.043185, 0.0020049964, 0.04082192, 0.3066524, 0.8448969, 0.19645037, 0.1692338, 0.7935556, 0.84846586, 0.022346947, 0.644866, 0.5353965, 0.53771865, 0.004024079, 0.091008425, 0.013202901, 0.8157389, 0.74695516, 0.84814185, 0.81009907, 0.64972526, 0.6997421, 0.0072838967, 0.12986587, 0.5241173, 0.21465914, 0.7808845, 0.71026766, 0.3751122, 0.8285045, 0.44596052, 0.0077716913, 0.7859661, 0.42359212, 0.84874153, 0.0055861524, 0.3762739, 0.33704194, 0.008075616, 0.08718803, 0.65080357, 0.6981838, 0.7914677, 0.81885296, 0.7774798, 0.11975385, 0.06356107, 0.67223865, 0.071642764, 0.84457946, 0.7190291, 0.0025578397, 0.73537135, 0.8408905, 0.021702126, 0.79422593, 0.31520772, 0.07533564, 0.022401754, 0.026127, 0.21905066, 0.2341139, 0.023773568, 0.17367662, 0.002768088, 0.15038277, 0.8036836, 0.0024900187, 0.4281397, 0.73770136, 0.00713355, 0.848474, 0.004467392, 0.0020049396, 0.75996923, 0.74114907, 0.6946971, 0.10890823, 0.18418287, 0.510111, 0.86154306, 0.3813818, 0.7800867, 0.13308026, 0.21712533, 0.04476617, 0.7642749, 0.26074478, 0.5678501, 0.088136226, 0.0075384662, 0.32058045, 0.14572245, 0.039464638, 0.0043379245, 0.010034134, 0.8517206, 0.12445263, 0.7431597, 0.5215371, 0.005280099, 0.7913839, 0.8331187, 0.8158423, 0.82095164, 0.33773136, 0.73597825, 0.86012197, 0.30473444, 0.84747344, 0.80763215, 0.5115918, 0.3464989, 0.35241687, 0.004823199, 0.18231848, 0.06322573, 0.75232345, 0.08697586, 0.7417749, 0.5059915, 0.2986765, 0.8267959, 0.5992727, 0.042175017, 0.8288461, 0.0051751225, 0.8483823, 0.775788, 0.36971325, 0.83785355, 0.0098642, 0.8542371, 0.7783181, 0.36301824, 0.8316393, 0.8378301, 0.7061746, 0.21036935, 0.10198084, 0.032205068, 0.4645174, 0.6262573, 0.8285954, 0.46283248, 0.8614345]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 2/3 [00:19<00:09,  9.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.86213356, 0.003980151, 0.035144392, 0.6132624, 0.75796264, 0.83089435, 0.6005238, 0.8118866, 0.46177718, 0.06186548, 0.8687662, 0.8243675, 0.14816026, 0.6996984, 0.8429698, 0.0076438594, 0.04756771, 0.3202796, 0.83714217, 0.13839525, 0.8545097, 0.38070533, 0.7365635, 0.8578779, 0.8620319, 0.051974375, 0.5307797, 0.8057818, 0.34916183, 0.2959169, 0.84927046, 0.21821316, 0.036304943, 0.6785242, 0.659052, 0.8550491, 0.8547522, 0.008818584, 0.8287596, 0.7810859, 0.8441255, 0.05264803, 0.8324131, 0.7346262, 0.8373968, 0.85325235, 0.043774836, 0.024039563, 0.82511085, 0.01290045, 0.8545681, 0.83262146, 0.024370007, 0.105772905, 0.80041295, 0.8532373, 0.8424274, 0.70840776, 0.6116571, 0.8212868, 0.8559321, 0.022429608, 0.012032435, 0.21483144, 0.080795385, 0.8026127, 0.81140584, 0.42879778, 0.16733481, 0.8472511, 0.861845, 0.3102774, 0.8437901, 0.8605962, 0.8456415, 0.86160094, 0.62944347, 0.0016501349, 0.8620463, 0.5898789, 0.85533804, 0.79665655, 0.0957264, 0.78953516, 0.023331624, 0.2723356, 0.09556469, 0.86541754, 0.81364435, 0.8117809, 0.8600294, 0.8412194, 0.8253564, 0.1108551, 0.040299956, 0.8015676, 0.009305691, 0.2923152, 0.7522802, 0.0921064, 0.7917966, 0.8542127, 0.8069025, 0.6207739, 0.27175158, 0.5242916, 0.8047203, 0.7777045, 0.75505906, 0.7744976, 0.3115853, 0.18438695, 0.85614955, 0.61318636, 0.8532126, 0.17619802, 0.86427444, 0.8276742, 0.8591528, 0.8568351, 0.2236744, 0.8617411, 0.009029654, 0.8562163, 0.46714836, 0.34788796, 0.85684484, 0.8470034, 0.6691398, 0.8592752, 0.2793857, 0.51756734, 0.8420814, 0.8269054, 0.7709386, 0.8550679, 0.7824047, 0.016116304, 0.83458877, 0.6399007, 0.8632092, 0.013997162, 0.8191612, 0.860294, 0.37202942, 0.8636289, 0.8362355, 0.7109824, 0.7176413, 0.24464548, 0.102661245, 0.74658144, 0.4239543, 0.5733774, 0.8577972, 0.19291015, 0.8194897, 0.31785652, 0.110716894, 0.85063976, 0.8147786, 0.1412438, 0.7996281, 0.31769308, 0.38105726, 0.74812734, 0.82930833, 0.8468449, 0.7733734, 0.62490225, 0.79019415, 0.45460144, 0.04311162, 0.860752, 0.5748544, 0.71009994, 0.8626677, 0.8526612, 0.8583951, 0.8497018, 0.22558764, 0.8528396, 0.72259146, 0.24141572, 0.85409063, 0.84194636, 0.77646905, 0.85632694, 0.60750276, 0.84713244, 0.031922393, 0.55596614, 0.65447044, 0.07817745, 0.5375597, 0.034498006, 0.8549805, 0.7838351, 0.06305994, 0.5401984, 0.09474048, 0.5189585, 0.848932, 0.050538756, 0.009632433, 0.21589063, 0.8517665, 0.75985044, 0.04023951, 0.763067, 0.85390365, 0.83672345, 0.58388436, 0.75896925, 0.77766216, 0.8599289, 0.8403791, 0.8191922, 0.6826753, 0.69599503, 0.5841269, 0.85228074, 0.05966098, 0.81632197, 0.8613215, 0.65837866, 0.2280014, 0.85749865, 0.031507496, 0.6937726, 0.84718055, 0.80835855, 0.82865125, 0.05709949, 0.81115633, 0.83823013, 0.8650707, 0.81207687, 0.7655492, 0.76551664, 0.6469858, 0.8567168, 0.03981763, 0.8253637, 0.62903565, 0.80506784, 0.075800635, 0.659675, 0.8615403, 0.6929098, 0.86543244, 0.4850151, 0.44729075, 0.618352, 0.8380381, 0.29550743, 0.6667345, 0.78617394, 0.668378, 0.8645691, 0.2794047, 0.15559933, 0.8563495, 0.83647233, 0.8589619, 0.8577053, 0.8652772, 0.58887255, 0.85643333, 0.8045263, 0.32990605, 0.8639161, 0.82752186, 0.8205388, 0.8486426, 0.09373288, 0.85855526, 0.78737545, 0.5995838, 0.4691254, 0.0040085907, 0.81395745, 0.050785642, 0.7084419, 0.79993266, 0.75708896, 0.5711626, 0.8354647, 0.009196474, 0.8525993, 0.005221835, 0.8138416, 0.7782898, 0.6247375, 0.79173243, 0.4729398, 0.85151774, 0.17637675, 0.005557923, 0.8543466, 0.8450491, 0.8236125, 0.5243887, 0.5569886, 0.46635935, 0.8308556, 0.82323974, 0.038798667, 0.8181667, 0.8152705, 0.8491938, 0.8385416, 0.79063404, 0.8538552, 0.31646448, 0.7782724, 0.009985952, 0.8149885, 0.8348066, 0.34514254, 0.82757133, 0.821906, 0.8599186, 0.0018443397, 0.11506494, 0.41823035, 0.8142982, 0.6441021, 0.7948562, 0.8608208, 0.8398694, 0.79581773, 0.55562335, 0.7109971, 0.25435308, 0.7101819, 0.85849315, 0.093855456, 0.5675923, 0.78874743, 0.85017896, 0.07742465, 0.8485502, 0.019295223, 0.39812967, 0.64118147, 0.10753317, 0.17645751, 0.83048147, 0.8132737, 0.8308462, 0.65766984, 0.79119605, 0.84452105, 0.6130626, 0.8361564, 0.81941456, 0.0025605054, 0.014758781, 0.29739958, 0.2704238, 0.6853692, 0.8665435, 0.16813555, 0.55080426, 0.8574734, 0.8214397, 0.7984316, 0.43538848, 0.86101896, 0.33843905, 0.26287782, 0.010642711, 0.82180935, 0.8377674, 0.049641747, 0.030743377, 0.8607682, 0.85391116, 0.8453464, 0.7474847, 0.83535916, 0.81966585, 0.8352709, 0.7329732, 0.7127686, 0.19301304, 0.8486389, 0.37926525, 0.006738061, 0.80975264, 0.84705895, 0.701143, 0.38763183, 0.84691733, 0.84319454, 0.8379282, 0.8582745, 0.84162956, 0.4642658, 0.69315755, 0.8148237, 0.86390275, 0.8551753, 0.61194366, 0.7818065, 0.63305646, 0.86271256, 0.85087526, 0.8674485, 0.8589088, 0.024700148, 0.786742, 0.86558825, 0.70808905, 0.8499845, 0.8235934, 0.5773005, 0.83760196, 0.8393334, 0.84291404, 0.6990992, 0.009281526, 0.86283815, 0.008184953, 0.1629029, 0.8100112, 0.007329482, 0.82103753, 0.86144817, 0.83421415, 0.83757377, 0.35878083, 0.854273, 0.54207975, 0.8148577, 0.80124635, 0.78723496, 0.79244566, 0.7888845, 0.78377473, 0.71453524, 0.5573649, 0.80492586, 0.8585297, 0.85799634, 0.479845, 0.8462348, 0.03692541, 0.013932422, 0.55166364, 0.85363954, 0.82776314, 0.8571906, 0.72890866, 0.84271514, 0.7953117, 0.03285798, 0.037095223, 0.6217353, 0.54046667, 0.8617212, 0.17364614, 0.8612279, 0.8355498, 0.414087, 0.85218394, 0.84733254, 0.84431964, 0.85324836, 0.6384647, 0.47317347, 0.8385569, 0.8302335, 0.8477135, 0.848483, 0.82557863, 0.8536176, 0.8050055, 0.17734979, 0.8366912, 0.86631745, 0.86483455, 0.5543605, 0.018483317, 0.82194185, 0.80022717, 0.6890125, 0.7053326, 0.03761256, 0.07753522, 0.6484314, 0.69478375, 0.85443336, 0.85563457, 0.77671224, 0.57983595, 0.85004133, 0.60359937, 0.0030311877]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:29<00:00,  9.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.8341422, 0.12613793, 0.8576452, 0.8566885, 0.84671885, 0.8575311, 0.84274924, 0.0793846, 0.83219993, 0.8535068, 0.85551965, 0.8499258, 0.8434709, 0.8499898, 0.85103434, 0.84640324, 0.85477054, 0.79664224, 0.8550273, 0.8506198, 0.015806949, 0.010430862, 0.36927533, 0.009509663, 0.029927423, 0.8507745, 0.8631282, 0.8429468, 0.0021748375, 0.8420089, 0.08560479, 0.8641538, 0.48489487, 0.25142258, 0.12776917, 0.8396991, 0.85695004, 0.8445097, 0.84370077, 0.8541948, 0.8562248, 0.8623709, 0.8517543, 0.84793603, 0.84275573, 0.84331775, 0.7375057, 0.25290534, 0.7485661, 0.84953004, 0.8533376, 0.7493143, 0.30629075, 0.8611337, 0.856114, 0.8476673, 0.8200375, 0.85774875, 0.86326253, 0.5735194, 0.85880405, 0.8196411, 0.09433267, 0.8213588, 0.84555787, 0.013839574, 0.83858967, 0.85905945, 0.8420442, 0.8289877, 0.8625771, 0.4052716, 0.36470538, 0.85907024, 0.856382, 0.8167418, 0.6382231, 0.8611037, 0.030615874, 0.8408913, 0.7989988, 0.8332019, 0.8608829, 0.8528213, 0.7518338, 0.8302951, 0.8466927, 0.8615969, 0.32562584, 0.8586937, 0.85723007, 0.7766299, 0.75671613, 0.81666017, 0.12649688, 0.73425126, 0.8418782, 0.007143888, 0.859844, 0.8524526, 0.7679946, 0.83320236, 0.8525017, 0.8506727, 0.85847104, 0.84989405, 0.83555716, 0.85615027, 0.86063427, 0.024795137, 0.7077883, 0.85707176, 0.46781743, 0.8311705, 0.02910079, 0.2219015, 0.0150212, 0.4459033, 0.8535313, 0.85674983, 0.86349076, 0.003279036, 0.8611478, 0.80455554, 0.86295927, 0.8549292, 0.85889137, 0.85685694, 0.849453, 0.8543217, 0.38139313, 0.85129136, 0.84739166, 0.8653661, 0.0021493726, 0.5523234, 0.86108464, 0.83875513, 0.85171247, 0.85752773, 0.8465129, 0.8578954, 0.8496286, 0.0745239, 0.009178898, 0.8025017, 0.07177896, 0.53041524, 0.8506646, 0.8570449, 0.81868327, 0.86043626, 0.64038765, 0.6679035, 0.6069465, 0.8377687, 0.8510988, 0.8647042, 0.84641594, 0.7391951, 0.85139287, 0.85682344, 0.8432056, 0.8594432, 0.85787976, 0.8507082, 0.84289914, 0.8583008, 0.7073994, 0.8523442, 0.8580056, 0.844714, 0.8490616, 0.7299245, 0.18976705, 0.8473617, 0.8483523, 0.86025, 0.8072772, 0.8614809, 0.7125533, 0.06639402, 0.8462588, 0.12373736, 0.83712095, 0.6446907, 0.84827435, 0.85300404, 0.86079735, 0.8571536, 0.14715067, 0.82471377, 0.84346247, 0.84253824, 0.8509219, 0.8493109, 0.8128308, 0.78330415, 0.09552019, 0.8524377, 0.8562105, 0.1467747, 0.7824378, 0.7904576, 0.7234238, 0.8315735, 0.85876745, 0.8381777, 0.8528885, 0.8557463, 0.040214427, 0.08729685, 0.80588, 0.7069647, 0.86627984, 0.6919088, 0.82397133, 0.56716996, 0.84920067, 0.8644689, 0.8459088, 0.68924487, 0.8449944, 0.85245985, 0.11933495, 0.3455226, 0.0017279199, 0.8564365, 0.83879334, 0.83766764, 0.86630815, 0.0039745285, 0.83936095, 0.85085374, 0.6296656, 0.77760404, 0.6887285, 0.86223745, 0.8565048, 0.6547812, 0.8568549, 0.84172714, 0.8544491, 0.8458157, 0.72259635, 0.82564175, 0.855919, 0.8581486, 0.008468002, 0.8605342, 0.85008234, 0.8406736, 0.8459636, 0.855186, 0.5598399, 0.859431, 0.856131, 0.83253676, 0.8401257, 0.8243786, 0.71692836, 0.8449038, 0.8086975, 0.4957484, 0.22948577, 0.85712326, 0.8538915, 0.8591872, 0.20096454, 0.85499316, 0.8531577, 0.822132, 0.6708462, 0.84975845, 0.8333487, 0.30630958, 0.86445487, 0.8294142, 0.86669874, 0.8595832, 0.77748615, 0.8596717, 0.86037314, 0.8640161, 0.3035285, 0.83375496, 0.83383965, 0.08206997, 0.062698014, 0.8375704, 0.8468765, 0.8546408, 0.004293611, 0.85826015, 0.75794303, 0.8508604, 0.0476424, 0.8639334, 0.8556097, 0.8520329, 0.8538431, 0.85543895, 0.8124515, 0.8591885, 0.8515501, 0.86358327, 0.111587174, 0.8620761, 0.8445821, 0.8509864, 0.86072063, 0.8528232, 0.8470371, 0.85644263, 0.8587563, 0.8606572, 0.8601661, 0.12669861, 0.39416423, 0.8518217, 0.8359512, 0.8325866, 0.8534343, 0.845139, 0.8125045, 0.79500526, 0.823901, 0.8486884, 0.8543537, 0.85601664, 0.8641787, 0.565882, 0.25285992, 0.8554743, 0.0027910967, 0.84452766, 0.854495, 0.81546265, 0.850029, 0.5132476, 0.8581071, 0.8496668, 0.7752648, 0.8593725, 0.85759234, 0.8096386, 0.84446627, 0.860466, 0.004034179, 0.8508233, 0.8654134, 0.8525699, 0.8485496, 0.7567994, 0.84559894, 0.8511862, 0.8620246, 0.57689404, 0.8550228, 0.83325356, 0.85697496, 0.6609275, 0.79438627, 0.84978545, 0.8493673, 0.0018439079, 0.86424935, 0.8626265, 0.85530776, 0.83653325, 0.84318846, 0.84171623, 0.8494611, 0.8566953, 0.86217976, 0.85779506, 0.66010606, 0.8618682, 0.8420335, 0.8553855, 0.86289394, 0.8574314, 0.8533092, 0.8598997, 0.6197068, 0.816013, 0.8481855, 0.8439036, 0.8578543, 0.85736567, 0.85832286, 0.865507, 0.01681286, 0.8575702, 0.82884854, 0.8442631, 0.8499121, 0.038955394, 0.8577254, 0.8545215, 0.84746253, 0.83245695, 0.8545118, 0.851269, 0.0055650757, 0.77989674, 0.72309345, 0.6893069, 0.63286704, 0.8460588, 0.8562266, 0.8511647, 0.82055825, 0.8364517, 0.803595, 0.860165, 0.85991704, 0.8605463, 0.849942, 0.86596036, 0.81385106, 0.8597031, 0.7570175, 0.8118334, 0.55607027, 0.8529663, 0.86018497, 0.8564576, 0.84256685, 0.66681236, 0.85978407, 0.84796137, 0.8531336, 0.69989204, 0.8484569, 0.0028853058, 0.8428414, 0.841768, 0.8500228, 0.8323105, 0.853177, 0.1940881, 0.85734016, 0.83752936, 0.8480574, 0.7229124, 0.8496774, 0.83875495, 0.86053526, 0.84512717, 0.8334089, 0.8558099, 0.8575097, 0.85988456, 0.8113659, 0.436518, 0.86055344, 0.8543713, 0.8411372, 0.85542, 0.010992721, 0.49457407, 0.81461865, 0.82050055, 0.80879796, 0.74744505, 0.81197405, 0.8487817, 0.8513507, 0.8579659, 0.8526067, 0.8616142, 0.8311488, 0.7055137, 0.7885451, 0.75376284, 0.8651421, 0.20866479, 0.8160402, 0.8172149, 0.026631389, 0.86128736, 0.0875367, 0.7824349, 0.7883939, 0.0019526202, 0.83056, 0.860453, 0.7353106, 0.86184543, 0.8580337, 0.04132124, 0.6256894, 0.8439887, 0.83949095, 0.8489461, 0.5576157, 0.84912604, 0.8054431, 0.8535597]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTc2SQVhuv2Q",
        "colab_type": "code",
        "outputId": "b7e252ee-78e5-44c7-8d8e-f3341ef78102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "k = 10\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "# MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, test_qid_rel, k)\n",
        "MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, toy_test_label, k)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 333 queries: 0.16666666666666666\n",
            "\n",
            "MRR@10 for 333 queries: 0.08333333333333333\n",
            "\n",
            "Average Precision@1: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcfbXb5eBcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'rank/rank_pairwise_001.pickle', qid_pred_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}