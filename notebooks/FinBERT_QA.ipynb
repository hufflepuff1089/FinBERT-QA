{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinBERT-QA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "crBA7JjXT9MM",
        "colab_type": "code",
        "outputId": "34126bba-dfa3-4ce0-e7b8-ebbf27b97d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pyserini==0.8.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 30.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 851kB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 1.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 1.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 1.3MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 1.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 1.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 59.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 64.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 70.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=a2c2861ae8d8f927e2122a11690601a157d93610df5ca359bed8327db30ee50c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n",
            "Collecting pyserini==0.8.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/f4/45a9175211da4b7d4aed77af707eab938fc068d6124fc6673da748bc74bd/pyserini-0.8.1.0-py3-none-any.whl (57.7MB)\n",
            "\u001b[K     |████████████████████████████████| 57.7MB 57kB/s \n",
            "\u001b[?25hCollecting pyjnius\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/4f/e3d9f4bb53f7f1854f81a279c274c4ad8537e4d71117258515158403bc10/pyjnius-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 74.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from pyserini==0.8.1.0) (0.29.16)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pyjnius->pyserini==0.8.1.0) (1.12.0)\n",
            "Installing collected packages: pyjnius, pyserini\n",
            "Successfully installed pyjnius-1.2.1 pyserini-0.8.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTqNRzy8T-It",
        "colab_type": "code",
        "outputId": "5e0cf42b-e2eb-4092-81d8-dbcb839e5a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.functional import softmax\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, BertConfig\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "from pyserini.search import pysearch\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz6OttiShXhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download FiQA data and models\n",
        "!wget https://www.dropbox.com/s/tm9zqqmxnwx1esm/finbert-qa.zip?dl=1\n",
        "!unzip finbert-qa.zip?dl=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxMzCR0TzKul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {'max_seq_len': 512,\n",
        "          'batch_size': 16,\n",
        "          'learning_rate': 3e-6,\n",
        "          'weight_decay': 0.01,\n",
        "          'n_epochs': 2,\n",
        "          'num_warmup_steps': 10000}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "758ZQWNpo_bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def load_pickle(path):\n",
        "    \"\"\"Load pickle file.\n",
        "    ----------\n",
        "    Arguments:\n",
        "        path: str file path\n",
        "    \"\"\"\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB-cqergndhX",
        "colab_type": "code",
        "outputId": "137d0bee-2cf1-4efc-fae0-a0dc2d77ac4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Dictionary mapping docid and qid to raw text\n",
        "docid_to_text = load_pickle('finbert-qa/data/docid_to_text.pickle')\n",
        "qid_to_text = load_pickle('finbert-qa/data/qid_to_text.pickle')\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list contraining [qid, list of pos docid, list of candidate docid]\n",
        "train_set = load_pickle('finbert-qa/data/train_set_50.pickle')\n",
        "valid_set = load_pickle('finbert-qa/data/valid_set_50.pickle')\n",
        "test_set = load_pickle('finbert-qa/data/test_set_50.pickle')\n",
        "\n",
        "# Labels\n",
        "labels = load_pickle('finbert-qa/data/labels.pickle')\n",
        "\n",
        "print(\"Number of questions in the training set: {}\".format(len(train_set)))\n",
        "print(\"Number of questions in the validation set: {}\".format(len(valid_set)))\n",
        "print(\"Number of questions in the test set: {}\".format(len(test_set)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of questions in the training set: 5676\n",
            "Number of questions in the validation set: 631\n",
            "Number of questions in the test set: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpL9adVEhIYN",
        "colab_type": "code",
        "outputId": "5d142d76-d50a-42ba-dce6-7c33139ea431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('\\nLoading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_5ElnCj99XDp"
      },
      "source": [
        "## **Prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxvRLo_vnDj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input_data(dataset, max_seq_len):\n",
        "    \"\"\"Creates input parameters for training and validation.\n",
        "\n",
        "    Returns:\n",
        "        input_ids: List of lists\n",
        "                Each element contains a list of padded/truncated numericalized\n",
        "                tokens of the sequences including [CLS] and [SEP] tokens\n",
        "                e.g. [[101, 2054, 2003, 102, 2449, 1029, 102], ...]\n",
        "        token_type_ids: List of lists\n",
        "                Each element contains a list of segment token indices to\n",
        "                indicate first (question) and second (answer) parts of the inputs.\n",
        "                0 corresponds to a question token, 1 corresponds an answer token\n",
        "                e.g. [[0, 0, 0, 0, 1, 1, 1], ...]\n",
        "        att_masks: List of lists\n",
        "                Each element contains a list of mask values to avoid\n",
        "                performing attention on padding token indices.\n",
        "                1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n",
        "                e.g. [[1, 1, 1, 1, 1, 1, 1], ...]\n",
        "        labels: List of 1's and 0's incidating relevacy of answer\n",
        "    -----------------\n",
        "    Arguements:\n",
        "        dataset: List of lists in the form of [qid, [pos ans], [ans cands]]\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    token_type_ids = []\n",
        "    att_masks = []\n",
        "    labels = []\n",
        "\n",
        "    for i, seq in enumerate(tqdm(dataset)):\n",
        "        qid, ans_labels, cands = seq[0], seq[1], seq[2]\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "        # For each answer in the candidates\n",
        "        for docid in cands:\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "            # Encode the sequence using BERT tokenizer\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text,\n",
        "                                                max_length=max_seq_len,\n",
        "                                                pad_to_max_length=True,\n",
        "                                                return_token_type_ids=True,\n",
        "                                                return_attention_mask = True)\n",
        "            # Get parameters\n",
        "            input_id = encoded_seq['input_ids']\n",
        "            token_type_id = encoded_seq['token_type_ids']\n",
        "            att_mask = encoded_seq['attention_mask']\n",
        "\n",
        "            # If an answer is in the list of relevant answers assign\n",
        "            # positive label\n",
        "            if docid in ans_labels:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "\n",
        "            # Each parameter list has the length of the max_seq_len\n",
        "            assert len(input_id) == max_seq_len, \"Input id dimension incorrect!\"\n",
        "            assert len(token_type_id) == max_seq_len, \"Token type id dimension incorrect!\"\n",
        "            assert len(att_mask) == max_seq_len, \"Attention mask dimension incorrect!\"\n",
        "\n",
        "            input_ids.append(input_id)\n",
        "            token_type_ids.append(token_type_id)\n",
        "            att_masks.append(att_mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    return input_ids, token_type_ids, att_masks, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJKHcED_l_QN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataloader(dataset, type, max_seq_len, batch_size):\n",
        "    \"\"\"Creates train and validation DataLoaders with input_ids,\n",
        "    token_type_ids, att_masks, and labels\n",
        "\n",
        "    Returns:\n",
        "        train_dataloader: DataLoader object\n",
        "        validation_dataloader: DataLoader object\n",
        "\n",
        "    -----------------\n",
        "    Arguements:\n",
        "        dataset: List of lists in the form of [qid, [pos ans], [ans cands]]\n",
        "        type: str - 'train' or 'validation'\n",
        "        max_seq_len: int\n",
        "        batch_size: int\n",
        "    \"\"\"\n",
        "    input_id, token_type_id, \\\n",
        "    att_mask, label = get_input_data(dataset, max_seq_len)\n",
        "\n",
        "    # Convert all inputs to torch tensors\n",
        "    input_ids = torch.tensor(input_id)\n",
        "    token_type_ids = torch.tensor(token_type_id)\n",
        "    att_masks = torch.tensor(att_mask)\n",
        "    labels = torch.tensor(label)\n",
        "\n",
        "    # Create the DataLoader for our training set.\n",
        "    data = TensorDataset(input_ids, token_type_ids, att_masks, labels)\n",
        "    if type == \"train\":\n",
        "        sampler = RandomSampler(data)\n",
        "    else:\n",
        "        sampler = SequentialSampler(data)\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "    \n",
        "    return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUEW5GC5y6r4",
        "colab_type": "code",
        "outputId": "77b2afb8-c574-4621-9443-cbd5dc6e370e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Get dataloaders\n",
        "train_dataloader = get_dataloader(train_set, 'train', \n",
        "                                  config['max_seq_len'], \n",
        "                                  config['batch_size'])\n",
        "validation_dataloader = get_dataloader(valid_set, 'validation', \n",
        "                                       config['max_seq_len'], \n",
        "                                       config['batch_size'])\n",
        "\n",
        "print(\"\\n\\nSize of the training DataLoader: {}\".format(len(train_dataloader)))\n",
        "print(\"Size of the validation DataLoader: {}\".format(len(validation_dataloader)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5676/5676 [20:26<00:00,  4.63it/s]\n",
            "100%|██████████| 631/631 [02:15<00:00,  4.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Size of the training DataLoader: 17738\n",
            "Size of the validation DataLoader: 1972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U70_1Juv9wN4"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AXZjwPO90ok",
        "colab_type": "code",
        "outputId": "dafed0c1-d805-4709-db16-49cb6747f20f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load BertForSequenceClassification - pretrained BERT model \n",
        "# with a single linear classification layer on top\n",
        "\n",
        "model_path = \"finbert-qa/model/bert-qa\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, cache_dir=None, num_labels=2)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZLwjs5kZeLj",
        "colab_type": "text"
      },
      "source": [
        "## **Training/Validation methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpDWdi2S7_2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(preds, labels):\n",
        "    \"\"\"Compute the accuracy of binary predictions.\n",
        "\n",
        "    Returns:\n",
        "        accuracy: float\n",
        "    -----------------\n",
        "    Arguments:\n",
        "        preds: Numpy list with two columns of probabilities for each label\n",
        "        labels: List of labels\n",
        "    \"\"\"\n",
        "    # Get the label (column) with the higher probability\n",
        "    predictions = np.argmax(preds, axis=1).flatten()\n",
        "    labels = labels.flatten()\n",
        "    # Compute accuracy\n",
        "    accuracy = np.sum(predictions == labels) / len(labels)\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivNsu5AN8bhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataloader, optimizer, scheduler):\n",
        "    \"\"\"Trains the model and returns the average loss and accuracy.\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: Float\n",
        "        avg_acc: Float\n",
        "    ----------\n",
        "    Arguements:\n",
        "        model: Torch model\n",
        "        train_dataloader: DataLoader object\n",
        "        optimizer: Optimizer object\n",
        "        scheduler: Scheduler object\n",
        "    \"\"\"\n",
        "    # Cumulated Training loss and accuracy\n",
        "    total_loss = 0\n",
        "    train_accuracy = 0\n",
        "    # Track the number of steps\n",
        "    num_steps = 0\n",
        "    # Set model in train mode\n",
        "    model.train()\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # Get tensors and move to gpu\n",
        "        # batch contains four PyTorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: token_type_ids\n",
        "        #   [2]: attention masks\n",
        "        #   [3]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_token_type_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        model.zero_grad()\n",
        "        # Forward pass: the model will return the loss and the logits\n",
        "        outputs = model(b_input_ids,\n",
        "                        token_type_ids = b_token_type_ids,\n",
        "                        attention_mask = b_input_mask,\n",
        "                        labels = b_labels)\n",
        "\n",
        "        # Get loss and predictions\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for a batch\n",
        "        tmp_accuracy = get_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        train_accuracy += tmp_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        num_steps += 1\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    avg_acc = train_accuracy/num_steps\n",
        "\n",
        "    return avg_loss, avg_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9-Neo9v9C-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, validation_dataloader):\n",
        "    \"\"\"Validates the model and returns the average loss and accuracy.\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: Float\n",
        "        avg_acc: Float\n",
        "    ----------\n",
        "    Arguements:\n",
        "        model: Torch model\n",
        "        validation_dataloader: DataLoader object\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Cumulated Training loss and accuracy\n",
        "    total_loss = 0\n",
        "    eval_accuracy = 0\n",
        "    # Track the number of steps\n",
        "    num_steps = 0\n",
        "\n",
        "    # For each batch of the validation data\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        # Move tensors from batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from the dataloader\n",
        "        b_input_ids, b_token_type_ids, b_input_masks, b_labels = batch\n",
        "        # Don't to compute or store gradients\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids = b_token_type_ids,\n",
        "                            attention_mask = b_input_masks,\n",
        "                            labels= b_labels)\n",
        "        # Get loss and logits\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = get_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of steps\n",
        "        num_steps += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate loss and accuracy\n",
        "    avg_loss = total_loss / len(validation_dataloader)\n",
        "    avg_acc = eval_accuracy/num_steps\n",
        "\n",
        "    return avg_loss, avg_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YmEFBmayZpFG"
      },
      "source": [
        "## **Fine-tune FinBERT-QA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CINMiZkJ-j9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), \n",
        "                  lr = config['learning_rate'], \n",
        "                  weight_decay = config['weight_decay'])\n",
        "\n",
        "n_epochs = config['n_epochs']\n",
        "\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * n_epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = config['num_warmup_steps'],\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q9TZ6b__Ubq",
        "colab_type": "code",
        "outputId": "f1484e12-e30d-46e1-8eb4-fb1306264b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "# Train and validate the model and print the average loss and accuracy.\n",
        "\n",
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Evaluate training loss\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, scheduler)\n",
        "    # Evaluate validation loss\n",
        "    valid_loss, valid_acc = validate(model, validation_dataloader)\n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'finbert-qa/model/' + \\\n",
        "        str(epoch+1)+ '_finbert-qa.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17738/17738 [4:12:50<00:00,  1.17it/s]\n",
            "100%|██████████| 1972/1972 [08:55<00:00,  3.68it/s]\n",
            "  0%|          | 0/17738 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 1:\n",
            "\t Train Loss: 0.09 | Train Accuracy: 97.8%\n",
            "\t Validation Loss: 0.078 | Validation Accuracy: 98.06%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 15589/17738 [3:42:09<30:39,  1.17it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4lcUemVFCVHN"
      },
      "source": [
        "## **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS4qwt6uAe9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rank(model, test_set, max_seq_len):\n",
        "    \"\"\"Re-ranks the candidates answers for each question.\n",
        "\n",
        "    Returns:\n",
        "        qid_pred_rank: Dictionary\n",
        "            key - qid\n",
        "            value - list of re-ranked candidates\n",
        "    -------------------\n",
        "    Arguments:\n",
        "        model - PyTorch model\n",
        "    \"\"\"\n",
        "    # Initiate empty dictionary\n",
        "    qid_pred_rank = {}\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # For each element in the test set\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "        # question id, list of rel answers, list of candidates\n",
        "        qid, label, cands = seq[0], seq[1], seq[2]\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "        # Convert list to numpy array\n",
        "        cands_id = np.array(cands)\n",
        "        # Empty list for the probability scores of relevancy\n",
        "        scores = []\n",
        "\n",
        "        # For each answer in the candidates\n",
        "        for docid in cands:\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "            # Create inputs for the model\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text,\n",
        "                                            max_length=max_seq_len,\n",
        "                                            pad_to_max_length=True,\n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "            # Numericalized, padded, clipped seq with special tokens\n",
        "            input_ids = torch.tensor([encoded_seq['input_ids']]).to(device)\n",
        "            # Specify question seq and answer seq\n",
        "            token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(device)\n",
        "            # Sepecify which position is part of the seq which is padded\n",
        "            att_mask = torch.tensor([encoded_seq['attention_mask']]).to(device)\n",
        "\n",
        "            # Don't calculate gradients\n",
        "            with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions for each QA pair\n",
        "                outputs = model(input_ids,\n",
        "                                token_type_ids=token_type_ids,\n",
        "                                attention_mask=att_mask)\n",
        "            # Get the predictions\n",
        "            logits = outputs[0]\n",
        "            # Apply activation function\n",
        "            pred = softmax(logits, dim=1)\n",
        "            # Move logits and labels to CPU\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "            # Append relevant scores to list (where label = 1)\n",
        "            scores.append(pred[:,1][0])\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "        # Get the list of docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[qid] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnuLcNmXBx9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from finbert-qa.evaluate import *\n",
        "\n",
        "trained_model_path = \"finbert-qa/model/finbert-qa/2_finbert-qa-50_512_16_3e6.pt\"\n",
        "# Load model\n",
        "model.load_state_dict(torch.load(trained_model_path), strict=False)\n",
        "\n",
        "# Get rank\n",
        "qid_pred_rank = get_rank(model, test_set, config['max_seq_len'])\n",
        "\n",
        "k = 10\n",
        "num_q = len(test_set)\n",
        "\n",
        "# Evaluate\n",
        "MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, labels, k)\n",
        "\n",
        "print(\"Average nDCG@{0} for {1} queries: {2:.3f}\".format(k, num_q, average_ndcg))\n",
        "print(\"MRR@{0} for {1} queries: {2:.3f}\".format(k, num_q, MRR))\n",
        "print(\"Average Precision@1 for {0} queries: {1:.3f}\".format(num_q, precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iLTS16TrDNOv"
      },
      "source": [
        "## **Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hzi_sPODb0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, q_text, cands, max_seq_len):\n",
        "    \"\"\"Re-ranks the candidates answers for each question.\n",
        "\n",
        "    Returns:\n",
        "        ranked_ans: list of re-ranked candidate docids\n",
        "        sorted_scores: list of relevancy scores of the answers\n",
        "    -------------------\n",
        "    Arguments:\n",
        "        model - PyTorch model\n",
        "        q_text - str - query\n",
        "        cands -List of retrieved candidate docids\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # Convert list to numpy array\n",
        "    cands_id = np.array(cands)\n",
        "    # Empty list for the probability scores of relevancy\n",
        "    scores = []\n",
        "    # For each answer in the candidates\n",
        "    for docid in cands:\n",
        "        # Map the docid to text\n",
        "        ans_text = docid_to_text[docid]\n",
        "        # Create inputs for the model\n",
        "        encoded_seq = tokenizer.encode_plus(q_text, ans_text,\n",
        "                                            max_length=max_seq_len,\n",
        "                                            pad_to_max_length=True,\n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "        # Numericalized, padded, clipped seq with special tokens\n",
        "        input_ids = torch.tensor([encoded_seq['input_ids']]).to(device)\n",
        "        # Specify question seq and answer seq\n",
        "        token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(device)\n",
        "        # Sepecify which position is part of the seq which is padded\n",
        "        att_mask = torch.tensor([encoded_seq['attention_mask']]).to(device)\n",
        "        # Don't calculate gradients\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions for each QA pair\n",
        "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=att_mask)\n",
        "        # Get the predictions\n",
        "        logits = outputs[0]\n",
        "        # Apply activation function\n",
        "        pred = softmax(logits, dim=1)\n",
        "        # Move logits and labels to CPU\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "        # Append relevant scores to list (where label = 1)\n",
        "        scores.append(pred[:,1][0])\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "        # Get the list of docid from the sorted indices\n",
        "        ranked_ans = list(cands_id[sorted_index])\n",
        "        sorted_scores = list(np.around(sorted(scores, reverse=True),decimals=3))\n",
        "\n",
        "    return ranked_ans, sorted_scores\n",
        "\n",
        "def get_rel(labels, cands):\n",
        "    \"\"\"Get relevant positions of the hits.\n",
        "\n",
        "    Returns: List of 0's and 1's incidating a relevant answer\n",
        "    -------------------\n",
        "    Arguments:\n",
        "        labels: List of relevant docids\n",
        "        cands: List of candidate docids\n",
        "    \"\"\"\n",
        "    rel = []\n",
        "    for cand in cands:\n",
        "        if cand in labels:\n",
        "            rel.append(1)\n",
        "        else:\n",
        "            rel.append(0)\n",
        "\n",
        "    return rel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkCnb0D1DOX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIQA_INDEX = \"fiqa/lucene-index-fiqa\"\n",
        "\n",
        "searcher = pysearch.SimpleSearcher(FIQA_INDEX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCT6vXZNEPdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq = test_set[91]\n",
        "qid, label, cands = seq[0], seq[1], seq[2]\n",
        "q_text = qid_to_text[qid]\n",
        "query = q_text\n",
        "print(query)\n",
        "# query = \"Which investments are the best?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18uG7jF2FPw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hits = searcher.search(query, k=50)\n",
        "self.cands = []\n",
        "\n",
        "for i in range(0, len(hits)):\n",
        "    cands.append(int(hits[i].docid))\n",
        "\n",
        "trained_model_path = \"fiqa/model/finbert-qa/2_finbert-qa-50_512_16_3e6.pt\"\n",
        "# Load model\n",
        "model.load_state_dict(torch.load(trained_model_path), strict=False)\n",
        "\n",
        "rank, scores = predict(model, query, cands)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GbSeLovF5SU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 5\n",
        "\n",
        "print(\"Question: \\n\\t{}\\n\".format(query))\n",
        "print(\"Top-{} Answers: \\n\".format(k))\n",
        "for i in range(0, 10):\n",
        "    print(\"{}.\\t{}\\n\".format(i+1, docid_to_text[rank[i]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9-ROSZOEZ07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cand_rel = get_rel(label, cands)\n",
        "print(\"Retriever: \\n\\t Ranking: {}\\n\\n\\t {}\".format(cands[:10], cand_rel[:10]))\n",
        "pred_rel = get_rel(label, rank)\n",
        "print(\"Re-ranker: \\n\\t Ranking: {}\\n\\n\\t Scores: {}\\n\\n\\t {}\".format(rank[:10], scores[:10], pred_rel[:10]))\n",
        "print(\"Label: \\n\\t{}\".format(label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPX9XA9tFkzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Question: \\n\\t{}\\n\".format(query))\n",
        "print(\"Answer Re-ranker\\n\")\n",
        "print(\"Answer: \\n\\t{}\\n\".format(docid_to_text[rank[0]]))\n",
        "print(\"Answer Retriever\\n\")\n",
        "print(\"Answer: \\n\\t{}\\n\".format(docid_to_text[cands[0]]))\n",
        "print(\"label: \\n\\t{}\".format(docid_to_text[label[1]]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}