{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qa_lstm2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX2UXHqaWsoH",
        "colab_type": "code",
        "outputId": "edf8308a-1352-41c5-99d4-55f5ccadf72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh0V2HoCmrer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NOakGS8WnB",
        "colab_type": "code",
        "outputId": "483f5f10-b4ec-4519-e9b2-f8cb8eaa6911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data \n",
        "import torchtext\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import csv\n",
        "from itertools import islice\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import regex as re\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import torch.utils.data as data\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f84404b7df0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20MlvHGSzV3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/fiqa/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwqmca0M1PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "    \n",
        "def pre_process(doc):\n",
        "    doc = str(doc)\n",
        "    x = re.sub('[…“”%!&\"@#()\\-\\*\\+,/:;<=>?@[\\]\\^_`{\\}~]', ' ', doc)\n",
        "    y = re.sub('[\\.\\']', \"\", x)\n",
        "    z = y.lower()\n",
        "    return z\n",
        "\n",
        "def remove_empty(test_set):\n",
        "    for index, row in enumerate(test_set):\n",
        "        for doc in row[1]:\n",
        "            if doc in empty_docs:\n",
        "                del test_set[index]\n",
        "    return test_set\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_pickle(path, data):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def pad_seq(seq, max_seq_len):\n",
        "    # Pad each seq to be the same length to process in batch.\n",
        "    # pad_token = 0\n",
        "    if len(seq) >= max_seq_len:\n",
        "        seq = seq[:max_seq_len]\n",
        "    else:\n",
        "        seq += [0]*(max_seq_len - len(seq))\n",
        "    return seq\n",
        "\n",
        "def vectorize(seq, vocab, max_seq_len):\n",
        "    # Map tokens in seq to idx\n",
        "    seq_idx = [vocab[token] for token in seq]\n",
        "    # Pad seq idx\n",
        "    # padded_seq_idx = [pad_seq(seq_idx, max_seq_len)]\n",
        "    padded_seq_idx = pad_seq(seq_idx, max_seq_len)\n",
        "\n",
        "    # return torch.tensor(padded_seq_idx)\n",
        "    return padded_seq_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNmKP8_M0x0q",
        "colab_type": "text"
      },
      "source": [
        "**Load pickle files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAYWDxGXyda4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict mapping of token to idx\n",
        "vocab = load_pickle(path + 'data/word2index.pickle')\n",
        "# dict mapping of docid to doc text\n",
        "docid_to_tokenized_text = load_pickle(path + 'data/docid_to_tokenized_text.pickle')\n",
        "\n",
        "# dict mapping of qid to question text\n",
        "qid_to_tokenized_text = load_pickle(path + 'data/qid_to_tokenized_text.pickle')\n",
        "\n",
        "test_qid_rel = load_pickle(path + \"data/qid_rel_test.pickle\")\n",
        "\n",
        "train_set = load_pickle(path + 'data/train_set_50.pickle')\n",
        "valid_set = load_pickle(path + 'data/valid_set_50.pickle')\n",
        "test_set = load_pickle(path + 'data/test_set_50.pickle')\n",
        "test_set_500 = load_pickle(path + 'data/test_set_500.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpORCyaAHn0-",
        "colab_type": "code",
        "outputId": "8f360f55-49ff-4256-a43d-a38b4ea127d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Vocabulary size: {}\".format(len(vocab)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 85034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMILoqqOEgug",
        "colab_type": "code",
        "outputId": "9fd97055-165a-4df2-82f4-39f367ee27bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 5676\n",
            "Number of validation samples: 631\n",
            "Number of test samples: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCUqSDFR8J1t",
        "colab_type": "text"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl3hClS13Gz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "# n_epochs = 20\n",
        "# batch_size = 64\n",
        "hidden_size = 256\n",
        "# max_seq_len = 128\n",
        "dropout = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-VHSXrLSfF",
        "colab_type": "code",
        "outputId": "f54502ef-88f5-42db-8cb6-b03f07f0e4cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "emb = torchtext.vocab.GloVe(\"6B\", dim=emb_dim)\n",
        "# dictionary mapping of word idx to glove vectors\n",
        "emb_weights = np.zeros((vocab_size, emb_dim))\n",
        "words_found = 0\n",
        "print(\"Embedding dim: {}\".format(emb_weights.shape))\n",
        "\n",
        "for token, idx in vocab.items():\n",
        "    # emb.stoi is a dict of token to idx mapping\n",
        "    if token in emb.stoi:\n",
        "        emb_weights[idx] = emb[token]\n",
        "        words_found += 1\n",
        "\n",
        "print(\"vocab size: \", vocab_size)\n",
        "print(words_found, \" words are found in GloVe\")\n",
        "\n",
        "# Convert numpy matrix to tensor\n",
        "emb_weights = torch.from_numpy(emb_weights).float()\n",
        "\n",
        "emb_weights.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding dim: (85034, 100)\n",
            "vocab size:  85034\n",
            "50456  words are found in GloVe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([85034, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8C6zB6MP4bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emb_layer(emb_weights):\n",
        "    vocab_size, emb_dim = emb_weights.shape\n",
        "    emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "    emb_layer.load_state_dict({'weight': emb_weights})\n",
        "\n",
        "    return emb_layer\n",
        "\n",
        "def hinge_loss(pos_sim, neg_sim):\n",
        "    margin = 0.2\n",
        "\n",
        "    loss = torch.max(torch.tensor(0, dtype=torch.float).to(device), margin - pos_sim + neg_sim)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxdQU4qJTV73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QA_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, dropout):\n",
        "\n",
        "        super(QA_LSTM, self).__init__()\n",
        "\n",
        "        # Shape - (max_seq_len, emb_dim)\n",
        "        self.embedding = create_emb_layer(emb_weights)\n",
        "\n",
        "        self.shared_lstm = nn.LSTM(emb_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, q, a):\n",
        "        # embedding\n",
        "        q = self.embedding(q) # (bs, L, E)\n",
        "        a = self.embedding(a) # (bs, L, E)\n",
        "\n",
        "        # LSTM\n",
        "        q, (hidden, cell) = self.shared_lstm(q) # (bs, L, 2H)\n",
        "        a, (hidden, cell) = self.shared_lstm(a) # (bs, L, 2H)\n",
        "\n",
        "        # Output shape (batch size, seq_len, num_direction * hidden_size)\n",
        "        # There are n of word level biLSTM representations for the seq where n is the number of seq len\n",
        "        # Use max pooling to generate the best representation\n",
        "        q = torch.max(q, 1)[0]  \n",
        "        a = torch.max(a, 1)[0] # (bs, 2H)\n",
        "\n",
        "        q = self.dropout(q)\n",
        "        a = self.dropout(a)\n",
        "\n",
        "        return self.cos(q, a) # (bs,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDOXLjeq21io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lstm_input_data(dataset, max_seq_len):\n",
        "    q_input_ids = []\n",
        "    pos_input_ids = []\n",
        "    neg_input_ids = []\n",
        "\n",
        "    for i, seq in enumerate(tqdm(dataset)):\n",
        "        qid, ans_labels, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        filtered_cands = list(set(cands)-set(ans_labels))\n",
        "\n",
        "        pos_docid = random.choice(ans_labels)\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_tokenized_text[qid]\n",
        "        q_input_id = vectorize(q_text, vocab, max_seq_len)\n",
        "\n",
        "        for neg_docid in filtered_cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            pos_ans_text = docid_to_tokenized_text[pos_docid]\n",
        "            neg_ans_text = docid_to_tokenized_text[neg_docid]\n",
        "\n",
        "            pos_input_id = vectorize(pos_ans_text, vocab, max_seq_len)\n",
        "            neg_input_id = vectorize(neg_ans_text, vocab, max_seq_len)\n",
        "\n",
        "            q_input_ids.append(q_input_id)\n",
        "            pos_input_ids.append(pos_input_id)\n",
        "            neg_input_ids.append(neg_input_id)\n",
        "\n",
        "    return q_input_ids, pos_input_ids, neg_input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgp4F2lq4Cfa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fadd38e4-05c8-4e37-8042-79058a5f0c82"
      },
      "source": [
        "train_q_input, train_pos_input, train_neg_input = get_lstm_input_data(train_set, 256)\n",
        "valid_q_input, valid_pos_input, valid_neg_input = get_lstm_input_data(valid_set, 256)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5676/5676 [00:15<00:00, 360.06it/s]\n",
            "100%|██████████| 631/631 [00:01<00:00, 449.25it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8MKXqPoOdoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "346a777b-07bf-421d-ca86-eff7275268c2"
      },
      "source": [
        "print(len(train_q_input))\n",
        "print(len(valid_q_input))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "277827\n",
            "30874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gVtEWwFGLCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# valid_q_input = valid_q_input[50:55]\n",
        "# valid_pos_input = valid_pos_input[50:55]\n",
        "# valid_neg_input = valid_neg_input[50:55]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA9p8R5X-g8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_q_inputs = torch.tensor(train_q_input)\n",
        "train_pos_inputs = torch.tensor(train_pos_input)\n",
        "train_neg_inputs = torch.tensor(train_neg_input)\n",
        "\n",
        "valid_q_inputs = torch.tensor(valid_q_input)\n",
        "valid_pos_inputs = torch.tensor(valid_pos_input)\n",
        "valid_neg_inputs = torch.tensor(valid_neg_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH1cmxqa-npW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_q_inputs, train_pos_inputs, train_neg_inputs)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(valid_q_inputs, valid_pos_inputs, valid_neg_inputs)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5As531eJCLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataloader, optimizer):\n",
        "\n",
        "    # Cumulated Training loss\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        " \n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # batch contains eight PyTorch tensors:\n",
        "        question = batch[0].to(device)\n",
        "        pos_ans = batch[1].to(device)\n",
        "        neg_ans = batch[2].to(device)\n",
        "\n",
        "        # 1. Zero gradients\n",
        "        model.zero_grad()\n",
        "            \n",
        "        # 2. Compute predictions\n",
        "        pos_sim = model(question, pos_ans)    \n",
        "        neg_sim = model(question, neg_ans)\n",
        "\n",
        "        # 3. Compute loss\n",
        "        loss = hinge_loss(pos_sim, neg_sim).mean()\n",
        "\n",
        "        # 4. Use loss to compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "            \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_loss = train_loss/len(train_dataloader)\n",
        "            \n",
        "    return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-GFmyfiAXXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, validation_dataloader):\n",
        "\n",
        "    # Cumulated Training loss\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        question, pos_ans, neg_ans = batch\n",
        "\n",
        "        # Don't calculate the gradients\n",
        "        with torch.no_grad():\n",
        "                \n",
        "            pos_sim = model(question, pos_ans)    \n",
        "            neg_sim = model(question, neg_ans)\n",
        "\n",
        "            loss = hinge_loss(pos_sim, neg_sim).mean()\n",
        "                \n",
        "            valid_loss += loss.item()\n",
        "        \n",
        "    avg_loss = valid_loss/len(validation_dataloader)\n",
        "                \n",
        "    return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xylTrRNW-Cyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "633f2aa6-70f3-4739-b0b2-984a0fc85dbf"
      },
      "source": [
        "model = QA_LSTM(vocab_size, emb_dim, hidden_size, dropout)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "n_epochs = 4\n",
        "\n",
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss = train(model, train_dataloader, optimizer)\n",
        "    # Evaluate validation loss\n",
        "    valid_loss = validate(model, validation_dataloader)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # Save the parameters of the model\n",
        "        torch.save(model.state_dict(), path + 'model/'+str(epoch+1)+'_lstm50_256_64_1e3.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {}\".format(round(train_loss, 3)))\n",
        "    print(\"\\t Validation Loss: {}\\n\".format(round(valid_loss, 3)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4342/4342 [11:51<00:00,  6.10it/s]\n",
            "100%|██████████| 483/483 [00:31<00:00, 15.21it/s]\n",
            "  0%|          | 1/4342 [00:00<12:26,  5.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 1:\n",
            "\t Train Loss: 0.103\n",
            "\t Validation Loss: 0.276\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4342/4342 [11:53<00:00,  6.08it/s]\n",
            "100%|██████████| 483/483 [00:31<00:00, 15.15it/s]\n",
            "  0%|          | 1/4342 [00:00<13:17,  5.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 2:\n",
            "\t Train Loss: 0.037\n",
            "\t Validation Loss: 0.284\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4342/4342 [11:52<00:00,  6.09it/s]\n",
            "100%|██████████| 483/483 [00:31<00:00, 15.39it/s]\n",
            "  0%|          | 1/4342 [00:00<13:08,  5.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 3:\n",
            "\t Train Loss: 0.018\n",
            "\t Validation Loss: 0.262\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4342/4342 [11:49<00:00,  6.12it/s]\n",
            "100%|██████████| 483/483 [00:31<00:00, 15.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 4:\n",
            "\t Train Loss: 0.012\n",
            "\t Validation Loss: 0.263\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9YIIOZt3waV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lstm_rank(model, test_set, qid_rel, max_seq_len):\n",
        "    \n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "\n",
        "        ques, pos_ans, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_tokenized_text[ques]\n",
        "        q_vec = torch.tensor([vectorize(q_text, vocab, max_seq_len)]).to(device)\n",
        "\n",
        "        cands_text = [docid_to_tokenized_text[c] for c in cands]\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        for cand in cands_text:\n",
        "            a_vec = torch.tensor([vectorize(cand, vocab, max_seq_len)]).to(device)\n",
        "            scores.append(model(q_vec, a_vec).item())\n",
        "\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        # Get the docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[ques] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Kd6zOOBkbc",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OnYU9rDU4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 2))\n",
        "toy_test = test_set[:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8TCRr4YMrJ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "948f81da-ed98-49d8-ef15-023abeb5f469"
      },
      "source": [
        "# Load the model with the best validation loss\n",
        "model.load_state_dict(torch.load(path+'model/3_lstm50_256_64_1e3.pt'))\n",
        "\n",
        "qid_pred_rank = get_lstm_rank(model, test_set, test_qid_rel, max_seq_len=128)\n",
        "# qid_pred_rank = get_lstm_rank(model, toy_test, toy_test_label, max_seq_len=128)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 333/333 [01:57<00:00,  2.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99aL2wiN4pFd",
        "colab_type": "code",
        "outputId": "e392659f-7082-4b61-a3e0-dc2d55839dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "k = 10\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, test_qid_rel, k)\n",
        "# MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, toy_test_label, k)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 333 queries: 0.11789992950838662\n",
            "\n",
            "MRR@10 for 333 queries: 0.07625840125840123\n",
            "\n",
            "Average Precision@1: 0.036036036036036036\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}