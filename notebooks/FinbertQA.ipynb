{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinbertQA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6389029b60474f949bd150054692a900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3932c4431a0f4011b2631f20b38f2c91",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3e33bb9e8c9e433b98d0dab5c4dc0099",
              "IPY_MODEL_59a0c1a8be4f4793abd2d5b957f8075f"
            ]
          }
        },
        "3932c4431a0f4011b2631f20b38f2c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e33bb9e8c9e433b98d0dab5c4dc0099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bb636aa1f58a4cfbbaf0320c1586c000",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_949539e1c98e404ba7ee5676a8ce0e76"
          }
        },
        "59a0c1a8be4f4793abd2d5b957f8075f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8ca65098f21144dcb54963dbbfa2cc90",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 232k/232k [00:00&lt;00:00, 5.59MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8881d877efd546328471bd70ce018eed"
          }
        },
        "bb636aa1f58a4cfbbaf0320c1586c000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "949539e1c98e404ba7ee5676a8ce0e76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ca65098f21144dcb54963dbbfa2cc90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8881d877efd546328471bd70ce018eed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXHdHsEUxNd",
        "colab_type": "code",
        "outputId": "7c9a8396-a287-4c16-9406-e8069c209c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hWS0v0sx7J",
        "colab_type": "code",
        "outputId": "c084b0f7-aace-4c51-dafa-71c45b6fd01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "source": [
        "import pickle\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\r\u001b[K     |▋                               | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 501kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 42.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=3d1be901656562faeba2f37e7ada3585b0235dc4fff0bb6b17ea1e191e7c3c36\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGukeI_cCPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbIIxiVhU1VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "\n",
        "def remove_empty(test_set):\n",
        "    for index, row in enumerate(test_set):\n",
        "        for doc in row[1]:\n",
        "            if doc in empty_docs:\n",
        "                del test_set[index]\n",
        "    return test_set\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_pickle(path, data):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def pad_seq(seq, max_seq_len):\n",
        "    # Pad each seq to be the same length to process in batch.\n",
        "    # pad_token = 0\n",
        "    if len(seq) >= max_seq_len:\n",
        "        seq = seq[:max_seq_len]\n",
        "    else:\n",
        "        seq += [0]*(max_seq_len - len(seq))\n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iCcUYUvb6-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict mapping of token to idx\n",
        "vocab = load_pickle(path + 'vocab_full.pickle')\n",
        "# dict mapping of docid to doc text\n",
        "docid_to_text = load_pickle(path + 'label_ans.pickle')\n",
        "\n",
        "# dict mapping of qid to question text\n",
        "qid_to_text = load_pickle(path + 'qid_text.pickle')\n",
        "\n",
        "train_qid_rel = load_pickle(path + \"qid_rel_train.pickle\")\n",
        "test_qid_rel = load_pickle(path + \"qid_rel_test.pickle\")\n",
        "valid_qid_rel = load_pickle(path + \"qid_rel_valid.pickle\")\n",
        "\n",
        "train_set = load_pickle(path + 'data/data_train_50.pickle')\n",
        "valid_set = load_pickle(path + 'data/data_valid_50.pickle')\n",
        "\n",
        "test_set = load_pickle(path + 'data/data_test_500_rel.pickle')\n",
        "test_set_full = load_pickle(path + 'data/data_test_500.pickle')\n",
        "\n",
        "empty_docs = load_pickle(path+'empty_docs.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_sM2Lrb794",
        "colab_type": "code",
        "outputId": "00b36ab7-f297-4e34-8837-1320789d61e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "train_set = [x for x in train_set if x[1] not in empty_docs]\n",
        "valid_set = [x for x in valid_set if x[1] not in empty_docs]\n",
        "\n",
        "test_set = remove_empty(test_set)\n",
        "test_set_full = remove_empty(test_set_full)\n",
        "\n",
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 283707\n",
            "Number of validation samples: 31582\n",
            "Number of test samples: 330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patXoUxq71mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collection = pd.read_csv(path+\"data-bert/collection_new.tsv\", sep=\"\\t\", header=None)\n",
        "collection = collection.rename(columns={0: 'docid', 1: 'doc'})\n",
        "\n",
        "def load_questions(path):\n",
        "    \"\"\"\n",
        "    Returns a dataframe of cols: qid, question\n",
        "    \"\"\"\n",
        "    # Question ID and Question text\n",
        "    query_df = pd.read_csv(path, sep=\"\\t\")\n",
        "    queries = query_df[['qid', 'question']]\n",
        "\n",
        "    return queries\n",
        "\n",
        "queries = load_questions(path + \"FiQA_train_question_final.tsv\")\n",
        "\n",
        "# Question to question text\n",
        "qid_to_text = {}\n",
        "\n",
        "for index, row in queries.iterrows():\n",
        "    qid_to_text[row['qid']] = row['question']\n",
        "\n",
        "docid_to_text = {}\n",
        "\n",
        "for index, row in collection.iterrows():\n",
        "    docid_to_text[row['docid']] = row['doc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npVgMDNewIW",
        "colab_type": "code",
        "outputId": "cdec20f0-93f5-4acf-b1d6-34f8ec640299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "6389029b60474f949bd150054692a900",
            "3932c4431a0f4011b2631f20b38f2c91",
            "3e33bb9e8c9e433b98d0dab5c4dc0099",
            "59a0c1a8be4f4793abd2d5b957f8075f",
            "bb636aa1f58a4cfbbaf0320c1586c000",
            "949539e1c98e404ba7ee5676a8ce0e76",
            "8ca65098f21144dcb54963dbbfa2cc90",
            "8881d877efd546328471bd70ce018eed"
          ]
        }
      },
      "source": [
        "# # Load the BERT tokenizer.\n",
        "# print('Loading BERT tokenizer...')\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6389029b60474f949bd150054692a900",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsNC1_bA57S5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(path+\"model/fin_model/vocab.txt\")\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"[SEP]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "    (\"[CLS]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "tokenizer.enable_padding(max_length=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXwjgq4zuPzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label_to_ans = load_pickle(path+\"data-bert/label_to_ans.pickle\")\n",
        "# qid_to_text = load_pickle(path+\"data-bert/qid_to_text.pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97-K-2Pr8L2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_ques_token(string):\n",
        "    question = string + \" [SEP] \"\n",
        "\n",
        "    return question"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VHo3w567Bza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sequence_df(dataset):\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df.rename(columns={0: 'qid', 1: 'pos', 2:'neg'})\n",
        "    df_pos = df[['qid', 'pos']]\n",
        "    df_pos = df_pos.rename(columns={'pos': 'docid'})\n",
        "    df_pos['label'] = df_pos.apply(lambda x: 1, axis=1)\n",
        "    df_pos = df_pos.drop_duplicates()\n",
        "\n",
        "    df_neg = df[['qid', 'neg']]\n",
        "    df_neg = df_neg.rename(columns={'neg': 'docid'})\n",
        "    df_neg['label'] = df_neg.apply(lambda x: 0, axis=1)\n",
        "    data_df = pd.concat([df_pos, df_neg]).sort_values(by=['qid'])\n",
        "\n",
        "    data_df['question'] = data_df['qid'].apply(lambda x: qid_to_text[x])\n",
        "    data_df['ans_cand'] = data_df['docid'].apply(lambda x: docid_to_text[x])\n",
        "\n",
        "    data_df['question'] = data_df['question'].apply(add_ques_token)\n",
        "    data_df['seq'] = data_df['question'] + data_df['ans_cand']\n",
        "    # dataset = dataset[['seq']]\n",
        "    # data_df['ques_token'] = data_df['question'].apply(lambda x: add_question_token(x))\n",
        "    # data_df['ans_cand'] = data_df['ans_cand'].apply(lambda x: add_ans_token(x))\n",
        "\n",
        "    # data_df = data_df[['qid', 'docid', 'label', 'ans_cand','ques_token']]\n",
        "    # data_df['seq'] = data_df['ques_token'] + data_df['ans_cand']\n",
        "\n",
        "    # data_df['seq_clipped'] = data_df['seq'].apply(clip)\n",
        "    # # train['len'] = train['seq_clipped'].apply(lambda x: len(x))\n",
        "\n",
        "    return data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8yDm8pv9DN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "train_data = get_sequence_df(train_set)\n",
        "train_sequences = train_data.seq.values\n",
        "train_labels = train_data.label.values\n",
        "\n",
        "# a = train_sequences[0]\n",
        "\n",
        "# tokenized_seq = tokenizer.encode(a)\n",
        "\n",
        "train_inputs = []\n",
        "train_att_masks = []\n",
        "\n",
        "\n",
        "for i, seq in enumerate(train_sequences):\n",
        "\n",
        "    tokenized_seq = tokenizer.encode(seq)\n",
        "\n",
        "    train_inputs.append(tokenized_seq.ids)\n",
        "    train_att_masks.append(tokenized_seq.attention_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OnRl-nt6dl-",
        "colab_type": "code",
        "outputId": "76a10f3c-10d6-49ae-94a5-f9ba15b89532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/My Drive/FiQA/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWcZgNdQ8XOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+\"data-finbert/train_inputs.pickle\", train_inputs)\n",
        "save_pickle(path+\"data-finbert/train_att_masks.pickle\", train_att_masks)\n",
        "save_pickle(path+\"data-finbert/train_labels.pickle\", train_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pIPITVM83VT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = load_pickle(path+\"data-finbert/train_inputs.pickle\")\n",
        "train_att_masks = load_pickle(path+\"data-finbert/train_att_masks.pickle\")\n",
        "train_labels = load_pickle(path+\"data-finbert/train_labels.pickle\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utKln9eH6M5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "valid_data = get_sequence_df(valid_set)\n",
        "valid_sequences = valid_data.seq.values\n",
        "valid_labels = valid_data.label.values\n",
        "\n",
        "# a = train_sequences[0]\n",
        "\n",
        "# tokenized_seq = tokenizer.encode(a)\n",
        "\n",
        "valid_inputs = []\n",
        "valid_att_masks = []\n",
        "\n",
        "\n",
        "for i, seq in enumerate(valid_sequences):\n",
        "\n",
        "    tokenized_seq = tokenizer.encode(seq)\n",
        "\n",
        "    valid_inputs.append(tokenized_seq.ids)\n",
        "    valid_att_masks.append(tokenized_seq.attention_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jglXff0IGQiT",
        "colab_type": "code",
        "outputId": "ce86080a-5712-4a30-c69e-c60673002995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(train_inputs))\n",
        "# print(len(valid_inputs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "298401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ggzgHk35yQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_inputs = train_inputs[:10000]\n",
        "# train_att_masks = train_att_masks[:10000]\n",
        "# train_labels = train_labels[:10000]\n",
        "\n",
        "# valid_inputs = train_inputs[:100]\n",
        "# valid_att_masks = train_att_masks[:100]\n",
        "# valid_labels = train_labels[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKxNW3iprhg7",
        "colab_type": "text"
      },
      "source": [
        "## **Pointwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy1wj92aI4hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_input = torch.tensor(train_inputs)\n",
        "# validation_input = torch.tensor(valid_inputs)\n",
        "\n",
        "train_label = torch.tensor(train_labels)\n",
        "# validation_label = torch.tensor(valid_labels)\n",
        "\n",
        "train_mask = torch.tensor(train_att_masks)\n",
        "# validation_mask = torch.tensor(valid_att_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcigYAsxypBs",
        "colab_type": "code",
        "outputId": "5165b1f1-8cfa-460a-8452-b4c4b3283174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(train_input))\n",
        "# print(len(validation_input))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "298401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P4Lb1dzK3Ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "training_data = TensorDataset(train_input, train_mask, train_label)\n",
        "train_sampler = RandomSampler(training_data)\n",
        "train_dataloader = DataLoader(training_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# # Create the DataLoader for our validation set.\n",
        "# validation_data = TensorDataset(validation_input, validation_mask, validation_label)\n",
        "# validation_sampler = SequentialSampler(validation_data)\n",
        "# validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjVUsNKVhXDn",
        "colab_type": "code",
        "outputId": "11dd56c7-6889-4b3e-a3c6-64728656ebce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(train_dataloader))\n",
        "# print(len(validation_dataloader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14DFmK3GM7LU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmJJeoxC4NKu",
        "colab_type": "text"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpM6uIwJ5GSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.config = BertConfig.from_pretrained(path+\"model/fin_model/config.json\")\n",
        "        # self.config = BertConfig.from_pretrained(\"/content/drive/My Drive/FiQA/bert-lm/weights2/config.json\")\n",
        "        # self.config = BertConfig()\n",
        "        self.num_labels = self.config.num_labels\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
        "\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6328iAv4HYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = BertForSequenceClassification.from_pretrained(path+\"model/fin_model\")\n",
        "# model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jKuO5y3ihOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # model_path = \"/content/drive/My Drive/FiQA/bert-lm/test_lm/\"\n",
        "# # bert = BertModel.from_pretrained(model_path)\n",
        "\n",
        "# model = BertClassifier(bert)\n",
        "\n",
        "# # Tell pytorch to run this model on the GPU.\n",
        "# model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naoa-MQF_lwE",
        "colab_type": "code",
        "outputId": "2e4038c6-f97d-40b5-c28e-e6ce70a320eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# model_path = \"/content/drive/My Drive/FiQA/bert-lm/weights2/\"\n",
        "model_path = path+\"model/fin_model/\"\n",
        "bert = BertModel.from_pretrained(model_path)\n",
        "\n",
        "model = BertClassifier(bert)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDc45R4mjGDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataloader, optimizer):\n",
        "\n",
        "    # Store the average loss after each epoch so we can plot them.\n",
        "    loss_values = []\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    return avg_train_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNhQobcGke2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, validation_dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_loss = 0\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "    avg_loss = total_loss / len(validation_dataloader) \n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7wlYPSlZcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3lU_QLEl4lY",
        "colab_type": "code",
        "outputId": "38ad50d2-578f-43d9-cf52-0fb1d85a18ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "n_epochs = 1\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer)\n",
        "    \n",
        "    # Evaluate validation loss\n",
        "    # valid_loss, valid_acc = validate(model, validation_dataloader)\n",
        "    \n",
        "    # # At each epoch, if the validation loss is the best\n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    # torch.save(model.state_dict(), path + 'model/' + str(epoch+1)+'_finbert-full.pt')\n",
        "    torch.save(model.state_dict(), path + 'model/1_finbert-full.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
        "    # print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 21%|██        | 7656/37301 [57:25<3:42:45,  2.22it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCVZwbcZqxl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), path + 'model/2_model-bert-512.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE1ghzszv_0D",
        "colab_type": "code",
        "outputId": "a027e52e-5f3c-476b-eb0a-1a36348a9458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(torch.cuda.get_device_name(0))\n",
        "print('Memory Usage:')\n",
        "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.4 GB\n",
            "Cached:    0.5 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X4VuB-nu8mQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = load_pickle(path + 'data/data_test_500_rel.pickle')\n",
        "test_set_full = load_pickle(path + 'data/data_test_500.pickle')\n",
        "\n",
        "empty_docs = load_pickle(path+'empty_docs.pickle')\n",
        "\n",
        "test_set = remove_empty(test_set)\n",
        "test_set_full = remove_empty(test_set_full)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y_hXQeACoNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for row in test_set:\n",
        "    row[2] = [x for x in row[2] if x is not 0]\n",
        "\n",
        "for row in test_set_full:\n",
        "    row[2] = [x for x in row[2] if x is not 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5PeEfoWsSLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_set2 = test_set\n",
        "\n",
        "# for row in test_set2:\n",
        "#     row[2] = random.sample(row[2], len(row[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NavDO7pTnjug",
        "colab_type": "code",
        "outputId": "8a152b99-7922-47fe-c252-ee3321a59c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "count = 0\n",
        "\n",
        "for row in test_set_full:\n",
        "    if not any(x in row[2]  for x in row[1]):\n",
        "        count += 1\n",
        "\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33uApw6Duf8B",
        "colab_type": "code",
        "outputId": "03724590-bed4-44e8-ebac-fc2f03b521b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "count = 0\n",
        "\n",
        "for row in test_set_full:\n",
        "    if not all(x in row[2]  for x in row[1]):\n",
        "        count += 1\n",
        "\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6yNzwdlpnNI",
        "colab_type": "code",
        "outputId": "40a5d12a-5dd4-4440-813e-f786569d76f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(test_set_full)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZWv0yqQU9oV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sequence_df(dataset):\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df.rename(columns={0: 'qid', 1: 'pos', 2:'neg'})\n",
        "    df_pos = df[['qid', 'pos']]\n",
        "    df_pos = df_pos.rename(columns={'pos': 'docid'})\n",
        "    df_pos['label'] = df_pos.apply(lambda x: 1, axis=1)\n",
        "    df_pos = df_pos.drop_duplicates()\n",
        "\n",
        "    df_neg = df[['qid', 'neg']]\n",
        "    df_neg = df_neg.rename(columns={'neg': 'docid'})\n",
        "    df_neg['label'] = df_neg.apply(lambda x: 0, axis=1)\n",
        "    data_df = pd.concat([df_pos, df_neg]).sort_values(by=['qid'])\n",
        "\n",
        "    data_df['question'] = data_df['qid'].apply(lambda x: qid_to_text[x])\n",
        "    data_df['ans_cand'] = data_df['docid'].apply(lambda x: docid_to_text[x])\n",
        "\n",
        "    data_df['question'] = data_df['question'].apply(add_ques_token)\n",
        "    data_df['seq'] = data_df['question'] + data_df['ans_cand']\n",
        "    # dataset = dataset[['seq']]\n",
        "    # data_df['ques_token'] = data_df['question'].apply(lambda x: add_question_token(x))\n",
        "    # data_df['ans_cand'] = data_df['ans_cand'].apply(lambda x: add_ans_token(x))\n",
        "\n",
        "    # data_df = data_df[['qid', 'docid', 'label', 'ans_cand','ques_token']]\n",
        "    # data_df['seq'] = data_df['ques_token'] + data_df['ans_cand']\n",
        "\n",
        "    # data_df['seq_clipped'] = data_df['seq'].apply(clip)\n",
        "    # # train['len'] = train['seq_clipped'].apply(lambda x: len(x))\n",
        "\n",
        "    return data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT0cIadqEdNp",
        "colab_type": "code",
        "outputId": "d93c1b03-064d-4f5f-fd88-63c2845b2147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "test_df = pd.DataFrame(test_set_full)\n",
        "test_df = test_df.rename(columns={0: 'qid', 1: 'pos', 2:'cand'})\n",
        "# test_df = test_df[['qid', 'cand']]\n",
        "\n",
        "test_pos = test_df[['qid', 'pos']]\n",
        "test_pos = test_pos.explode('pos')\n",
        "test_pos = test_pos.rename(columns={'pos': 'docid'})\n",
        "test_pos['label'] = test_pos.apply(lambda x: 1, axis=1)\n",
        "\n",
        "test_neg = test_df[['qid', 'cand']]\n",
        "test_neg = test_neg.explode('cand')\n",
        "test_neg = test_neg.rename(columns={'cand': 'docid'})\n",
        "test_neg ['label'] = test_neg .apply(lambda x: 0, axis=1)\n",
        "\n",
        "test_neg.head(5)\n",
        "\n",
        "test_data = pd.concat([test_pos, test_neg]).sort_values(by=['qid'])\n",
        "\n",
        "test_data['question'] = test_data['qid'].apply(lambda x: qid_to_text[x])\n",
        "test_data['ans_cand'] = test_data['docid'].apply(lambda x: docid_to_text[x])\n",
        "\n",
        "test_data['ques_token'] = test_data['question'].apply(lambda x: add_ques_token(x))\n",
        "# test_data['ans_cand'] = test_data['ans_cand'].apply(lambda x: add_ans_token(x))\n",
        "\n",
        "# test_data = test_data[['qid', 'docid', 'label', 'ans_cand','ques_token']]\n",
        "test_data['seq'] = test_data['ques_token'] + test_data['ans_cand']\n",
        "# test_data['seq_clipped'] = test_data['seq'].apply(clip)\n",
        "\n",
        "# test_data.head(5)\n",
        "\n",
        "# docid_map = test_data[['docid', 'seq_clipped']]\n",
        "test_full_lm_docid_to_seq = {}\n",
        "\n",
        "for index, row in test_data.iterrows():\n",
        "    test_full_lm_docid_to_seq[row['docid']] = row['seq']\n",
        "\n",
        "print(take(5, test_full_lm_docid_to_seq.items()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(14255, \"What is the easiest way to back-test index funds and ETFs? [SEP] Yes you can claim your business deductions if you are not making any income yet. But first you should decide what structure you want to have for your business. Either a Company structure or a Sole Trader or Partnership. Company Structure If you choose a Company Structure (which is more expensive to set up) you would claim your deductions but no income. So you would be making a loss, and continue making losses until your income from the business exceed your expenses. So these losses will remain inside the Company and can be carried forward to future income years when you are making profits to offset these profits. Refer to ATO - Company tax losses for more information. Sole Trader of Partnership Structure If you choose to be a Sole Trader or a Partnership and your business makes a loss you must check the non-commercial loss rules to see if you can offset the loss against your income from other sources, such as wages. In order to offset your business losses against your other income your business must  pass one of these tests: If you don't pass any of these tests, which being a start-up you most likely won't, you must carry forward your business losses until an income year in which you do pass one of the tests, then you can offset it against your other income. This is what differentiates a legitimate business from someone having a hobby, because unless you start making at least $20,000 in sales income (the easiest test to pass) you cannot use your business losses against your other income. Refer to ATO - Non-commercial losses for more information.\"), (541682, \"Is IRS Form 8938 asking me to double-count foreign assets? [SEP] If you are paid by foreigners then it is quite possible they don't file anything with the IRS. All of this income you are required to report as business income on schedule C.  There are opportunities on schedule C to deduct expenses like your health insurance, travel, telephone calls, capital expenses like a new computer, etc... You will be charged both the employees and employers share of social security/medicare, around ~17% or so, and that will be added onto your 1040.   You may still need a local business license to do the work locally, and may require a home business permit in some cities.  In some places, cities subscribe to data services based on your IRS tax return.... and will find out a year or two later that someone is running an unlicensed business.  This could result in a fine, or perhaps just a nice letter from the city attorneys office that it would be a good time to get the right licenses. Generally, tax treaties exist to avoid or limit double taxation. For instance, if you travel to Norway to give a report and are paid during this time, the treaty would explain whether that is taxable in Norway.  You can usually get a credit for taxes paid to foreign countries against your US taxes, which helps avoid paying double taxes in the USA.   If you were to go live in Norway for more than a year, the first $80,000/year or so is completely wiped off your US income.  This does NOT apply if you live in the USA and are paid from Norway. If you have a bank account overseas with more than $10,000 of value in it at any time during the year, you owe the US Government a FinCEN Form 114 (FBAR).  This is pretty important, there are some large fines for not doing it.  It could occur if you needed an account to get paid in Norway and then send the money here... If the Norwegian company wires the money to you from their account or sends a check in US$, and you don't have a foreign bank account, then this would not apply.\"), (420846, 'How does a typical vesting timeline work with respect to employer contributions? [SEP] Your wages are an expense to your employer and are therefore 100% tax deductible in the business income.  The company should not be paying tax on that, so your double-tax scenario, as described, isn\\'t really correct. [The phrase \"double taxation\" with respect to US corporations usually comes into play with dividends.  In that case, however, it\\'s the shareholders (owners) that pay double.  The answer to \"why?\" in that case can only be \"because it\\'s the law.\"]'), (413681, 'GnuCash register reimbursements [SEP] You can have a way for people to pay, i.e. some kind of payment gateway. Run as Business: Best create a company and get the funds there. This would be treated as income of the website and would be taxed accordingly. One can deduct expenses for running the website, etc. Run as Charity: Register as one, however the cause should be considered as charitable one by the tax authorities. Only then the donations would be tax free.'), (597053, \"Does “income” include capital gains? [SEP] Yes. W4 determines how much your employer will withhold from your wages. Leaving everything at default would mean that your salary is your only taxable income, and you only take default deductions. Your employee will calculate your tax withholding based on that. But, if your salary is >200k, I assume that you have other income (investment/capital gains, interest on your bank account), which you will have to pay taxes on. You're probably going to have some deductible expenses (business/partnership expenses, mortgage interest, donations, college funds etc) as well. So it is very likely, unless you're really not smart about money, that you have more to do with your taxes than just the employers' withholding.\")]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBIraaUKmZe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'data-bert/test_full_lm_docid_to_seq.pickle', test_full_lm_docid_to_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EewC_SM1fjqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_docid_to_seq = load_pickle(path+'data-bert/test_docid_to_seq.pickle')\n",
        "# test_docid_to_seq = load_pickle(path+'data-bert/test_full_docid_to_seq.pickle')\n",
        "test_docid_to_seq = load_pickle(path+'data-bert/test_lm_docid_to_seq.pickle')\n",
        "# test_docid_to_seq = load_pickle(path+'data-bert/test_full_lm_docid_to_seq.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu7rnpVXPV3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rank(model, test_set, qid_rel, max_seq_len):\n",
        "\n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "        \n",
        "        qid, label, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        for docid in cands:\n",
        "\n",
        "            seq_text = test_docid_to_seq[docid]\n",
        "\n",
        "            tokenized_seq = tokenizer.encode(seq_text)\n",
        "\n",
        "            input_ids = tokenized_seq.ids\n",
        "\n",
        "            att_mask = torch.tensor([tokenized_seq.attention_mask]).to(device)\n",
        "            \n",
        "            input_ids = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "                outputs = model(input_ids, token_type_ids=None, attention_mask=att_mask)\n",
        "\n",
        "            logits = outputs[0]\n",
        "\n",
        "            pred = torch.sigmoid(logits)\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "            scores.append(pred[:,1][0])\n",
        "\n",
        "        # print(scores)\n",
        "\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        # Get the docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[qid] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank\n",
        "    # MRR, average_ndcg, precision = evaluate(qid_pred_rank, qid_rel, k)\n",
        "\n",
        "    # return qid_pred_rank, MRR, average_ndcg, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13cmzGBmNX6f",
        "colab_type": "code",
        "outputId": "272446ed-e946-48ef-eef3-ba9506e9e932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(test_set2[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, [14255], [136804, 596289, 363495, 223624, 212661, 481114, 278460, 377547, 352640, 109546, 203905, 42665, 197352, 285255, 179144, 559671, 528564, 8200, 76618, 487551, 72053, 420295, 327002, 176054, 216008, 362069, 33544, 361978, 224167, 311285, 397920, 129355, 108062, 331898, 472824, 69317, 204554, 98072, 15270, 98636, 399115, 18539, 78486, 392484, 40628, 153377, 360925, 202457, 99745, 391619, 5762, 245447, 167494, 453455, 509346, 141738, 314161, 391841, 541809, 359814, 248629, 290265, 283113, 151645, 201546, 521044, 364938, 561750, 540395, 187384, 584232, 294738, 307779, 97489, 452248, 274870, 544381, 542213, 191848, 146657, 589416, 16911, 55500, 328745, 316074, 202645, 177074, 402437, 79411, 420846, 131453, 492456, 245011, 484231, 243822, 46791, 12822, 113876, 245810, 541682, 38249, 30343, 531442, 375357, 174714, 121063, 377621, 132780, 427849, 483664, 422199, 576218, 202179, 34810, 331384, 193367, 237207, 333954, 462184, 276934, 291079, 466718, 87113, 574417, 354314, 418999, 249450, 445298, 507871, 298970, 577284, 424008, 288995, 42924, 510692, 310612, 257738, 411825, 216745, 477476, 513051, 112793, 442968, 248448, 192516, 512151, 431010, 328073, 547087, 373481, 189765, 399762, 231279, 113776, 507371, 75005, 390435, 223170, 296345, 426343, 156554, 458079, 381151, 215761, 166183, 399882, 54742, 133701, 406789, 85622, 182989, 571062, 520922, 59000, 154931, 413976, 18850, 155490, 208219, 588327, 304980, 510181, 194308, 18889, 109250, 555732, 470066, 263521, 121551, 313361, 316932, 183891, 489633, 522671, 299971, 385506, 88967, 299211, 254158, 253541, 160340, 213041, 585341, 390368, 446553, 12729, 197205, 528838, 37636, 396056, 74774, 47260, 223697, 390559, 487728, 483489, 89190, 254151, 219274, 96158, 152027, 47957, 388667, 208989, 83346, 441038, 376129, 289620, 70738, 83004, 182168, 91325, 90290, 483385, 421301, 365558, 547941, 152407, 18934, 235823, 560776, 233751, 158738, 349672, 14255, 187227, 535673, 175889, 481692, 309909, 378484, 289582, 192726, 24421, 253210, 128861, 162500, 583956, 197175, 550345, 581265, 354716, 280021, 183612, 510144, 158409, 546277, 124009, 444589, 536849, 173212, 55261, 476980, 19455, 155648, 28764, 196374, 458930, 147080, 462831, 322064, 483942, 344093, 525149, 246461, 389164, 388042, 156063, 593760, 272248, 192843, 397152, 576985, 303078, 130631, 194899, 216077, 221281, 105543, 534997, 77325, 107584, 395139, 401832, 368263, 442142, 576295, 234436, 524879, 189642, 597053, 440061, 146317, 365456, 100280, 349674, 216783, 583757, 251392, 19640, 447231, 144783, 144190, 527037, 73427, 542139, 513362, 388713, 424001, 375423, 356884, 454537, 255101, 120500, 77171, 118615, 461526, 362060, 494000, 351672, 141458, 61782, 441568, 598547, 103405, 355897, 18570, 197501, 157233, 588253, 474795, 338700, 509862, 71987, 490489, 252758, 84963, 329209, 361637, 366074, 178942, 338096, 74688, 527776, 571232, 97719, 334902, 427469, 438515, 21846, 66356, 100764, 227757, 566417, 169613, 174321, 156444, 257168, 515690, 265527, 177946, 426639, 297938, 248761, 306600, 110117, 401731, 529853, 399848, 209997, 410431, 441419, 81343, 345070, 174025, 18001, 413681, 485102, 304559, 408112, 41793, 434846, 153541, 103668, 328853, 220621, 363178, 468741, 205791, 522619, 590775, 297841, 374443, 304452, 479542, 539511, 358631, 519473, 94600, 283505, 81886, 444899, 326717, 291717, 251649, 406418, 65771, 469043, 552632, 274721, 357717, 40257, 540325, 562777, 588211, 187586, 113632, 434351, 531578, 68969, 571711, 264659, 283079, 81599, 510716, 31117, 397608, 507107, 55200, 322906, 279149, 562802, 411063, 71569, 402327, 55666, 349424, 185999, 423083, 546509, 5587, 302049, 86134, 195207, 319234, 424720, 15643, 262960, 334603, 89008, 282958, 417981, 219313, 18647, 11132, 444273, 532888, 414834, 286245, 292748, 234743, 146388, 523564, 523415, 275312, 596598, 402404, 434619, 107794, 576362, 296717, 395011, 211485, 172745, 252273, 476173, 533808, 178501, 599336, 79397]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOR-OM-qbb3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 100))\n",
        "toy_test = test_set[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC0srkXA2M7_",
        "colab_type": "code",
        "outputId": "2faacb7f-5f55-47ff-c3ac-5a84019e9fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.load_state_dict(torch.load(path+'model/2_finbert-test.pt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa0KvT4cbbo",
        "colab_type": "code",
        "outputId": "596d6204-c8a9-4789-ae84-7a403e99ff21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# model.load_state_dict(torch.load(path+'model/2_model-bert-512.pt'))\n",
        "\n",
        "# qid_pred_rank = get_rank(model, test_set_full, test_qid_rel, max_seq_len=512)\n",
        "qid_pred_rank = get_rank(model, toy_test, toy_test_label, max_seq_len=512)\n",
        "# qid_pred_rank2 = get_rank(model,toy_test2, toy_test_label, max_seq_len=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [18:35<00:00, 11.10s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBWxiMZ5YXmp",
        "colab_type": "code",
        "outputId": "a378ebd4-ebc2-489e-cbc5-dc9f9e2a8f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "k = 10\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "# MRR, average_ndcg, precision = evaluate(qid_pred_rank, test_qid_rel, k)\n",
        "MRR, average_ndcg, precision = evaluate(qid_pred_rank, toy_test_label, k)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 330 queries: 0.023979694423373953\n",
            "\n",
            "MRR@10 for 330 queries: 0.010317460317460315\n",
            "\n",
            "Average Precision@1: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcfbXb5eBcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'rank/rank_lm_bert_test_full.pickle', qid_pred_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}