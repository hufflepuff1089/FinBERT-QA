{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertQA-pairwise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a9bd09916034dd882f020786257acfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5534a6e4e0b04e06a79067c61d93f4a5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8d32ae275a7640398ebe76b270b7fafd",
              "IPY_MODEL_89a21dd4ae0e4f1b8f0237dbe3d1073f"
            ]
          }
        },
        "5534a6e4e0b04e06a79067c61d93f4a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d32ae275a7640398ebe76b270b7fafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0e9ab47603914aeebe418231acfe5d36",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f413ea77edf42e8ab1e85156fef9f8b"
          }
        },
        "89a21dd4ae0e4f1b8f0237dbe3d1073f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c3c843c789f64104854a4e7330869719",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:09&lt;00:00, 24.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc26716357504bfdaecf5b815a6ce68c"
          }
        },
        "0e9ab47603914aeebe418231acfe5d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f413ea77edf42e8ab1e85156fef9f8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3c843c789f64104854a4e7330869719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc26716357504bfdaecf5b815a6ce68c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ff4bf4681b34083a648d9e8cecd9a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df1c14e22ec4426fb24943f87ec298e6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_03f3357ebbc44686b8f5a7963c0da8ec",
              "IPY_MODEL_91eccb2bb06f48678934cb279cc591ab"
            ]
          }
        },
        "df1c14e22ec4426fb24943f87ec298e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03f3357ebbc44686b8f5a7963c0da8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b5807417ea7b46cb881bbb24944dd67c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d3c75c04894487cb30996c1f5e95e69"
          }
        },
        "91eccb2bb06f48678934cb279cc591ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81f937e77bca44ddb3a2d01b0cecae18",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 361/361 [00:14&lt;00:00, 25.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d60dda4992eb4c9c9f62a51243c68c14"
          }
        },
        "b5807417ea7b46cb881bbb24944dd67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d3c75c04894487cb30996c1f5e95e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81f937e77bca44ddb3a2d01b0cecae18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d60dda4992eb4c9c9f62a51243c68c14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a78be2e17b4f47069519967e1c011ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_57d97c57fc514fafbf3a69b2751b1d17",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8bae9f9c67b04e1c888616d74143bb80",
              "IPY_MODEL_3890c797a4744787b3db695257f362cc"
            ]
          }
        },
        "57d97c57fc514fafbf3a69b2751b1d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8bae9f9c67b04e1c888616d74143bb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e1fbc488ea73433bbd90a22515bd6e7c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61c2e35d7c5b47928af9c11e9b309635"
          }
        },
        "3890c797a4744787b3db695257f362cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5d98815538f4bc7b10602d49565b73c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:13&lt;00:00, 32.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6156ffce152f47bea285ff17bfcf3cb8"
          }
        },
        "e1fbc488ea73433bbd90a22515bd6e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61c2e35d7c5b47928af9c11e9b309635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5d98815538f4bc7b10602d49565b73c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6156ffce152f47bea285ff17bfcf3cb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXHdHsEUxNd",
        "colab_type": "code",
        "outputId": "302fa8c4-1537-458e-b36f-3aeb5ef66af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hWS0v0sx7J",
        "colab_type": "code",
        "outputId": "d3a2e26b-8d99-4eb3-8641-9e682686ab6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5d54cca4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3dEiY-MzxU",
        "colab_type": "code",
        "outputId": "9ea69f20-9cc7-4df7-f539-30760a1db5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        }
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\r\u001b[K     |▋                               | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 3.4MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 112kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 122kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 133kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 143kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 153kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 163kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 174kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 184kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 194kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 204kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 215kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 225kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 235kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 245kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 256kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 266kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 276kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 286kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 296kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 307kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 317kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 327kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 337kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 348kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 358kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 368kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 378kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 389kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 399kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 409kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 419kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 430kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 440kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 450kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 460kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 471kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 481kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 491kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 501kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.23)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 51.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.23)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=56b79799c40b5c9badccb5ab431d172ddb4f381e3ccca2695fb58067a546413b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhPBExB2bjG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGukeI_cCPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *\n",
        "from utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iCcUYUvb6-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary - key: qid, value: list of positive docid\n",
        "train_qid_rel = load_pickle(path + \"new-data/qid_rel_train.pickle\")\n",
        "test_qid_rel = load_pickle(path + \"new-data/qid_rel_test.pickle\")\n",
        "valid_qid_rel = load_pickle(path + \"new-data/qid_rel_valid.pickle\")\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list containing [qid, positive docid, negative docid]\n",
        "# train_set = load_pickle(path + 'new-data/data_50/train_set_50.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_50/valid_set_50.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/data_25/train_set_25.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_25/valid_set_25.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/data_10/train_set_10.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_10/valid_set_10.pickle')\n",
        "train_set = load_pickle(path + 'new-data/train_set.pickle')\n",
        "valid_set = load_pickle(path + 'new-data/valid_set.pickle')\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list contraining [qid, list of pos docid, list of candidate docid]\n",
        "# Contains candidates with all pos docids\n",
        "test_set = load_pickle(path + 'new-data/data_50/test_set_50.pickle')\n",
        "# Contains candidates retrieved by BM25\n",
        "# May be missing pos docids in candidates\n",
        "test_set_full = load_pickle(path + 'new-data/data_50/test_set_full_50.pickle')\n",
        "\n",
        "# Dictionary mapping docid and qid to raw text\n",
        "docid_to_text = load_pickle(path + 'new-data/docid_to_text.pickle')\n",
        "qid_to_text = load_pickle(path + 'new-data/qid_to_text.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_sM2Lrb794",
        "colab_type": "code",
        "outputId": "7357abf5-b01e-4090-e716-4469d316f9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 14612\n",
            "Number of validation samples: 1595\n",
            "Number of test samples: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRSvaM8BEvH8",
        "colab_type": "code",
        "outputId": "c3f844ed-aafc-4a44-881d-7c7d50a19e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Example of the training set [qid, pos docid, neg docid]\n",
        "print(train_set[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 18850, 128322], [1, 14255, 287923], [2, 308938, 543898], [3, 296717, 576136], [3, 100764, 102443], [3, 314352, 436058], [3, 146317, 144261], [4, 196463, 569461], [5, 69306, 587727], [7, 411063, 449124]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npVgMDNewIW",
        "colab_type": "code",
        "outputId": "7334bc93-6d35-4fb2-deb9-ffad8f31b89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "8a9bd09916034dd882f020786257acfd",
            "5534a6e4e0b04e06a79067c61d93f4a5",
            "8d32ae275a7640398ebe76b270b7fafd",
            "89a21dd4ae0e4f1b8f0237dbe3d1073f",
            "0e9ab47603914aeebe418231acfe5d36",
            "2f413ea77edf42e8ab1e85156fef9f8b",
            "c3c843c789f64104854a4e7330869719",
            "bc26716357504bfdaecf5b815a6ce68c"
          ]
        }
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a9bd09916034dd882f020786257acfd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB4xGQLQwSXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input(questions, answers, max_seq_len):\n",
        "    \"\"\"\n",
        "    Returns input objects for training:\n",
        "        input_ids: List of lists\n",
        "                Each element contains a list of padded/clipped numericalized\n",
        "                tokens of the sequences including [CLS] and [SEP] tokens\n",
        "                e.g. [[101, 2054, 2003, 102, 2449, 1029, 102], ...]\n",
        "        token_type_ids: List of lists\n",
        "                Each element contains a list of segment token indices to \n",
        "                indicate first and second portions of the inputs. \n",
        "                0 corresponds to a question token, 1 corresponds an answer token\n",
        "                e.g. [[0, 0, 0, 0, 1, 1, 1], ...]\n",
        "        att_masks: List of lists\n",
        "                Each element contains a list of mask values\n",
        "                Mask to avoid performing attention on padding token indices. \n",
        "                1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n",
        "                e.g. [[1, 1, 1, 1, 1, 1, 1], ...]\n",
        "    -----------------\n",
        "    questions: List of strings\n",
        "            Each element contains a question string\n",
        "    answers: List of strings\n",
        "            Each element contains an asnwer string\n",
        "    max_seq_len: int\n",
        "            Maximum sequence length\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    token_type_ids = []\n",
        "    att_masks = []\n",
        "\n",
        "    for i in tqdm(range(len(questions))):\n",
        "        a = questions[i]\n",
        "        b = answers[i]\n",
        "\n",
        "        # Tokenize the questions and answers, apply padding, and trim the vectors\n",
        "        # to the max_seq_len\n",
        "        encoded_seq = tokenizer.encode_plus(a, b, \n",
        "                                            max_length=max_seq_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "        input_id = encoded_seq['input_ids']\n",
        "        token_type_id = encoded_seq['token_type_ids']\n",
        "        att_mask = encoded_seq['attention_mask']\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Input id dimension incorrect!\"\n",
        "        assert len(token_type_id) == max_seq_len, \"Token type id dimension incorrect!\"\n",
        "        assert len(att_mask) == max_seq_len, \"Attention mask dimension incorrect!\"\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        att_masks.append(att_mask)\n",
        "\n",
        "    return input_ids, token_type_ids, att_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_kGUNI4XK5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    # Get the column with the higher probability\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x5arM3DfYyTO"
      },
      "source": [
        "## **Pairwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1C8XAC3Yuj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pairwise_sequence_df(dataset):\n",
        "    \"\"\"\n",
        "    Converts training and validation data into a df with relevancy labels\n",
        "    and map the qid and docid to text.\n",
        "    \n",
        "    Returns data_df: df with columns qid, pos docid,\n",
        "            neg docid, pos label, neg_label, question (text), \n",
        "            pos answer (text), neg answer (text)\n",
        "    ---------------\n",
        "    dataset: train or validation set in the form of list of lists\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df.rename(columns={0: 'qid', 1: 'pos_id', 2:'neg_id'})\n",
        "    df['pos_label'] = df.apply(lambda x: 1, axis=1)\n",
        "    df['neg_label'] = df.apply(lambda x: 0, axis=1)\n",
        "\n",
        "    df['question'] = df['qid'].apply(lambda x: qid_to_text[x])\n",
        "    df['pos_ans'] = df['pos_id'].apply(lambda x: docid_to_text[x])\n",
        "    df['neg_ans'] = df['neg_id'].apply(lambda x: docid_to_text[x])\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYRQH1J2Zzxt",
        "colab_type": "code",
        "outputId": "87996686-8b57-4e69-e9d7-afe2c4c85601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "trainset = get_pairwise_sequence_df(train_set)\n",
        "train_questions = trainset.question.values\n",
        "train_pos_answers = trainset.pos_ans.values\n",
        "train_neg_answers = trainset.neg_ans.values\n",
        "\n",
        "train_pos_labels = trainset.pos_label.values\n",
        "train_neg_labels = trainset.neg_label.values\n",
        "\n",
        "train_pos_input, train_pos_type_id, train_pos_att_mask = get_input(train_questions, train_pos_answers, 256)\n",
        "train_neg_input, train_neg_type_id, train_neg_att_mask = get_input(train_questions, train_neg_answers, 256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14612/14612 [01:03<00:00, 229.31it/s]\n",
            "100%|██████████| 14612/14612 [01:26<00:00, 169.56it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DsejCaRZ9dO",
        "colab_type": "code",
        "outputId": "fb7f7d87-9a30-4a44-94f9-be14760a95ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "validset = get_pairwise_sequence_df(valid_set)\n",
        "valid_questions = validset.question.values\n",
        "valid_pos_answers = validset.pos_ans.values\n",
        "valid_neg_answers = validset.neg_ans.values\n",
        "\n",
        "valid_pos_labels = validset.pos_label.values\n",
        "valid_neg_labels = validset.neg_label.values\n",
        "\n",
        "valid_pos_input, valid_pos_type_id, valid_pos_att_mask = get_input(valid_questions, valid_pos_answers, 256)\n",
        "valid_neg_input, valid_neg_type_id, valid_neg_att_mask = get_input(valid_questions, valid_neg_answers, 256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1595/1595 [00:07<00:00, 221.77it/s]\n",
            "100%|██████████| 1595/1595 [00:07<00:00, 216.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHGawog0Z461",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'/data-bert/train_pos_labels_small.pickle', train_pos_labels)\n",
        "save_pickle(path+'/data-bert/train_neg_labels_small.pickle', train_neg_labels)\n",
        "save_pickle(path+'/data-bert/valid_pos_labels_small.pickle', valid_pos_labels)\n",
        "save_pickle(path+'/data-bert/valid_neg_labels_small.pickle', valid_neg_labels)\n",
        "\n",
        "save_pickle(path+'/data-bert/train_pos_input_256_small.pickle', train_pos_input)\n",
        "save_pickle(path+'/data-bert/train_neg_input_256_small.pickle', train_neg_input)\n",
        "save_pickle(path+'/data-bert/valid_pos_input_256_small.pickle', valid_pos_input)\n",
        "save_pickle(path+'/data-bert/valid_neg_input_256_small.pickle', valid_neg_input)\n",
        "\n",
        "save_pickle(path+'/data-bert/train_pos_type_id_256_small.pickle', train_pos_type_id)\n",
        "save_pickle(path+'/data-bert/train_neg_type_id_256_small.pickle', train_neg_type_id)\n",
        "save_pickle(path+'/data-bert/valid_pos_type_id_256_small.pickle', valid_pos_type_id)\n",
        "save_pickle(path+'/data-bert/valid_neg_type_id_256_small.pickle', valid_neg_type_id)\n",
        "\n",
        "save_pickle(path+'/data-bert/train_pos_mask_256_small.pickle', train_pos_att_mask)\n",
        "save_pickle(path+'/data-bert/train_neg_mask_256_small.pickle', train_neg_att_mask)\n",
        "save_pickle(path+'/data-bert/valid_pos_mask_256_small.pickle', valid_pos_att_mask)\n",
        "save_pickle(path+'/data-bert/valid_neg_mask_256_small.pickle', valid_neg_att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdFPxvp-ahTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pos_labels = load_pickle(path+'/data-bert/train_pos_labels_small.pickle')\n",
        "train_neg_labels = load_pickle(path+'/data-bert/train_neg_labels_small.pickle')\n",
        "valid_pos_labels = load_pickle(path+'/data-bert/valid_pos_labels_small.pickle')\n",
        "valid_neg_labels = load_pickle(path+'/data-bert/valid_neg_labels_small.pickle')\n",
        "\n",
        "train_pos_input = load_pickle(path+'/data-bert/train_pos_input_256_small.pickle')\n",
        "train_neg_input = load_pickle(path+'/data-bert/train_neg_input_256_small.pickle')\n",
        "valid_pos_input = load_pickle(path+'/data-bert/valid_pos_input_256_small.pickle')\n",
        "valid_neg_input = load_pickle(path+'/data-bert/valid_neg_input_256_small.pickle')\n",
        "\n",
        "train_pos_type_id = load_pickle(path+'/data-bert/train_pos_type_id_256_small.pickle')\n",
        "train_neg_type_id = load_pickle(path+'/data-bert/train_neg_type_id_256_small.pickle')\n",
        "valid_pos_type_id = load_pickle(path+'/data-bert/valid_pos_type_id_256_small.pickle')\n",
        "valid_neg_type_id = load_pickle(path+'/data-bert/valid_neg_type_id_256_small.pickle')\n",
        "\n",
        "train_pos_mask = load_pickle(path+'/data-bert/train_pos_mask_256_small.pickle')\n",
        "train_neg_mask = load_pickle(path+'/data-bert/train_neg_mask_256_small.pickle')\n",
        "valid_pos_mask = load_pickle(path+'/data-bert/valid_pos_mask_256_small.pickle')\n",
        "valid_neg_mask = load_pickle(path+'/data-bert/valid_neg_mask_256_small.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7q6wCjj3XdB",
        "colab_type": "code",
        "outputId": "6ed15ae9-683b-434b-f9c4-da31edae4704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_pos_input))\n",
        "print(len(valid_pos_input))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14612\n",
            "1595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEL7jdoecvbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_pos_labels = train_pos_labels[:100]\n",
        "# train_neg_labels = train_neg_labels[:100]\n",
        "# train_pos_input = train_pos_input[:100]\n",
        "# train_neg_input = train_neg_input[:100]\n",
        "# train_pos_type_id = train_pos_type_id[:100]\n",
        "# train_neg_type_id = train_neg_type_id[:100]\n",
        "# train_pos_mask = train_pos_mask[:100]\n",
        "# train_neg_mask = train_neg_mask[:100]\n",
        "\n",
        "# valid_pos_labels = valid_pos_labels[:10]\n",
        "# valid_neg_labels = valid_neg_labels[:10]\n",
        "# valid_pos_input = valid_pos_input[:10]\n",
        "# valid_neg_input = valid_neg_input[:10]\n",
        "# valid_pos_type_id = valid_pos_type_id[:10]\n",
        "# valid_neg_type_id = valid_neg_type_id[:10]\n",
        "# valid_pos_mask = valid_pos_mask[:10]\n",
        "# valid_neg_mask = valid_neg_mask[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6c9oM8jauOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert lists to PyTorch tensors\n",
        "train_pos_inputs = torch.tensor(train_pos_input)\n",
        "train_neg_inputs = torch.tensor(train_neg_input)\n",
        "valid_pos_inputs = torch.tensor(valid_pos_input)\n",
        "valid_neg_inputs = torch.tensor(valid_neg_input)\n",
        "\n",
        "train_pos_labels = torch.tensor(train_pos_labels)\n",
        "train_neg_labels = torch.tensor(train_neg_labels)\n",
        "valid_pos_labels = torch.tensor(valid_pos_labels)\n",
        "valid_neg_labels = torch.tensor(valid_neg_labels)\n",
        "\n",
        "train_pos_type_ids = torch.tensor(train_pos_type_id)\n",
        "train_neg_type_ids = torch.tensor(train_neg_type_id)\n",
        "valid_pos_type_ids = torch.tensor(valid_pos_type_id)\n",
        "valid_neg_type_ids = torch.tensor(valid_neg_type_id)\n",
        "\n",
        "train_pos_masks = torch.tensor(train_pos_mask)\n",
        "train_neg_masks = torch.tensor(train_neg_mask)\n",
        "valid_pos_masks = torch.tensor(valid_pos_mask)\n",
        "valid_neg_masks = torch.tensor(valid_neg_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sBUeEuDau3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create DataLoaders to train the model in batches\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_pos_inputs, train_pos_type_ids, train_pos_masks, train_pos_labels, train_neg_inputs, train_neg_type_ids, train_neg_masks, train_neg_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(valid_pos_inputs, valid_pos_type_ids, valid_pos_masks, valid_pos_labels, valid_neg_inputs, valid_neg_type_ids, valid_neg_masks, valid_neg_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46AZOrGeawlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairwise_loss(pos_scores, neg_scores):\n",
        "    \"\"\"\n",
        "    Pairwise learning approach introduced in https://arxiv.org/pdf/1905.07588.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    cross_entropy_loss = -torch.log(pos_scores) - torch.log(1 - neg_scores)\n",
        "\n",
        "    margin = 0.5\n",
        "\n",
        "    hinge_loss = torch.max(torch.tensor(0, dtype=torch.float).to(device), margin - pos_scores + neg_scores)\n",
        "\n",
        "    loss = (0.5 * cross_entropy_loss + 0.5 * hinge_loss)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ofMf9vDazyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_pairwise(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "    # Store the average loss after each epoch so we can plot them.\n",
        "    loss_values = []\n",
        "\n",
        "    # Reset the loss and accuracy for each epoch\n",
        "    total_loss = 0\n",
        "    nb_train_steps = 0\n",
        "    train_accuracy = 0\n",
        "\n",
        "    # Set model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # batch contains eight PyTorch tensors:\n",
        "        pos_input = batch[0].to(device)\n",
        "        pos_type_id = batch[1].to(device)\n",
        "        pos_mask = batch[2].to(device)\n",
        "        pos_labels = batch[3].to(device)\n",
        "\n",
        "        neg_input = batch[4].to(device)\n",
        "        neg_type_id = batch[5].to(device)\n",
        "        neg_mask = batch[6].to(device)\n",
        "        neg_labels = batch[7].to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Compute predictinos for postive and negative QA pairs\n",
        "        pos_outputs = model(pos_input, token_type_ids=pos_type_id, attention_mask=pos_mask, labels=pos_labels)\n",
        "        neg_outputs = model(neg_input, token_type_ids=neg_type_id, attention_mask=neg_mask, labels=neg_labels)\n",
        "\n",
        "        # Get the logits from the model for positive and negative QA pairs\n",
        "        pos_logits = pos_outputs[1]\n",
        "        neg_logits = neg_outputs[1]\n",
        "\n",
        "        # Get the column of the relevant scores and apply activation function\n",
        "        pos_scores = softmax(pos_logits, dim=1)[:,1]\n",
        "        neg_scores = softmax(neg_logits, dim=1)[:,1]\n",
        "        \n",
        "        # Compute pairwise loss and get the mean of each batch\n",
        "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        p_logits = pos_logits.detach().cpu().numpy()\n",
        "        p_labels = pos_labels.to('cpu').numpy()\n",
        "        n_logits = neg_logits.detach().cpu().numpy()\n",
        "        n_labels = neg_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for each batch\n",
        "        tmp_pos_accuracy = flat_accuracy(p_logits, p_labels)\n",
        "        tmp_neg_accuracy = flat_accuracy(n_logits, n_labels)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        train_accuracy += tmp_pos_accuracy\n",
        "        train_accuracy += tmp_neg_accuracy\n",
        "        \n",
        "        # Track the number of batches (2 for pos and neg accuracies)\n",
        "        nb_train_steps += 2\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    # Compute accuracy for each epoch\n",
        "    acc = train_accuracy/nb_train_steps\n",
        "\n",
        "    return avg_train_loss, acc, loss_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wZBD9-BdFEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_pairwise(model, validation_dataloader):\n",
        "\n",
        "    # Set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    eval_accuracy = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        pos_input, pos_type_id, pos_mask, pos_labels, neg_input, neg_type_id, neg_mask, neg_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Compute predictinos for postive and negative QA pairs\n",
        "            pos_outputs = model(pos_input, token_type_ids=pos_type_id, attention_mask=pos_mask, labels=pos_labels)\n",
        "            neg_outputs = model(neg_input, token_type_ids=neg_type_id, attention_mask=neg_mask, labels=neg_labels)\n",
        "\n",
        "            # Get logits\n",
        "            pos_logits = pos_outputs[1]\n",
        "            neg_logits = neg_outputs[1]\n",
        "\n",
        "            # Apply activation function\n",
        "            pos_scores = softmax(pos_logits, dim=1)[:,1]\n",
        "            neg_scores = softmax(neg_logits, dim=1)[:,1]\n",
        "        \n",
        "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        p_logits = pos_logits.detach().cpu().numpy()\n",
        "        p_labels = pos_labels.to('cpu').numpy()\n",
        "        n_logits = neg_logits.detach().cpu().numpy()\n",
        "        n_labels = neg_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_pos_accuracy = flat_accuracy(p_logits, p_labels)\n",
        "        tmp_neg_accuracy = flat_accuracy(n_logits, n_labels)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_pos_accuracy\n",
        "        eval_accuracy += tmp_neg_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 2\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(validation_dataloader)\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxFYsHtADVla"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_zn-HiYolL",
        "colab_type": "code",
        "outputId": "3e3d4be0-a4ef-4f0d-8f28-875d87c4635f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ff4bf4681b34083a648d9e8cecd9a29",
            "df1c14e22ec4426fb24943f87ec298e6",
            "03f3357ebbc44686b8f5a7963c0da8ec",
            "91eccb2bb06f48678934cb279cc591ab",
            "b5807417ea7b46cb881bbb24944dd67c",
            "0d3c75c04894487cb30996c1f5e95e69",
            "81f937e77bca44ddb3a2d01b0cecae18",
            "d60dda4992eb4c9c9f62a51243c68c14",
            "a78be2e17b4f47069519967e1c011ffc",
            "57d97c57fc514fafbf3a69b2751b1d17",
            "8bae9f9c67b04e1c888616d74143bb80",
            "3890c797a4744787b3db695257f362cc",
            "e1fbc488ea73433bbd90a22515bd6e7c",
            "61c2e35d7c5b47928af9c11e9b309635",
            "e5d98815538f4bc7b10602d49565b73c",
            "6156ffce152f47bea285ff17bfcf3cb8"
          ]
        }
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top\n",
        "\n",
        "# model_path = \"/content/drive/My Drive/FiQA/model/fin_model\"\n",
        "# model = BertForSequenceClassification.from_pretrained(model_path, cache_dir=None, num_labels=2)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=None, num_labels=2)\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", cache_dir=None, num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ff4bf4681b34083a648d9e8cecd9a29",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a78be2e17b4f47069519967e1c011ffc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7wlYPSlZcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-7, eps = 1e-8)\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 6\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3lU_QLEl4lY",
        "colab_type": "code",
        "outputId": "adbfe1c9-9596-4476-bd76-d9e1faae6780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Evaluate training loss\n",
        "    # train_loss, train_acc, loss_values = train(model, train_dataloader, optimizer, scheduler)\n",
        "    train_loss, train_acc, loss_values = train_pairwise(model, train_dataloader, optimizer, scheduler)\n",
        "    # Evaluate validation loss\n",
        "    # valid_loss, valid_acc = validate(model, validation_dataloader)\n",
        "    valid_loss, valid_acc = validate_pairwise(model, validation_dataloader)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), path + 'model/' + str(epoch+1)+'_pairwise_e7.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 914/914 [12:17<00:00,  1.24it/s]\n",
            "100%|██████████| 100/100 [00:25<00:00,  3.96it/s]\n",
            "  0%|          | 0/914 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 1:\n",
            "\t Train Loss: 0.674 | Train Accuracy: 82.36%\n",
            "\t Validation Loss: 0.544 | Validation Accuracy: 89.02%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 309/914 [04:09<08:08,  1.24it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHBg5FXKOYHU"
      },
      "source": [
        "## **Evalulation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGbsz18-Urm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rank(model, test_set, qid_rel, max_seq_len):\n",
        "    \"\"\"\n",
        "    Returns a dictionary - key: qid, value: list of ranked candidates\n",
        "    -------------------\n",
        "    model - PyTorch model\n",
        "    test_set - List of lists:\n",
        "            Each element is a list contraining \n",
        "            [qid, list of pos docid, list of candidate docid]\n",
        "    qid_rel: Dictionary\n",
        "            key: qid, value: list of relevant answer id\n",
        "    max_seq_len: int\n",
        "            Maximum sequence length\n",
        "    \"\"\"\n",
        "\n",
        "    # Initiate empty dictionary\n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # For each element in the test set\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "        \n",
        "        # question id, list of rel answers, list of candidates\n",
        "        qid, label, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        # Convert list to numpy array\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        # Empty list for the probability scores of relevancy\n",
        "        scores = []\n",
        "\n",
        "        # For each answer in the candidates\n",
        "        for docid in cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "\n",
        "            # Create inputs for the model\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text, \n",
        "                                            max_length=max_seq_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "            # Numericalized, padded, clipped seq with special tokens\n",
        "            input_ids = torch.tensor([encoded_seq['input_ids']]).to(device)\n",
        "            # Specify question seq and answer seq\n",
        "            token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(device)\n",
        "            # Sepecify which position is part of the seq which is padded\n",
        "            att_mask = torch.tensor([encoded_seq['attention_mask']]).to(device)\n",
        "\n",
        "            # Don't calculate gradients\n",
        "            with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions for each QA pair\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=att_mask)\n",
        "\n",
        "            # Get the predictions\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Apply activation function\n",
        "            pred = softmax(logits, dim=1)\n",
        "            # pred = torch.sigmoid(logits)\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "\n",
        "            # Append relevant scores to list (where label = 1)\n",
        "            scores.append(pred[:,1][0])\n",
        "\n",
        "        # print(scores)\n",
        "\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        # Get the list of docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[qid] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOR-OM-qbb3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 3))\n",
        "toy_test = test_set[:3]\n",
        "# toy_test = [[14, [398960], [84963, 14255, 398960]],\n",
        "#             [68, [19183], [107584, 562777, 19183]],\n",
        "#             [70, [327002], [107584, 327002, 19183]]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa0KvT4cbbo",
        "colab_type": "code",
        "outputId": "dd752327-b771-4f7c-90d9-7fcf27137320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.load_state_dict(torch.load(path+'model/6_pairwise_001.pt'))\n",
        "\n",
        "qid_pred_rank = get_rank(model, test_set, test_qid_rel, max_seq_len=256)\n",
        "# qid_pred_rank = get_rank(model, toy_test, toy_test_label, max_seq_len=256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 333/333 [59:25<00:00, 10.75s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTc2SQVhuv2Q",
        "colab_type": "code",
        "outputId": "0c33fd38-0658-40f6-fd0d-d0230d7dabb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "k = 10\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, test_qid_rel, k)\n",
        "# MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, toy_test_label, k)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 333 queries: 0.23927752184002785\n",
            "\n",
            "MRR@10 for 333 queries: 0.16802278468945148\n",
            "\n",
            "Average Precision@1: 0.08408408408408409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcfbXb5eBcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'rank/rank_pairwise_001.pickle', qid_pred_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}