{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statistics import mean \n",
    "import math \n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question ID and Answer ID pair\n",
    "qid_docid = pd.read_csv(\"train/FiQA_train_question_doc_final.tsv\", sep=\"\\t\")\n",
    "qid_docid = qid_docid [['qid', 'docid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict for question id and relevant passgages\n",
    "# keys: query ids, values: list of relevant passages\n",
    "qid_rel = {}\n",
    "\n",
    "for index, row in qid_docid.iterrows():\n",
    "    \n",
    "    if row['qid'] not in qid_rel:\n",
    "        qid_rel[row['qid']] = []\n",
    "    qid_rel[row['qid']].append(row['docid'])\n",
    "    \n",
    "# take(10, qid_rel.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of relevant passages for each query\n",
    "num_rel = [len(v) for v in qid_rel.values()]\n",
    "\n",
    "avg_num_rel = mean(num_rel)\n",
    "max_num_rel = max(num_rel)\n",
    "min_num_rel = min(num_rel)\n",
    "\n",
    "print(\"Average number of relevant passages for each query: {}\\n\".format(avg_num_rel))\n",
    "print(\"Max number of relevant passages for each query: {}\\n\".format(max_num_rel))\n",
    "print(\"Min number of relevant passages for each query: {}\\n\".format(min_num_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Ranking for each question\n",
    "doc_ranking = pd.read_csv(\"fiqa-passage/run2_train.tsv\", sep=\"\\t\", header=None)\n",
    "doc_ranking = doc_ranking.rename(columns={1: 'qid', 2: 'doc_id', 3:'rank'})\n",
    "# doc_ranking.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of candidate answers for all questions: {}\".format(len(doc_ranking)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict for query id and ranked candidates\n",
    "# key: query ids, values: list of 1000 ranked candidates\n",
    "qid_ranked_docs = {}\n",
    "\n",
    "with open(\"fiqa-passage/run2_train.tsv\",'r') as f:\n",
    "    for line in f:\n",
    "        # [qid, doc_id, rank]\n",
    "        line = line.strip().split('\\t')\n",
    "        qid = int(line[0])\n",
    "        doc_id = int(line[1])\n",
    "        rank = int(line[2])\n",
    "        \n",
    "        if qid not in qid_ranked_docs:\n",
    "            # Create a list of size 1000 for each query to store the candidates\n",
    "            candidates = [0]*1000\n",
    "            qid_ranked_docs[qid] = candidates\n",
    "        qid_ranked_docs[qid][rank-1] = doc_id\n",
    "        \n",
    "# take(1, qid_ranked_docs.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for evaluation\n",
    "def get_rel_score(rel_scores, cand_docs, rel_docs, k):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of the top-k relevancy scores of docs in the candidate answers\n",
    "    \n",
    "    rel_scores - empty_dict\n",
    "    \n",
    "    key - question id\n",
    "    value - list of relevancy scores with 1 being relevant and 0 being irrelevant\n",
    "    \n",
    "    Example: {0: [0, 1, 0], 1: [1, 1, 0]}\n",
    "    \"\"\"\n",
    "    if qid not in rel_scores:\n",
    "        rel_scores[qid] = []\n",
    "\n",
    "        for i in range(0, k):\n",
    "            if cand_docs[i] in rel_docs:\n",
    "                rel_scores[qid].append(1)\n",
    "            else:\n",
    "                rel_scores[qid].append(0)\n",
    "\n",
    "    return rel_scores\n",
    "\n",
    "def dcg(rels, k):\n",
    "    \"\"\"\n",
    "    Discounted Cumulative Gain\n",
    "    \n",
    "    Returns the cumulated DCG of the top-k relevant docs across all queries\n",
    "    \"\"\"\n",
    "    cumulated_sum = rels[0]\n",
    "    for i in range(1, k):\n",
    "        cumulated_sum += rels[i]/math.log(i+1,2)\n",
    "    return cumulated_sum\n",
    "\n",
    "def avg_ndcg(rel_score):\n",
    "    \"\"\"\n",
    "    Average Normalized Discounted Cumulative Gain\n",
    "    \n",
    "    Computes the DCG, iDCG, and nDCG for each query\n",
    "    \n",
    "    Returns the averyage nDCG across all queries\n",
    "    \"\"\"\n",
    "    ndcg_list = []\n",
    "    for qid, rels in rel_score.items():\n",
    "        dcg_val = dcg(rels, k)   \n",
    "        sorted_rel = sorted(rels, reverse=True)\n",
    "        idcg_val = dcg(sorted_rel, k)\n",
    "\n",
    "        try:\n",
    "            ndcg_val = dcg_val/idcg_val\n",
    "            ndcg_list.append(ndcg_val)\n",
    "        except ZeroDivisionError:\n",
    "            ndcg_list.append(0)\n",
    "            \n",
    "    assert len(ndcg_list) == len(rel_score), \"Relevant score doesn't match\"\n",
    "\n",
    "    avg = mean(ndcg_list)\n",
    "\n",
    "    return avg\n",
    "\n",
    "def compute_RR(cand_docs, rel_docs, cumulated_reciprocal_rank, rank_pos, k):\n",
    "    \"\"\"\n",
    "    Computes the reciprocal rank - probability of correctness of rank\n",
    "    \n",
    "    Returns the cumulated reciprocal rank across all queries and the\n",
    "    positions of the relevant docs in the candidates\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, k):\n",
    "        # If the doc_id of the top k ranked candidate passages is in the list of relevant passages\n",
    "        if cand_docs[i] in rel_docs:\n",
    "            # Compute the reciprocal rank (i is the ranking)\n",
    "            rank_pos.append(i+1)\n",
    "            cumulated_reciprocal_rank += 1/(i+1)\n",
    "            break\n",
    "            \n",
    "    return cumulated_reciprocal_rank, rank_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate top-1000 candidates\n",
    "def evaluate(qid_ranked_docs, qid_rel, k):\n",
    "    \"\"\"\n",
    "    qid_ranked_docs: dict - key - qid, value - list of cand ans\n",
    "    qid_rel:  key- qid, value - list of relevant ans\n",
    "    \"\"\"\n",
    "    cumulated_reciprocal_rank = 0\n",
    "    num_rel_docs = 0\n",
    "    rel_scores = {}\n",
    "    precision_list = {}\n",
    "    rank_pos = []\n",
    "    \n",
    "    # For each query\n",
    "    for qid in qid_ranked_docs:\n",
    "        # If the query has a relevant passage\n",
    "        if qid in qid_rel:\n",
    "            # Get the list of relevant docs for a query\n",
    "            rel_docs = qid_rel[qid]\n",
    "            # Get the list of ranked docs for a query\n",
    "            cand_docs = qid_ranked_docs[qid]\n",
    "            # Compute relevant scores of the candidates\n",
    "            #scores = get_rel_score(rel_scores, cand_docs, rel_docs, k)\n",
    "            if qid not in rel_scores:\n",
    "                rel_scores[qid] = []\n",
    "\n",
    "                for i in range(0, k):\n",
    "                    if cand_docs[i] in rel_docs:\n",
    "                        rel_scores[qid].append(1)\n",
    "                    else:\n",
    "                        rel_scores[qid].append(0)\n",
    "            # MRR@k\n",
    "            cumulated_reciprocal_rank, r_pos = compute_RR(cand_docs, rel_docs, cumulated_reciprocal_rank, rank_pos, k)\n",
    "        \n",
    "    MRR = cumulated_reciprocal_rank/len(qid_rel)\n",
    "    average_ndcg = avg_ndcg(scores)\n",
    "    \n",
    "    precision_at_k = []\n",
    "    for qid, score in rel_scores.items():\n",
    "        num_rel = 0\n",
    "        for i in range(0, 1):\n",
    "            if score[i] == 1:\n",
    "                num_rel += 1\n",
    "        precision_at_k.append(num_rel/1) \n",
    "    \n",
    "    return MRR, average_ndcg, mean(precision_at_k)\n",
    "\n",
    "def precision(rel_scores, k):\n",
    "    \n",
    "    precision_at_k = []\n",
    "    for qid, score in rel_scores.items():\n",
    "        num_rel = 0\n",
    "        for i in range(0, k):\n",
    "            if score[i] == 1:\n",
    "                num_rel += 1\n",
    "        precision_at_k.append(num_rel/k)     \n",
    "        \n",
    "    return mean(precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average nDCG@10 for 6648 queries: 0.3638862156147449\n",
      "\n",
      "MRR@10 for 6648 queries: 0.3092924474242155\n",
      "\n",
      "Average Precision@1: 0.2378158844765343\n"
     ]
    }
   ],
   "source": [
    "num_q = len(qid_rel)\n",
    "k = 10\n",
    "\n",
    "MRR, average_ndcg, precision = evaluate(qid_ranked_docs, qid_rel, k)\n",
    "\n",
    "print(\"Average nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
    "\n",
    "\n",
    "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
    "\n",
    "print(\"Average Precision@{}: {}\".format(1, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate top-1000 candidates\n",
    "k = 1000\n",
    "cumulated_reciprocal_rank = 0\n",
    "num_rel_docs = 0\n",
    "rel_score = {}\n",
    "precision_list = {}\n",
    "rank_pos = []\n",
    "\n",
    "# For each query\n",
    "for qid in qid_ranked_docs:\n",
    "    # If the query has a relevant passage\n",
    "    if qid in qid_rel:\n",
    "        # Get the list of relevant docs for a query\n",
    "        rel_docs = qid_rel[qid]\n",
    "        # Get the list of ranked docs for a query\n",
    "        cand_docs = qid_ranked_docs[qid]\n",
    "        # Compute relevant scores of the candidates\n",
    "        rel_scores = get_rel_score(rel_score, cand_docs, rel_docs, k)\n",
    "        # MRR@k\n",
    "        cumulated_reciprocal_rank, r_pos = compute_RR(cand_docs, rel_docs, cumulated_reciprocal_rank, rank_pos, k)\n",
    "        \n",
    "\n",
    "print(\"Average nDCG@{} for {} queries: {}\".format(k, len(qid_rel), avg_ndcg(rel_scores)))\n",
    "print()\n",
    "\n",
    "MRR = cumulated_reciprocal_rank/len(qid_rel)\n",
    "\n",
    "print(\"MRR@{} for {} queries: {}\\n\".format(k, len(qid_rel), MRR))\n",
    "\n",
    "# Mean precision at k\n",
    "k = 1\n",
    "scores = {}\n",
    "precision_at_k = []\n",
    "for qid, scores in rel_scores.items():\n",
    "    num_rel = 0\n",
    "    for i in range(0, k):\n",
    "        if scores[i] == 1:\n",
    "            num_rel += 1\n",
    "    precision_at_k.append(num_rel/k)     \n",
    "mean_precision_at_k = mean(precision_at_k)\n",
    "\n",
    "print(\"Average Precision@{}: {}\".format(k, mean_precision_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The lowest rank of the first relevant passage: {}\\n\".format(max(r_pos)))\n",
    "print(\"The highest rank of the first relevant passage: {}\\n \".format(min(r_pos)))\n",
    "print(\"The average rank of the first relevant passage {}\\n\".format(mean(r_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any candidate passages don't contain any relevant passages\n",
    "counter = 0\n",
    "for qid, scores in rel_scores.items():\n",
    "    np_scores = np.asarray(scores)\n",
    "    \n",
    "    if np.all(np_scores==0):\n",
    "        counter += 0\n",
    "\n",
    "print(\"Number of candidate passage that don't contrain any relevant passages: {}\".format(counter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
