{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "smallBERTa_Pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Pirlhhohke8T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4OynugZvMG2",
        "colab_type": "text"
      },
      "source": [
        "# Pre-training SmallBERTa - A tiny model to train on a tiny dataset\n",
        "(Using HuggingFace Transformers)<br>\n",
        "Admittedly, while language modeling is associated with terabytes of data, not all of use have either the processing power nor the resources to train huge models on such huge amounts of data.\n",
        "In this example, we are going to train a relatively small neural net on a small dataset (which still happens to have over 2M rows).\n",
        "<br>\n",
        "\n",
        "The ***main purpose*** of this blog is not to achieve state-of-the-art performance on LM tasks but to show a simple idea of how the recent language_modeling.py script can be used to train a Transformer model from scratch.\n",
        "\n",
        "This very notebook can be extended to various esoteric use cases where general purpose pre-trained models fail to perform well. Examples include medical dataset, scientific literature, legal documentation, etc.\n",
        "\n",
        "Input:\n",
        "  1. To the Tokenizer:<br>\n",
        "      LM data in a directory containing all samples in separate *.txt files.\n",
        "  \n",
        "  2. To the Model:<br>\n",
        "      LM data split into:<br>\n",
        "        1. train.txt <br>\n",
        "        2. eval.txt \n",
        "        \n",
        "Output:<br>\n",
        "  Trained Model weights(that can be used elsewhere) and Tensorboard logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sHQ_tWig474",
        "colab_type": "text"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPxoElNugaMu",
        "colab_type": "code",
        "outputId": "803bee43-0c9b-4f04-d659-7bdced6da0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "#tokenizer working version --- 0.5.0\n",
        "#transformer working version --- 2.5.0\n",
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install tensorboard==2.1.0"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: tensorboard==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (1.27.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (2.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (45.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (1.17.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (0.34.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0) (3.2.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.1.0) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBcbCQoEg9cT",
        "colab_type": "text"
      },
      "source": [
        "## Fetch Data\n",
        "We will be using a tiny dataset(The Examiner - SpamClickBait News) of around 3M rows from kaggle to train our model. The dataset also contains output labels which will be dropped and only the text shall be used. For convenience we are using the Kaggle API to direcltly download the data from Kaggle to save our time and efforts. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtFnApKwiGUb",
        "colab_type": "code",
        "outputId": "99c4c4e6-147a-46ae-91da-d89a148a6c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "#For a kaggle username & key, just go to your kaggle account and generate key\n",
        "#The JSON file so downloaded contains both of them\n",
        "if(\"examine-the-examiner.zip\" not in os.listdir()):\n",
        "  print(\"Copy these two values from the JSON file so generated\")\n",
        "  os.environ['KAGGLE_USERNAME'] = getpass.getpass(prompt='Kaggle username: ') \n",
        "  os.environ['KAGGLE_KEY'] =  getpass.getpass(prompt='Kaggle key: ')\n",
        "  !kaggle datasets download -d therohk/examine-the-examiner\n",
        "  !unzip /content/examine-the-examiner.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copy these two values from the JSON file so generated\n",
            "Kaggle username: ··········\n",
            "Kaggle key: ··········\n",
            "Downloading examine-the-examiner.zip to /content\n",
            " 86% 123M/142M [00:00<00:00, 132MB/s]\n",
            "100% 142M/142M [00:00<00:00, 163MB/s]\n",
            "Archive:  /content/examine-the-examiner.zip\n",
            "  inflating: examiner-date-text.csv  \n",
            "  inflating: examiner-date-tokens.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ7hj9kuhBIj",
        "colab_type": "text"
      },
      "source": [
        "## Load and Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOG-fl1cGhJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import regex as re\n",
        "def basicPreprocess(text):\n",
        "  try:\n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'\\W +', ' ', processed_text)\n",
        "  except Exception as e:\n",
        "    print(\"Exception:\",e,\",on text:\", text)\n",
        "    return None\n",
        "  return processed_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn68O17MsqYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCWVENk1RODl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "aeabbffa-13b8-41dc-f39c-42424ffd20db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUtf-gZ_hWEE",
        "colab_type": "text"
      },
      "source": [
        "## Read and Prune the data\n",
        "For our purpose we are going to read a subset (~200,000 samples) to train, just to see results quickly. Feel free to increase (or remove) this limitation.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj7Bo6hMiySr",
        "colab_type": "code",
        "outputId": "2d141594-b14e-4cf6-ea70-876589d96f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/examiner-date-text.csv\")\n",
        "print(data)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         publish_date                                      headline_text\n",
            "0            20100101       100 Most Anticipated books releasing in 2010\n",
            "1            20100101       10 best films of 2009 - What's on your list?\n",
            "2            20100101  10 days of free admission at Lan Su Chinese Ga...\n",
            "3            20100101      10 PlayStation games to watch out for in 2010\n",
            "4            20100101  10 resolutions for a Happy New Year for you an...\n",
            "...               ...                                                ...\n",
            "3089776      20151231  Which is better investment, Lego bricks or gol...\n",
            "3089777      20151231  Wild score three unanswered goals to defeat th...\n",
            "3089778      20151231  With NASA and Russia on the sidelines, Europe ...\n",
            "3089779      20151231  Wolf Pack battling opponents, officials on the...\n",
            "3089780      20151231          Writespace hosts all genre open mic night\n",
            "\n",
            "[3089781 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JaLDYtAnZIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.sample(frac=1).sample(frac=1)\n",
        "data = data[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYYUOhiXhHP8",
        "colab_type": "text"
      },
      "source": [
        "### Before Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8STrareTIxox",
        "colab_type": "code",
        "outputId": "530a7e6a-d898-4db4-fa65-fb5c8afc6e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(data)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         publish_date                                      headline_text\n",
            "2404600      20130531  ‘Pokemon X’ and ‘Pokemon Y’ EVs attribute info...\n",
            "1029672      20110207    SFA's 'A Cappella Choir' Presents Concert Today\n",
            "1080823      20110302  Nintendo President reveals 3DS Super Mario at ...\n",
            "968212       20110111                 The Whisky a go-go opens its doors\n",
            "2653615      20140125                Maryland's media: can it be trusted\n",
            "...               ...                                                ...\n",
            "1401063      20110829  The New York Mets launched Anti-Bullying Campa...\n",
            "1268249      20110607                   THIN AIR features Peter Robinson\n",
            "2609929      20131211  Free ‘GTA V’ Online deathmatch & race creators...\n",
            "1690518      20120218                         Ubu Rex and the ridiculous\n",
            "1977367      20120814  Gamescom 2012: Battlefield 3 Armored Kill scre...\n",
            "\n",
            "[5000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8md5U5tGx1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"headline_text\"] = data[\"headline_text\"].apply(basicPreprocess).dropna() #ignore exception if for empty/nan values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCzsk_sVhLsi",
        "colab_type": "text"
      },
      "source": [
        "### After Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV8ysU3cI1a-",
        "colab_type": "code",
        "outputId": "131c2cae-7390-427c-a87b-cfa01e74f55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(data)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         publish_date                                      headline_text\n",
            "2404600      20130531  ‘pokemon x and ‘pokemon y evs attribute inform...\n",
            "1029672      20110207     sfa's 'a cappella choir presents concert today\n",
            "1080823      20110302  nintendo president reveals 3ds super mario at ...\n",
            "968212       20110111                 the whisky a go-go opens its doors\n",
            "2653615      20140125                 maryland's media can it be trusted\n",
            "...               ...                                                ...\n",
            "1401063      20110829  the new york mets launched anti-bullying campa...\n",
            "1268249      20110607                   thin air features peter robinson\n",
            "2609929      20131211  free ‘gta v online deathmatch  race creators u...\n",
            "1690518      20120218                         ubu rex and the ridiculous\n",
            "1977367      20120814  gamescom 2012 battlefield 3 armored kill scree...\n",
            "\n",
            "[5000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbp40Xkrhs8l",
        "colab_type": "text"
      },
      "source": [
        "Removing newline characters just in case the input text has them. This is because the LineByLine class that we are going to use later assumes that samples are separated by newline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoL1ubfgSOtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "83b48fcf-f661-4d2e-a746-ca4f947fc680"
      },
      "source": [
        "data = data[['headline_text']]\n",
        "print(data)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             headline_text\n",
            "2404600  ‘pokemon x and ‘pokemon y evs attribute inform...\n",
            "1029672     sfa's 'a cappella choir presents concert today\n",
            "1080823  nintendo president reveals 3ds super mario at ...\n",
            "968212                  the whisky a go-go opens its doors\n",
            "2653615                 maryland's media can it be trusted\n",
            "...                                                    ...\n",
            "1401063  the new york mets launched anti-bullying campa...\n",
            "1268249                   thin air features peter robinson\n",
            "2609929  free ‘gta v online deathmatch  race creators u...\n",
            "1690518                         ubu rex and the ridiculous\n",
            "1977367  gamescom 2012 battlefield 3 armored kill scree...\n",
            "\n",
            "[5000 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVBYYA0qSjiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data.to_csv('data.txt',index=False,header=False, quoting=csv.QUOTE_NONE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9GH6nkKRcEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a9d2a7d-c2c9-4a69-97ad-5c0cf0c645bd"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer, BertWordPieceTokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\"/content\").glob(\"**/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"[UNK]\",\n",
        "    \"[SEP]\",\n",
        "    \"[CLS]\",\n",
        "    \"[MASK]\",\n",
        "    \"[PAD]\",\n",
        "])\n",
        "\n",
        "# Save files to disk\n",
        "tokenizer.save(\".\", \"smallbert\")"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./smallbert-vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRWXa3ltUJVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/smallbert-vocab.txt /content/vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOBmLbcUUch",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "94f64845-a7e1-455a-899c-4a2a282a2e31"
      },
      "source": [
        "tokenizer = BertWordPieceTokenizer(\"/content/vocab.txt\")\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"[SEP]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "    (\"[CLS]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "\n",
        "print(\n",
        "    tokenizer.encode(\"the whisky a go-go opens its doors\").tokens\n",
        ")"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'the', 'whisky', 'a', 'go', '-', 'go', 'opens', 'its', 'doors', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo_0p2WdWLN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI1Tp54IiVBj",
        "colab_type": "text"
      },
      "source": [
        "## Train a custom tokenizer\n",
        "I have used a ByteLevelBPETokenizer just to prevent \\<unk> tokens entirely.\n",
        "Furthermore, the function used to train the tokenizer assumes that each sample is stored in a different text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs-wK-N1EACp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt_files_dir = \"/tmp/text_split\"\n",
        "!mkdir {txt_files_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIvCE_svi7sQ",
        "colab_type": "text"
      },
      "source": [
        "Split LM data into individual files. These files are stored in /tmp/text_split and are used to train the tokenizer **only**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2oI92Z0tyAp",
        "colab_type": "code",
        "outputId": "022fc930-6312-4e83-eb4f-24b68e0b0394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "i=0\n",
        "for row in tqdm(data.to_list()):\n",
        "  file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
        "  try:\n",
        "    f = open(file_name, 'w')\n",
        "    f.write(row)\n",
        "    f.close()\n",
        "  except Exception as e:  #catch exceptions(for eg. empty rows)\n",
        "    print(row, e) \n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200000/200000 [00:09<00:00, 20693.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bv78Z2UjIci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm_data_dir = \"/tmp/lm_data\"\n",
        "!mkdir {lm_data_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI5kEwUojOQo",
        "colab_type": "text"
      },
      "source": [
        "## Split into Valdation and Train set\n",
        "We split the train data into validation and train. These two files are used to train and evaluate our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nWv7Yuki66k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_split = 0.9\n",
        "train_data_size = int(len(data)*train_split)\n",
        "\n",
        "with open(os.path.join(lm_data_dir,'train.txt') , 'w') as f:\n",
        "    for item in data[:train_data_size].tolist():\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "with open(os.path.join(lm_data_dir,'eval.txt') , 'w') as f:\n",
        "    for item in data[train_data_size:].tolist():\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKaVWBiVTtEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/models\n",
        "!mkdir /content/models/smallBERTa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noQfBUkhJmFC",
        "colab_type": "code",
        "outputId": "a808ab2d-baa8-47d2-bd06-60b88c5c19e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer.save(\"/content/models/smallBERTa\", \"smallBERTa\")"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/models/smallBERTa/smallBERTa-vocab.json',\n",
              " '/content/models/smallBERTa/smallBERTa-merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odSTiCM--4_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/models/smallBERTa/smallBERTa-vocab.json /content/models/smallBERTa/vocab.json\n",
        "!mv /content/models/smallBERTa/smallBERTa-merges.txt /content/models/smallBERTa/merges.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naEJbZDjFnNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = os.path.join(lm_data_dir,\"train.txt\")\n",
        "eval_path = os.path.join(lm_data_dir,\"eval.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P91yVQkXj9rc",
        "colab_type": "text"
      },
      "source": [
        "## Set Model Configuration\n",
        "For our purpose, we are training a very small model for demo purposes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS4q1YtxZ2GW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.3,\n",
        "  \"hidden_size\": 128,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"num_attention_heads\": 1,\n",
        "  \"num_hidden_layers\": 1,\n",
        "  \"vocab_size\": vocab_size,\n",
        "  \"intermediate_size\": 256,\n",
        "  \"max_position_embeddings\": 256\n",
        "}\n",
        "with open(\"/content/models/smallBert/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CbVBgrDbmVJ2",
        "outputId": "160bd4f1-ae4b-474e-bb4f-19a8907d05e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "#%cd /content\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 19858 (delta 5), reused 6 (delta 0), pack-reused 19834\u001b[K\n",
            "Receiving objects: 100% (19858/19858), 11.95 MiB | 4.05 MiB/s, done.\n",
            "Resolving deltas: 100% (14423/14423), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZMJ0zMxDIyc",
        "colab_type": "text"
      },
      "source": [
        "## Run training using the run_language_modeling.py examples script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kvkxHIk2Vgn",
        "colab_type": "code",
        "outputId": "7dbd97f4-e05b-4158-86a5-083818c57082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "!nvidia-smi #just to confirm that you are on a GPU, if not go to Runtime->Change Runtime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Feb 21 12:17:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk2MUnKFV58z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d504e5fa-007e-440e-ab2e-d96275a85dea"
      },
      "source": [
        "#Setting environment variables\n",
        "os.environ[\"train_path\"] = train_path\n",
        "os.environ[\"eval_path\"] = eval_path\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]='1'  #Makes for easier debugging (just in case)\n",
        "weights_dir = \"/content/models/smallBert/weights\"\n",
        "!mkdir {weights_dir}"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/models/smallBert/weights’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGRn_wgtW-J8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb62b846-488c-493e-d810-3268128b28b8"
      },
      "source": [
        "train_path"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/tmp/lm_data/train.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sIv6YB7WmJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/models/smallBert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6rTamw_WsTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/vocab.txt /content/models/smallBert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UJ_BSAlmccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmd = '''python /content/transformers/examples/run_language_modeling.py --output_dir {0}  \\\n",
        "    --model_type bert \\\n",
        "    --mlm \\\n",
        "    --train_data_file {1} \\\n",
        "    --eval_data_file {2} \\\n",
        "    --config_name /content/models/smallBert \\\n",
        "    --tokenizer_name /content/models/smallBert \\\n",
        "    --do_train \\\n",
        "    --line_by_line \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_eval \\\n",
        "    --block_size 256 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --save_total_limit 2 \\\n",
        "    --save_steps 2000 \\\n",
        "    --logging_steps 500 \\\n",
        "    --per_gpu_eval_batch_size 32 \\\n",
        "    --per_gpu_train_batch_size 32 \\\n",
        "    --evaluate_during_training \\\n",
        "    --seed 42 \\\n",
        "    '''.format(weights_dir, train_path, eval_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqhJzq03Fc15",
        "colab_type": "code",
        "outputId": "09b82d4d-db00-4a6d-8e1c-123581efaaa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!{cmd}"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/27/2020 11:15:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/27/2020 11:15:58 - INFO - transformers.configuration_utils -   loading configuration file /content/models/smallBert/config.json\n",
            "02/27/2020 11:15:58 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": null,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 128,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 256,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 256,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 5000\n",
            "}\n",
            "\n",
            "02/27/2020 11:15:58 - INFO - transformers.tokenization_utils -   Model name '/content/models/smallBert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/models/smallBert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/27/2020 11:15:58 - INFO - transformers.tokenization_utils -   Didn't find file /content/models/smallBert/added_tokens.json. We won't load it.\n",
            "02/27/2020 11:15:58 - INFO - transformers.tokenization_utils -   Didn't find file /content/models/smallBert/special_tokens_map.json. We won't load it.\n",
            "02/27/2020 11:15:58 - INFO - transformers.tokenization_utils -   Didn't find file /content/models/smallBert/tokenizer_config.json. We won't load it.\n",
            "02/27/2020 11:15:58 - INFO - transformers.tokenization_utils -   loading file /content/models/smallBert/vocab.txt\n",
            "02/27/2020 11:15:58 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2020 11:15:58 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2020 11:15:58 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2020 11:15:58 - INFO - __main__ -   Training new model from scratch\n",
            "02/27/2020 11:16:02 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=256, cache_dir=None, config_name='/content/models/smallBert', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/tmp/lm_data/eval.txt', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.0001, line_by_line=True, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=5.0, output_dir='/content/models/smallBert/weights', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=32, save_steps=2000, save_total_limit=2, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='/content/models/smallBert', train_data_file='/tmp/lm_data/train.txt', warmup_steps=0, weight_decay=0.0)\n",
            "02/27/2020 11:16:02 - INFO - __main__ -   Creating features from dataset file at /tmp/lm_data/train.txt\n",
            "02/27/2020 11:16:03 - INFO - __main__ -   ***** Running training *****\n",
            "02/27/2020 11:16:03 - INFO - __main__ -     Num examples = 4500\n",
            "02/27/2020 11:16:03 - INFO - __main__ -     Num Epochs = 5\n",
            "02/27/2020 11:16:03 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "02/27/2020 11:16:03 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "02/27/2020 11:16:03 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/27/2020 11:16:03 - INFO - __main__ -     Total optimization steps = 705\n",
            "Epoch:   0% 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/141 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   4% 5/141 [00:00<00:03, 42.46it/s]\u001b[A\n",
            "Iteration:   7% 10/141 [00:00<00:03, 43.49it/s]\u001b[A\n",
            "Iteration:  11% 15/141 [00:00<00:02, 43.30it/s]\u001b[A\n",
            "Iteration:  14% 20/141 [00:00<00:02, 44.24it/s]\u001b[A\n",
            "Iteration:  18% 25/141 [00:00<00:02, 45.00it/s]\u001b[A\n",
            "Iteration:  21% 30/141 [00:00<00:02, 44.82it/s]\u001b[A\n",
            "Iteration:  25% 35/141 [00:00<00:02, 44.82it/s]\u001b[A\n",
            "Iteration:  28% 40/141 [00:00<00:02, 43.01it/s]\u001b[A\n",
            "Iteration:  32% 45/141 [00:01<00:02, 42.92it/s]\u001b[A\n",
            "Iteration:  35% 50/141 [00:01<00:02, 43.79it/s]\u001b[A\n",
            "Iteration:  39% 55/141 [00:01<00:01, 44.92it/s]\u001b[A\n",
            "Iteration:  43% 60/141 [00:01<00:01, 45.06it/s]\u001b[A\n",
            "Iteration:  46% 65/141 [00:01<00:01, 45.67it/s]\u001b[A\n",
            "Iteration:  50% 70/141 [00:01<00:01, 46.35it/s]\u001b[A\n",
            "Iteration:  53% 75/141 [00:01<00:01, 46.57it/s]\u001b[A\n",
            "Iteration:  57% 80/141 [00:01<00:01, 45.99it/s]\u001b[A\n",
            "Iteration:  60% 85/141 [00:01<00:01, 45.33it/s]\u001b[A\n",
            "Iteration:  64% 90/141 [00:02<00:01, 44.50it/s]\u001b[A\n",
            "Iteration:  67% 95/141 [00:02<00:01, 45.37it/s]\u001b[A\n",
            "Iteration:  71% 100/141 [00:02<00:00, 44.97it/s]\u001b[A\n",
            "Iteration:  74% 105/141 [00:02<00:00, 46.31it/s]\u001b[A\n",
            "Iteration:  78% 110/141 [00:02<00:00, 46.86it/s]\u001b[A\n",
            "Iteration:  82% 115/141 [00:02<00:00, 47.36it/s]\u001b[A\n",
            "Iteration:  85% 120/141 [00:02<00:00, 47.81it/s]\u001b[A\n",
            "Iteration:  89% 125/141 [00:02<00:00, 48.10it/s]\u001b[A\n",
            "Iteration:  92% 130/141 [00:02<00:00, 48.46it/s]\u001b[A\n",
            "Iteration:  96% 135/141 [00:02<00:00, 47.67it/s]\u001b[A\n",
            "Iteration:  99% 140/141 [00:03<00:00, 46.18it/s]\u001b[A\n",
            "Epoch:  20% 1/5 [00:03<00:12,  3.08s/it]\n",
            "Iteration:   0% 0/141 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   4% 5/141 [00:00<00:03, 41.31it/s]\u001b[A\n",
            "Iteration:   7% 10/141 [00:00<00:03, 42.38it/s]\u001b[A\n",
            "Iteration:  11% 15/141 [00:00<00:02, 42.94it/s]\u001b[A\n",
            "Iteration:  14% 20/141 [00:00<00:02, 44.62it/s]\u001b[A\n",
            "Iteration:  18% 25/141 [00:00<00:02, 44.73it/s]\u001b[A\n",
            "Iteration:  21% 30/141 [00:00<00:02, 43.56it/s]\u001b[A\n",
            "Iteration:  25% 35/141 [00:00<00:02, 45.11it/s]\u001b[A\n",
            "Iteration:  28% 40/141 [00:00<00:02, 45.38it/s]\u001b[A\n",
            "Iteration:  32% 45/141 [00:00<00:02, 46.07it/s]\u001b[A\n",
            "Iteration:  36% 51/141 [00:01<00:01, 47.19it/s]\u001b[A\n",
            "Iteration:  40% 56/141 [00:01<00:01, 47.97it/s]\u001b[A\n",
            "Iteration:  43% 61/141 [00:01<00:01, 47.21it/s]\u001b[A\n",
            "Iteration:  47% 66/141 [00:01<00:01, 47.27it/s]\u001b[A\n",
            "Iteration:  50% 71/141 [00:01<00:01, 47.95it/s]\u001b[A\n",
            "Iteration:  54% 76/141 [00:01<00:01, 47.04it/s]\u001b[A\n",
            "Iteration:  58% 82/141 [00:01<00:01, 47.53it/s]\u001b[A\n",
            "Iteration:  62% 87/141 [00:01<00:01, 46.97it/s]\u001b[A\n",
            "Iteration:  65% 92/141 [00:01<00:01, 46.86it/s]\u001b[A\n",
            "Iteration:  69% 97/141 [00:02<00:00, 46.13it/s]\u001b[A\n",
            "Iteration:  72% 102/141 [00:02<00:00, 45.83it/s]\u001b[A\n",
            "Iteration:  76% 107/141 [00:02<00:00, 45.33it/s]\u001b[A\n",
            "Iteration:  79% 112/141 [00:02<00:00, 44.53it/s]\u001b[A\n",
            "Iteration:  83% 117/141 [00:02<00:00, 45.64it/s]\u001b[A\n",
            "Iteration:  87% 122/141 [00:02<00:00, 46.24it/s]\u001b[A\n",
            "Iteration:  91% 128/141 [00:02<00:00, 47.00it/s]\u001b[A\n",
            "Iteration:  94% 133/141 [00:02<00:00, 46.94it/s]\u001b[A\n",
            "Iteration:  98% 138/141 [00:02<00:00, 46.53it/s]\u001b[A\n",
            "Epoch:  40% 2/5 [00:06<00:09,  3.07s/it]\n",
            "Iteration:   0% 0/141 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   4% 5/141 [00:00<00:03, 45.02it/s]\u001b[A\n",
            "Iteration:   8% 11/141 [00:00<00:02, 46.50it/s]\u001b[A\n",
            "Iteration:  11% 16/141 [00:00<00:02, 46.36it/s]\u001b[A\n",
            "Iteration:  15% 21/141 [00:00<00:02, 47.03it/s]\u001b[A\n",
            "Iteration:  18% 26/141 [00:00<00:02, 46.84it/s]\u001b[A\n",
            "Iteration:  22% 31/141 [00:00<00:02, 46.52it/s]\u001b[A\n",
            "Iteration:  26% 37/141 [00:00<00:02, 47.43it/s]\u001b[A\n",
            "Iteration:  30% 42/141 [00:00<00:02, 46.47it/s]\u001b[A\n",
            "Iteration:  33% 47/141 [00:01<00:02, 46.13it/s]\u001b[A\n",
            "Iteration:  37% 52/141 [00:01<00:01, 44.88it/s]\u001b[A\n",
            "Iteration:  40% 57/141 [00:01<00:01, 42.92it/s]\u001b[A\n",
            "Iteration:  44% 62/141 [00:01<00:01, 42.39it/s]\u001b[A\n",
            "Iteration:  48% 67/141 [00:01<00:01, 43.06it/s]\u001b[A\n",
            "Iteration:  51% 72/141 [00:01<00:01, 43.44it/s]\u001b[A\n",
            "Iteration:  55% 77/141 [00:01<00:01, 42.79it/s]\u001b[A\n",
            "Iteration:  58% 82/141 [00:01<00:01, 43.73it/s]\u001b[A\n",
            "Iteration:  62% 87/141 [00:01<00:01, 42.42it/s]\u001b[A\n",
            "Iteration:  65% 92/141 [00:02<00:01, 43.20it/s]\u001b[A\n",
            "Iteration:  69% 97/141 [00:02<00:01, 42.99it/s]\u001b[A\n",
            "Iteration:  72% 102/141 [00:02<00:00, 42.12it/s]\u001b[A\n",
            "Iteration:  76% 107/141 [00:02<00:00, 41.45it/s]\u001b[A\n",
            "Iteration:  79% 112/141 [00:02<00:00, 43.36it/s]\u001b[A\n",
            "Iteration:  83% 117/141 [00:02<00:00, 43.82it/s]\u001b[A\n",
            "Iteration:  87% 122/141 [00:02<00:00, 44.98it/s]\u001b[A\n",
            "Iteration:  90% 127/141 [00:02<00:00, 46.28it/s]\u001b[A\n",
            "Iteration:  94% 132/141 [00:02<00:00, 45.79it/s]\u001b[A\n",
            "Iteration:  97% 137/141 [00:03<00:00, 44.92it/s]\u001b[A\n",
            "Epoch:  60% 3/5 [00:09<00:06,  3.10s/it]\n",
            "Iteration:   0% 0/141 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   4% 5/141 [00:00<00:03, 43.16it/s]\u001b[A\n",
            "Iteration:   7% 10/141 [00:00<00:03, 43.07it/s]\u001b[A\n",
            "Iteration:  11% 15/141 [00:00<00:02, 42.79it/s]\u001b[A\n",
            "Iteration:  14% 20/141 [00:00<00:02, 44.44it/s]\u001b[A\n",
            "Iteration:  18% 25/141 [00:00<00:02, 45.55it/s]\u001b[A\n",
            "Iteration:  21% 30/141 [00:00<00:02, 45.54it/s]\u001b[A\n",
            "Iteration:  25% 35/141 [00:00<00:02, 44.90it/s]\u001b[A\n",
            "Iteration:  28% 40/141 [00:00<00:02, 43.35it/s]\u001b[A\n",
            "Iteration:  32% 45/141 [00:01<00:02, 45.04it/s]\u001b[A\n",
            "Iteration:  35% 50/141 [00:01<00:02, 45.19it/s]\u001b[A\n",
            "Iteration:  39% 55/141 [00:01<00:01, 46.13it/s]\u001b[A\n",
            "Iteration:  43% 60/141 [00:01<00:01, 46.01it/s]\u001b[A\n",
            "Iteration:  46% 65/141 [00:01<00:01, 45.09it/s]\u001b[A\n",
            "Iteration:  50% 70/141 [00:01<00:01, 45.67it/s]\u001b[A\n",
            "Iteration:  53% 75/141 [00:01<00:01, 46.25it/s]\u001b[A02/27/2020 11:16:14 - INFO - __main__ -   Creating features from dataset file at /tmp/lm_data/eval.txt\n",
            "02/27/2020 11:16:14 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/27/2020 11:16:14 - INFO - __main__ -     Num examples = 500\n",
            "02/27/2020 11:16:14 - INFO - __main__ -     Batch size = 32\n",
            "\n",
            "\n",
            "Evaluating:   0% 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88% 14/16 [00:00<00:00, 134.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100% 16/16 [00:00<00:00, 137.09it/s]\u001b[A\u001b[A02/27/2020 11:16:14 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/27/2020 11:16:14 - INFO - __main__ -     perplexity = tensor(9594.3809)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:  57% 80/141 [00:01<00:02, 29.10it/s]\u001b[A\n",
            "Iteration:  60% 85/141 [00:02<00:01, 32.70it/s]\u001b[A\n",
            "Iteration:  64% 90/141 [00:02<00:01, 35.88it/s]\u001b[A\n",
            "Iteration:  67% 95/141 [00:02<00:01, 38.79it/s]\u001b[A\n",
            "Iteration:  71% 100/141 [00:02<00:00, 41.29it/s]\u001b[A\n",
            "Iteration:  74% 105/141 [00:02<00:00, 43.56it/s]\u001b[A\n",
            "Iteration:  79% 111/141 [00:02<00:00, 45.32it/s]\u001b[A\n",
            "Iteration:  82% 116/141 [00:02<00:00, 46.61it/s]\u001b[A\n",
            "Iteration:  86% 121/141 [00:02<00:00, 46.07it/s]\u001b[A\n",
            "Iteration:  89% 126/141 [00:02<00:00, 44.11it/s]\u001b[A\n",
            "Iteration:  94% 132/141 [00:03<00:00, 45.79it/s]\u001b[A\n",
            "Iteration:  97% 137/141 [00:03<00:00, 46.09it/s]\u001b[A\n",
            "Epoch:  80% 4/5 [00:12<00:03,  3.15s/it]\n",
            "Iteration:   0% 0/141 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   4% 5/141 [00:00<00:03, 42.52it/s]\u001b[A\n",
            "Iteration:   7% 10/141 [00:00<00:02, 44.08it/s]\u001b[A\n",
            "Iteration:  11% 15/141 [00:00<00:02, 44.80it/s]\u001b[A\n",
            "Iteration:  15% 21/141 [00:00<00:02, 46.08it/s]\u001b[A\n",
            "Iteration:  18% 26/141 [00:00<00:02, 47.00it/s]\u001b[A\n",
            "Iteration:  22% 31/141 [00:00<00:02, 47.26it/s]\u001b[A\n",
            "Iteration:  26% 36/141 [00:00<00:02, 46.96it/s]\u001b[A\n",
            "Iteration:  30% 42/141 [00:00<00:02, 48.47it/s]\u001b[A\n",
            "Iteration:  33% 47/141 [00:00<00:01, 47.34it/s]\u001b[A\n",
            "Iteration:  37% 52/141 [00:01<00:01, 45.27it/s]\u001b[A\n",
            "Iteration:  40% 57/141 [00:01<00:01, 46.33it/s]\u001b[A\n",
            "Iteration:  44% 62/141 [00:01<00:01, 46.43it/s]\u001b[A\n",
            "Iteration:  48% 67/141 [00:01<00:01, 46.76it/s]\u001b[A\n",
            "Iteration:  52% 73/141 [00:01<00:01, 47.86it/s]\u001b[A\n",
            "Iteration:  55% 78/141 [00:01<00:01, 48.04it/s]\u001b[A\n",
            "Iteration:  60% 84/141 [00:01<00:01, 48.87it/s]\u001b[A\n",
            "Iteration:  63% 89/141 [00:01<00:01, 47.10it/s]\u001b[A\n",
            "Iteration:  67% 94/141 [00:01<00:01, 46.73it/s]\u001b[A\n",
            "Iteration:  70% 99/141 [00:02<00:00, 46.58it/s]\u001b[A\n",
            "Iteration:  74% 104/141 [00:02<00:00, 46.96it/s]\u001b[A\n",
            "Iteration:  78% 110/141 [00:02<00:00, 48.01it/s]\u001b[A\n",
            "Iteration:  82% 115/141 [00:02<00:00, 48.43it/s]\u001b[A\n",
            "Iteration:  85% 120/141 [00:02<00:00, 47.82it/s]\u001b[A\n",
            "Iteration:  89% 125/141 [00:02<00:00, 47.45it/s]\u001b[A\n",
            "Iteration:  92% 130/141 [00:02<00:00, 47.99it/s]\u001b[A\n",
            "Iteration:  96% 135/141 [00:02<00:00, 47.60it/s]\u001b[A\n",
            "Iteration:  99% 140/141 [00:02<00:00, 48.11it/s]\u001b[A\n",
            "Epoch: 100% 5/5 [00:15<00:00,  3.10s/it]\n",
            "02/27/2020 11:16:19 - INFO - __main__ -    global_step = 705, average loss = 9.207786840073606\n",
            "02/27/2020 11:16:19 - INFO - __main__ -   Saving model checkpoint to /content/models/smallBert/weights\n",
            "02/27/2020 11:16:19 - INFO - transformers.configuration_utils -   Configuration saved in /content/models/smallBert/weights/config.json\n",
            "02/27/2020 11:16:19 - INFO - transformers.modeling_utils -   Model weights saved in /content/models/smallBert/weights/pytorch_model.bin\n",
            "02/27/2020 11:16:19 - INFO - transformers.configuration_utils -   loading configuration file /content/models/smallBert/weights/config.json\n",
            "02/27/2020 11:16:19 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 128,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 256,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 256,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 11061\n",
            "}\n",
            "\n",
            "02/27/2020 11:16:19 - INFO - transformers.modeling_utils -   loading weights file /content/models/smallBert/weights/pytorch_model.bin\n",
            "02/27/2020 11:16:19 - INFO - transformers.tokenization_utils -   Model name '/content/models/smallBert/weights' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/models/smallBert/weights' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/27/2020 11:16:19 - INFO - transformers.tokenization_utils -   Didn't find file /content/models/smallBert/weights/added_tokens.json. We won't load it.\n",
            "02/27/2020 11:16:19 - INFO - transformers.tokenization_utils -   loading file /content/models/smallBert/weights/vocab.txt\n",
            "02/27/2020 11:16:19 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2020 11:16:19 - INFO - transformers.tokenization_utils -   loading file /content/models/smallBert/weights/special_tokens_map.json\n",
            "02/27/2020 11:16:19 - INFO - transformers.tokenization_utils -   loading file /content/models/smallBert/weights/tokenizer_config.json\n",
            "02/27/2020 11:16:19 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/smallBert/weights']\n",
            "02/27/2020 11:16:19 - INFO - transformers.configuration_utils -   loading configuration file /content/models/smallBert/weights/config.json\n",
            "02/27/2020 11:16:19 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 128,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 256,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 256,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 11061\n",
            "}\n",
            "\n",
            "02/27/2020 11:16:19 - INFO - transformers.modeling_utils -   loading weights file /content/models/smallBert/weights/pytorch_model.bin\n",
            "02/27/2020 11:16:19 - INFO - __main__ -   Creating features from dataset file at /tmp/lm_data/eval.txt\n",
            "02/27/2020 11:16:19 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/27/2020 11:16:19 - INFO - __main__ -     Num examples = 500\n",
            "02/27/2020 11:16:19 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 16/16 [00:00<00:00, 157.02it/s]\n",
            "02/27/2020 11:16:19 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/27/2020 11:16:19 - INFO - __main__ -     perplexity = tensor(9918.9736)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIbXQh2aXNo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('/content/models/smallBert/model/')\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(\"/content/models/smallBert/vocab.txt\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TR_YXNfXkrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/models/smallBert/model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xneby5sYXnhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/models/smallBert/weights/config.json /content/models/smallBert/model\n",
        "!cp /content/models/smallBert/weights/pytorch_model.bin /content/models/smallBert/model\n",
        "!cp /content/models/smallBert/weights/vocab.txt /content/models/smallBert/model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGaVn10PXeaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=bert,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k_skJozZIFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "d4334720-476a-4907-c313-6b8c2f07ee43"
      },
      "source": [
        "!pip install --upgrade transformers"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAdt3dkVZt3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "9ae25f73-504c-4e8d-d8fc-5ea3069ef20a"
      },
      "source": [
        "text = \"the whisky a go-go opens its door\"\n",
        "tokenized_text = tokenizer.encode(text).tokens\n",
        "tokenized_text"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'the',\n",
              " 'whisky',\n",
              " 'a',\n",
              " 'go',\n",
              " '-',\n",
              " 'go',\n",
              " 'opens',\n",
              " 'its',\n",
              " 'door',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AY32CiOaQQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "dc71bf86-0545-4716-b91b-b2bafeca5240"
      },
      "source": [
        "masked_index = 9\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "tokenized_text"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'the',\n",
              " 'whisky',\n",
              " 'a',\n",
              " 'go',\n",
              " '-',\n",
              " 'go',\n",
              " 'opens',\n",
              " 'its',\n",
              " '[MASK]',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6gKPBDOaiO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a0bdf80-6dee-4ca2-be32-d9f93eb22e7e"
      },
      "source": [
        "segments_ids = tokenizer.encode(text).type_ids\n",
        "indexed_tokens = tokenizer.encode(text).ids\n",
        "indexed_tokens "
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1183, 10535, 43, 1459, 17, 1459, 6665, 2769, 3914, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu8XphBlaXB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTqP-X4mbVCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained('/content/models/smallBert/model/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRG23p_Rbl9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    predictions = outputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAq_-qRabsPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "058f9bb9-b354-467e-d217-46ec65b5d31f"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1060, -0.1224,  0.3537,  ..., -0.1021,  0.0311, -0.5150],\n",
              "         [-0.0526, -0.0738,  0.3816,  ..., -0.0949, -0.2198, -0.4439],\n",
              "         [ 0.0329,  0.0447,  0.3771,  ...,  0.0593,  0.0518, -0.5494],\n",
              "         ...,\n",
              "         [ 0.0656, -0.1294,  0.0816,  ..., -0.0535, -0.0254, -0.3831],\n",
              "         [-0.0361, -0.2877,  0.4497,  ..., -0.0881,  0.0264, -0.6028],\n",
              "         [-0.0789, -0.0009,  0.1128,  ...,  0.0317, -0.1268, -0.5132]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjrdRHWtbvRq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ad2bba8-6309-42f1-cc21-12ca39cc5623"
      },
      "source": [
        "# confirm we were able to predict 'henson'\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "\n",
        "predicted_index\n",
        "\n",
        "# predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4439"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6RoSSsCcXjK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6759af7-c60f-48b5-9e27-85d7f506552c"
      },
      "source": [
        "tokenizer.id_to_token(4439)"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'common'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pirlhhohke8T",
        "colab_type": "text"
      },
      "source": [
        "## View Results on Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siSNrtODEqUV",
        "colab_type": "code",
        "outputId": "01e91567-abde-47ee-eea2-e674fab6292b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!tensorboard dev upload --logdir /content/runs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "/content/runs\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=kgAdxJj3xxL6gDgTUoUWbPVrkXeIzl&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/wwHWmLi7O1avExJ9mp5Ka_Bbo3lSCOsRUHS1r2a5lqOiyIAllUK6KpY\n",
            "\n",
            "Upload started and will continue reading any new data as it's added\n",
            "to the logdir. To stop uploading, press Ctrl-C.\n",
            "View your TensorBoard live at: https://tensorboard.dev/experiment/wKOIBs5zRgCb0MY8KGi7Sg/\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_main.py\", line 426, in execute\n",
            "    uploader.start_uploading()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader.py\", line 111, in start_uploading\n",
            "    self._upload_once()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader.py\", line 116, in _upload_once\n",
            "    self._rate_limiter.tick()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/util.py\", line 41, in tick\n",
            "    self._time.sleep(wait_secs)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 66, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 268, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_main.py\", line 579, in run\n",
            "    return _run(flags)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_main.py\", line 259, in _run\n",
            "    intent.execute(server_info, channel)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_main.py\", line 431, in execute\n",
            "    print()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}