{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qa-lstm-model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX2UXHqaWsoH",
        "colab_type": "code",
        "outputId": "d6078185-bfa4-438a-cad5-fefbc3949518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh0V2HoCmrer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NOakGS8WnB",
        "colab_type": "code",
        "outputId": "1e4c627b-b55e-4823-ced2-b395878e8ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data \n",
        "import torchtext\n",
        "import csv\n",
        "from itertools import islice\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import regex as re\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import torch.utils.data as data\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwqmca0M1PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "    \n",
        "def pre_process(doc):\n",
        "    doc = str(doc)\n",
        "    x = re.sub('[…“”%!&\"@#()\\-\\*\\+,/:;<=>?@[\\]\\^_`{\\}~]', ' ', doc)\n",
        "    y = re.sub('[\\.\\']', \"\", x)\n",
        "    z = y.lower()\n",
        "    return z\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_pickle(path, data):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def timer(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Returns the minutes and seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    time = end_time - start_time\n",
        "    mins = int(time / 60)\n",
        "    secs = int(time - (mins * 60))\n",
        "\n",
        "    return mins, secs\n",
        "\n",
        "def pad_seq(seq, max_seq_len):\n",
        "    # Pad each seq to be the same length to process in batch.\n",
        "    # pad_token = 0\n",
        "    if len(seq) >= max_seq_len:\n",
        "        seq = seq[:max_seq_len]\n",
        "    else:\n",
        "        seq += [0]*(max_seq_len - len(seq))\n",
        "    return seq\n",
        "\n",
        "def vectorize(seq, vocab, max_seq_len):\n",
        "    # Map tokens in seq to idx\n",
        "    seq_idx = [vocab[token] for token in seq]\n",
        "    # Pad seq idx\n",
        "    padded_seq_idx = [pad_seq(seq_idx, max_seq_len)]\n",
        "    # padded_seq_idx = pad_seq(seq_idx, max_seq_len)\n",
        "\n",
        "    # return torch.tensor(padded_seq_idx)\n",
        "    return padded_seq_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoDZxXUnEES2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qid_docid = pd.read_csv(path + \"FiQA_train_question_doc_final.tsv\", sep=\"\\t\")\n",
        "qid_docid = qid_docid [['qid', 'docid']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNmKP8_M0x0q",
        "colab_type": "text"
      },
      "source": [
        "**Load pickle files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAYWDxGXyda4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict mapping of token to idx\n",
        "vocab = load_pickle(path + 'vocab_full.pickle')\n",
        "# dict mapping of docid to doc text\n",
        "docid_to_text = load_pickle(path + 'label_ans.pickle')\n",
        "# dict mapping of qid to question text\n",
        "qid_to_text = load_pickle(path + 'qid_text.pickle')\n",
        "# dict mapping of qid to relevant docs\n",
        "qid_rel = load_pickle(path + 'qid_rel.pickle')\n",
        "# dict mapping of qid to ranked candidates\n",
        "qid_ranked_docs = load_pickle(path+'qid_ranked_docs_100.pickle')\n",
        "\n",
        "empty_docs = load_pickle(path+'empty_docs.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS9TviMG1BTm",
        "colab_type": "text"
      },
      "source": [
        "**Example data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Xm-zCjj83u",
        "colab_type": "code",
        "outputId": "ae2a2483-904c-4b1b-dc9d-70fbd5f16116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "toy_label = dict(itertools.islice(qid_rel.items(), 100))\n",
        "toy_cand = dict(itertools.islice(qid_ranked_docs.items(), 100))\n",
        "\n",
        "print(toy_label)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: [18850], 1: [14255], 2: [308938], 3: [296717, 100764, 314352, 146317], 4: [196463], 5: [69306], 6: [560251, 188530, 564488], 7: [411063], 8: [566392, 65404], 9: [509122, 184698], 11: [596427], 12: [192516, 338700, 158738], 13: [503678], 14: [398960], 15: [325273], 16: [60590], 17: [146657], 18: [88124], 19: [315086, 142623], 20: [447231], 21: [497642], 23: [550624, 32102], 25: [107584, 562777], 26: [285255, 350819], 27: [537326], 28: [250640], 29: [274832, 114494, 189642, 103662], 30: [551175, 434082, 336922, 19233], 31: [156554], 32: [279480, 69623, 84645], 33: [519798, 425387], 34: [599545], 35: [498681, 80913], 36: [275249, 368649], 37: [523564], 38: [85517, 195207, 357037, 233751], 41: [176229], 42: [272709, 327263, 331981], 43: [76662], 44: [385881], 45: [284610, 261220], 46: [91325], 47: [133299], 48: [108062, 401260, 329810, 512151], 49: [352927], 51: [107817, 257168, 75195], 52: [566417, 125111], 53: [84077, 562798, 102362, 323269, 119308, 184852, 119210, 176196], 54: [590775, 109546, 511651], 55: [324854, 579628, 237207], 56: [572690], 57: [203633, 391403, 77818, 226530], 59: [71601], 60: [381151], 61: [524134], 62: [34810], 63: [509617], 64: [391619], 65: [580624, 109203], 66: [397608, 540395, 405412], 67: [370815, 294864, 18792, 511571], 68: [19183], 69: [378484], 70: [327002], 71: [372052], 72: [549870, 302049], 74: [46680], 75: [297965], 76: [375357], 77: [551315], 78: [152407], 80: [252473], 81: [451207], 82: [500708], 83: [534277], 85: [431230], 86: [71569], 87: [357938, 537593], 88: [415946], 89: [413229, 590102, 268026, 248624, 508754, 64556], 90: [31793], 91: [523431], 92: [465787], 93: [292748], 94: [245447], 95: [586355], 96: [328863], 97: [578529, 77352], 98: [575929, 527522], 99: [474248, 193081, 341413, 136662, 153679], 102: [494264, 187073], 104: [575869, 523158], 105: [41356, 107564], 106: [76695], 107: [276408, 269646], 108: [396982], 109: [73427], 110: [520386, 220063, 410128, 459119, 482165, 397152], 111: [46092], 112: [153541]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7vU-XnlgjTB",
        "colab_type": "code",
        "outputId": "ae6d8e8e-531c-4bd0-be42-04530b8720b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Get train set\n",
        "\n",
        "neg_ans = {}\n",
        "\n",
        "train_set = []\n",
        "\n",
        "cand_size = 50\n",
        "\n",
        "for qid, pos_ans_lst in tqdm(toy_label.items()):\n",
        "    num_sample = math.floor(cand_size/len(pos_ans_lst))\n",
        "    for i, cand_lst in toy_cand.items():\n",
        "        trimed_cand = [x for x in cand_lst if x not in pos_ans_lst]\n",
        "\n",
        "    # If there is only 1 rel doc\n",
        "    if num_sample == cand_size:\n",
        "        for _ in range(cand_size):\n",
        "            tmp = []\n",
        "            tmp.append(qid)\n",
        "            tmp.append(pos_ans_lst[0])\n",
        "            neg_doc = random.choice(trimed_cand)\n",
        "            tmp.append(neg_doc)\n",
        "            train_set.append(tmp)\n",
        "    else:\n",
        "        for _ in range(num_sample):\n",
        "            for j in range(len(pos_ans_lst)):\n",
        "                tmp = []\n",
        "                tmp.append(qid)\n",
        "                tmp.append(pos_ans_lst[j])\n",
        "                neg_doc = random.choice(trimed_cand)\n",
        "                tmp.append(neg_doc)\n",
        "                train_set.append(tmp)\n",
        "        for k in range(cand_size % len(pos_ans_lst)):\n",
        "            tmp = []\n",
        "            tmp.append(qid)\n",
        "            tmp.append(pos_ans_lst[k])\n",
        "            neg_doc = random.choice(trimed_cand)\n",
        "            tmp.append(neg_doc)\n",
        "            train_set.append(tmp)\n",
        "\n",
        "for row in train_set:\n",
        "    assert len(row) == 3, \"Train set len is incorrect!\""
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            " 28%|██▊       | 28/100 [00:00<00:00, 272.60it/s]\u001b[A\n",
            " 55%|█████▌    | 55/100 [00:00<00:00, 270.95it/s]\u001b[A\n",
            " 91%|█████████ | 91/100 [00:00<00:00, 290.51it/s]\u001b[A\n",
            "100%|██████████| 100/100 [00:00<00:00, 291.79it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dty6SSFH4Nm4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0bf57e42-295b-4706-c81e-9206e877177d"
      },
      "source": [
        "print(len(train_set))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmh9VsYQ4eon",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "937a0561-a9ae-4d35-d161-3a9f51b39590"
      },
      "source": [
        "c = 0\n",
        "for row in train_set:\n",
        "    if row[0] == 9:\n",
        "        # print(row)\n",
        "        c+=1\n",
        "print(c)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcSwoycjoL65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Question ID and Answer ID pair\n",
        "qid_docid = pd.read_csv(path + \"FiQA_train_question_doc_final.tsv\", sep=\"\\t\")\n",
        "\n",
        "qid_docid = qid_docid [['qid', 'docid']]\n",
        "\n",
        "# test = qid_docid[:117]\n",
        "test = qid_docid[:16]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b0m3UZL9d3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_df = qid_docid\n",
        "# test_df[\"neg_docid\"] = test_df['qid'].map(neg_ans)\n",
        "# test_df = test_df.explode('neg_docid')\n",
        "\n",
        "# test_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPC2IEIzz2e9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "3f842fc1-4438-4eab-f4bd-0f09c83db99d"
      },
      "source": [
        "test.head(5)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>docid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>18850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>14255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>308938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>296717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>100764</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   qid   docid\n",
              "0    0   18850\n",
              "1    1   14255\n",
              "2    2  308938\n",
              "3    3  296717\n",
              "4    3  100764"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sibXqTHtL_PE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = []\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    for qid, neg_doc in neg_ans.items():\n",
        "        if len(neg_doc) == cand_size:\n",
        "            if row['qid'] == qid:\n",
        "                tmp = []\n",
        "                tmp.append(row['qid'])\n",
        "                tmp.append(row['docid'])\n",
        "                tmp.append(neg_doc)\n",
        "                train_set.append(tmp)\n",
        "            else:\n",
        "                tmp = []\n",
        "                tmp.append(row['qid'])\n",
        "                tmp.append(row['docid'])\n",
        "                tmp.append(neg_doc)\n",
        "\n",
        "\n",
        "# # train_set = load_pickle(path+'train_set_1000.pickle')\n",
        "\n",
        "# # for idx, sample in enumerate(train_set):\n",
        "# #     if len(sample[2]) > 1:\n",
        "# #         sample[2] = random.choice(sample[2])\n",
        "# #     else:\n",
        "# #         sample[2] = sample[2][0]\n",
        "\n",
        "# # count = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgiHx3AX8wvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "040d1512-9b42-4daa-cf68-4141eec014d0"
      },
      "source": [
        "for row in train_set:\n",
        "    if row[0] == 3:\n",
        "        count += 1\n",
        "\n",
        "count "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f583fc51b192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htDYqahQL8wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = [x for x in train_set if x[1] not in empty_docs]\n",
        "\n",
        "print(len(train_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwwJgdVHGcIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set, valid_set = train_test_split(train_set, test_size=0.1)\n",
        "\n",
        "print(\"Number of train data: {}\".format(len(train_set)))\n",
        "print(\"Number of validation data: {}\".format(len(valid_set)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCUqSDFR8J1t",
        "colab_type": "text"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl3hClS13Gz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "n_epochs = 2\n",
        "batch_size = 8\n",
        "hidden_size = 141\n",
        "max_seq_len = 200\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-VHSXrLSfF",
        "colab_type": "code",
        "outputId": "4938353d-6791-46c6-96f2-5f1d4f8d67e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "emb = torchtext.vocab.GloVe(\"6B\", dim=emb_dim)\n",
        "# dictionary mapping of word idx to glove vectors\n",
        "emb_weights = np.zeros((vocab_size, emb_dim))\n",
        "words_found = 0\n",
        "print(\"Embedding dim: {}\".format(emb_weights.shape))\n",
        "\n",
        "for token, idx in vocab.items():\n",
        "    # emb.stoi is a dict of token to idx mapping\n",
        "    if token in emb.stoi:\n",
        "        emb_weights[idx] = emb[token]\n",
        "        words_found += 1\n",
        "\n",
        "print(\"vocab size: \", vocab_size)\n",
        "print(words_found, \" words are found in GloVe\")\n",
        "\n",
        "# Convert numpy matrix to tensor\n",
        "emb_weights = torch.from_numpy(emb_weights).float()\n",
        "\n",
        "emb_weights.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n",
            "100%|█████████▉| 398600/400000 [00:14<00:00, 28345.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Embedding dim: (85034, 100)\n",
            "vocab size:  85034\n",
            "50456  words are found in GloVe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([85034, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8C6zB6MP4bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emb_layer(emb_weights):\n",
        "    vocab_size, emb_dim = emb_weights.shape\n",
        "    emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "    emb_layer.load_state_dict({'weight': emb_weights})\n",
        "\n",
        "    return emb_layer\n",
        "\n",
        "def loss_fn(pos_sim, neg_sim):\n",
        "    margin = 0.05\n",
        "\n",
        "    loss = margin - pos_sim + neg_sim\n",
        "    if loss.data[0] < 0:\n",
        "        loss.data[0] = 0\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxdQU4qJTV73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QA_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "\n",
        "        super(QA_LSTM, self).__init__()\n",
        "\n",
        "        # Shape - (max_seq_len, emb_dim)\n",
        "        self.embedding = create_emb_layer(emb_weights)\n",
        "\n",
        "        self.shared_lstm = nn.LSTM(emb_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, q, a):\n",
        "        # embedding\n",
        "        q = self.embedding(q) # (bs, L, E)\n",
        "        a = self.embedding(a) # (bs, L, E)\n",
        "\n",
        "        # LSTM\n",
        "        q, (hidden, cell) = self.shared_lstm(q) # (bs, L, 2H)\n",
        "        a, (hidden, cell) = self.shared_lstm(a) # (bs, L, 2H)\n",
        "\n",
        "        # Output shape (batch size, seq_len, num_direction * hidden_size)\n",
        "        # There are n of word level biLSTM representations for the seq where n is the number of seq len\n",
        "        # Use max pooling to generate the best representation\n",
        "        q = torch.max(q, 1)[0] \n",
        "        a = torch.max(a, 1)[0] # (bs, 2H)\n",
        "\n",
        "        return self.cos(q, a) # (bs,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HZNF3LQC8Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, q_lst, pos_ans, neg_ans):\n",
        "        'Initialization'\n",
        "        self.q_lst = q_lst\n",
        "        self.pos_ans_lst = pos_ans\n",
        "        self.neg_ans_lst = neg_ans\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.q_lst)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.q_lst[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        q = self.q_lst[index]\n",
        "        pos_ans = self.pos_ans_lst[index]\n",
        "        neg_ans = self.neg_ans_lst[index]\n",
        "\n",
        "        return q, pos_ans, neg_ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OP2Qcdd8xmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_set, optimizer, batch_size):\n",
        "\n",
        "    # Cumulated Training loss\n",
        "    training_loss = 0.0\n",
        "\n",
        "    q_lst = []\n",
        "    pos_lst = []\n",
        "    neg_lst = []\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        " \n",
        "    for i, seq in enumerate(train_set):\n",
        "\n",
        "        ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = vectorize(q_text, vocab, max_seq_len)\n",
        "\n",
        "        q_lst.append(q_vec)\n",
        "\n",
        "        pos_ans_text = docid_to_text[pos_ans]\n",
        "        pos_ans_vec = vectorize(pos_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        pos_lst.append(pos_ans_vec)\n",
        "\n",
        "        for docid in neg_ans:\n",
        "            neg_ans_text = docid_to_text[neg_ans]\n",
        "            neg_ans_vec = vectorize(neg_ans_text, vocab, max_seq_len)\n",
        "            neg_lst.append(neg_ans_vec)\n",
        "\n",
        "    q_lst = torch.tensor(q_lst)\n",
        "    pos_lst = torch.tensor(pos_lst)\n",
        "    neg_lst = torch.tensor(neg_lst)\n",
        "\n",
        "    train_data = Dataset(q_lst, pos_lst, neg_lst)\n",
        "\n",
        "    train_loader = data.DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "    for ques, pos_ans, neg_ans in tqdm(train_loader):\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for q in ques:\n",
        "            batch_q = q.to(device)\n",
        "\n",
        "        for p in pos_ans:\n",
        "            batch_pos = p.to(device)\n",
        "\n",
        "        for n in neg_ans:\n",
        "            batch_neg = n.to(device)\n",
        "            \n",
        "        # 2. Compute predictions\n",
        "        pos_sim = model(batch_q, batch_pos)    \n",
        "        neg_sim = model(batch_q, batch_neg)\n",
        "\n",
        "        # 3. Compute loss\n",
        "        loss = loss_fn(pos_sim, neg_sim)\n",
        "\n",
        "        # 4. Use loss to compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "            \n",
        "        training_loss += loss.item()\n",
        "            \n",
        "    return training_loss / len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwOnHJH28nUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, valid_set, batch_size):\n",
        "\n",
        "    # Cumulated Training loss\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    q_lst = []\n",
        "    pos_lst = []\n",
        "    neg_lst = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        " \n",
        "    for i, seq in enumerate(train_set):\n",
        "\n",
        "        ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = vectorize(q_text, vocab, max_seq_len)\n",
        "\n",
        "        q_lst.append(q_vec)\n",
        "\n",
        "        pos_ans_text = docid_to_text[pos_ans]\n",
        "        pos_ans_vec = vectorize(pos_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        pos_lst.append(pos_ans_vec)\n",
        "\n",
        "        neg_ans_text = docid_to_text[neg_ans]\n",
        "        neg_ans_vec = vectorize(neg_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        neg_lst.append(neg_ans_vec)\n",
        "\n",
        "    q_lst = torch.tensor(q_lst)\n",
        "    pos_lst = torch.tensor(pos_lst)\n",
        "    neg_lst = torch.tensor(neg_lst)\n",
        "\n",
        "    train_data = Dataset(q_lst, pos_lst, neg_lst)\n",
        "\n",
        "    train_loader = data.DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "        \n",
        "    # Don't calculate the gradients\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for ques, pos_ans, neg_ans in tqdm(train_loader):\n",
        "\n",
        "            for q in ques:\n",
        "                batch_q = q.to(device)\n",
        "\n",
        "            for p in pos_ans:\n",
        "                batch_pos = p.to(device)\n",
        "\n",
        "            for n in neg_ans:\n",
        "                batch_neg = n.to(device)\n",
        "                \n",
        "            pos_sim = model(batch_q, batch_pos)    \n",
        "            neg_sim = model(batch_q, batch_neg)\n",
        "\n",
        "            loss = loss_fn(pos_sim, neg_sim)\n",
        "                \n",
        "            valid_loss += loss.item()\n",
        "                \n",
        "        return valid_loss / len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf1BztmOuZaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = QA_LSTM(vocab_size, emb_dim, hidden_size)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
        "\n",
        "seq = train_set[0]\n",
        "ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "q_text = qid_to_text[ques]\n",
        "q_vec = torch.tensor(vectorize(q_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "pos_ans_text = docid_to_text[pos_ans]\n",
        "pos_ans_vec = torch.tensor(vectorize(pos_ans_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "for docid in neg_ans:\n",
        "    neg_ans_text = docid_to_text[docid]\n",
        "    neg_ans_vec = torch.tensor(vectorize(neg_ans_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "    pos_sim = model(q_vec, pos_ans_vec)\n",
        "    neg_sim = model(q_vec, neg_ans_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xylTrRNW-Cyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = QA_LSTM(vocab_size, emb_dim, hidden_size)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
        "\n",
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss = train(model, train_set, optimizer, batch_size)\n",
        "    # Evaluate validation loss\n",
        "    valid_loss = validate(model, valid_set, batch_size)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # Save the parameters of the model\n",
        "        torch.save(model.state_dict(), 'model-train100.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {}\".format(round(train_loss, 3)))\n",
        "    print(\"\\t Validation Loss: {}\\n\".format(round(valid_loss, 3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuM1bIzzD-G_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(take(5, toy_label.items()))\n",
        "\n",
        "# TODO:\n",
        "# 1. Get cand for test data\n",
        "# 2. Process test data\n",
        "# 3. Get test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-vFCHyGF92F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = []\n",
        "\n",
        "for qid, docid in toy_label.items():\n",
        "    for k, v in toy_cand.items():\n",
        "        if k == qid:\n",
        "            tmp = []\n",
        "            tmp.append(qid)\n",
        "            tmp.append(docid)\n",
        "            tmp.append(v)\n",
        "            test_set.append(tmp)\n",
        "\n",
        "test_set = test_set[:10]\n",
        "\n",
        "# print(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9YIIOZt3waV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model, test_set, qid_rel, max_seq_len, k):\n",
        "    \n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "\n",
        "        ques, pos_ans, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = torch.tensor(vectorize(q_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "        cands_text = [docid_to_text[c] if c is not 0 else \"\" for c in cands]\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        for cand in cands_text:\n",
        "            a_vec = torch.tensor(vectorize(cand, vocab, max_seq_len)).to(device)\n",
        "            scores.append(model(q_vec, a_vec).item())\n",
        "\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        qid_pred_rank[ques] = ranked_ans\n",
        "\n",
        "    # return qid_pred_rank\n",
        "    MRR, average_ndcg, precision = evaluate(qid_pred_rank, qid_rel, k)\n",
        "\n",
        "    return MRR, average_ndcg, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Kd6zOOBkbc",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OnYU9rDU4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(qid_rel.items(), 10))\n",
        "\n",
        "print(toy_test_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APyw7XzpV4qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model with the best validation loss\n",
        "model.load_state_dict(torch.load('model-train100.pt'))\n",
        "\n",
        "rank = eval(model, test_set, toy_test_label, max_seq_len, k=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrdWP3GoWAGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(toy_test_label[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99aL2wiN4pFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model with the best validation loss\n",
        "model.load_state_dict(torch.load('model-train100.pt'))\n",
        "\n",
        "k = 10\n",
        "\n",
        "MRR, average_ndcg, precision = eval(model, test_set, toy_test_label, max_seq_len, k=10)\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}