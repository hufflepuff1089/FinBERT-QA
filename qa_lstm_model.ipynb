{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qa-lstm-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX2UXHqaWsoH",
        "colab_type": "code",
        "outputId": "6f114826-21a5-439d-f34d-2fd4e6654df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh0V2HoCmrer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NOakGS8WnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "575ed5df-af2f-423a-cde2-630fdd267cf3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data \n",
        "import torchtext\n",
        "import csv\n",
        "from itertools import islice\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import regex as re\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import torch.utils.data as data\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwqmca0M1PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "    \n",
        "def pre_process(doc):\n",
        "    doc = str(doc)\n",
        "    x = re.sub('[…“”%!&\"@#()\\-\\*\\+,/:;<=>?@[\\]\\^_`{\\}~]', ' ', doc)\n",
        "    y = re.sub('[\\.\\']', \"\", x)\n",
        "    z = y.lower()\n",
        "    return z\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_pickle(path, data):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def pad_seq(seq, max_seq_len):\n",
        "    # Pad each seq to be the same length to process in batch.\n",
        "    # pad_token = 0\n",
        "    if len(seq) >= max_seq_len:\n",
        "        seq = seq[:max_seq_len]\n",
        "    else:\n",
        "        seq += [0]*(max_seq_len - len(seq))\n",
        "    return seq\n",
        "\n",
        "def vectorize(seq, vocab, max_seq_len):\n",
        "    # Map tokens in seq to idx\n",
        "    seq_idx = [vocab[token] for token in seq]\n",
        "    # Pad seq idx\n",
        "    padded_seq_idx = [pad_seq(seq_idx, max_seq_len)]\n",
        "    # padded_seq_idx = pad_seq(seq_idx, max_seq_len)\n",
        "\n",
        "    # return torch.tensor(padded_seq_idx)\n",
        "    return padded_seq_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoDZxXUnEES2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# qid_docid = pd.read_csv(path + \"FiQA_train_question_doc_final.tsv\", sep=\"\\t\")\n",
        "# qid_docid = qid_docid [['qid', 'docid']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNmKP8_M0x0q",
        "colab_type": "text"
      },
      "source": [
        "**Load pickle files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAYWDxGXyda4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict mapping of token to idx\n",
        "vocab = load_pickle(path + 'vocab_full.pickle')\n",
        "# dict mapping of docid to doc text\n",
        "docid_to_text = load_pickle(path + 'label_ans.pickle')\n",
        "\n",
        "# dict mapping of qid to question text\n",
        "qid_to_text = load_pickle(path + 'qid_text.pickle')\n",
        "\n",
        "train_qid_rel = load_pickle(path + \"qid_rel_train.pickle\")\n",
        "test_qid_rel = load_pickle(path + \"qid_rel_test.pickle\")\n",
        "valid_qid_rel = load_pickle(path + \"qid_rel_valid.pickle\")\n",
        "\n",
        "train_set = load_pickle(path + 'data/data_train_50.pickle')\n",
        "valid_set = load_pickle(path + 'data/data_valid_50.pickle')\n",
        "\n",
        "# train_set = load_pickle(path + 'data/data_train_100.pickle')\n",
        "# valid_set = load_pickle(path + 'data/data_valid_100.pickle')\n",
        "\n",
        "# train_set = load_pickle(path + 'data/data_train_200.pickle')\n",
        "# valid_set = load_pickle(path + 'data/data_valid_200.pickle')\n",
        "\n",
        "test_set = load_pickle(path + 'data/data_test_500_rel.pickle')\n",
        "test_set_full = load_pickle(path + 'data/data_test_500.pickle')\n",
        "\n",
        "empty_docs = load_pickle(path+'empty_docs.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rueQyOtIAzn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "68817541-57ac-4c8b-a2ea-c1d4d7201d94"
      },
      "source": [
        "train_set = [x for x in train_set if x[1] not in empty_docs]\n",
        "valid_set = [x for x in valid_set if x[1] not in empty_docs]\n",
        "\n",
        "def remove_empty(test_set):\n",
        "    for index, row in enumerate(test_set):\n",
        "        for doc in row[1]:\n",
        "            if doc in empty_docs:\n",
        "                del test_set[index]\n",
        "    return test_set\n",
        "\n",
        "test_set = remove_empty(test_set)\n",
        "test_set_full = remove_empty(test_set_full)\n",
        "\n",
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 283707\n",
            "Number of validation samples: 31582\n",
            "Number of test samples: 330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCUqSDFR8J1t",
        "colab_type": "text"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl3hClS13Gz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "n_epochs = 20\n",
        "batch_size = 64\n",
        "hidden_size = 256\n",
        "max_seq_len = 200\n",
        "dropout = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-VHSXrLSfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "449a5cee-7573-46c8-8d75-8a536add5d49"
      },
      "source": [
        "emb = torchtext.vocab.GloVe(\"6B\", dim=emb_dim)\n",
        "# dictionary mapping of word idx to glove vectors\n",
        "emb_weights = np.zeros((vocab_size, emb_dim))\n",
        "words_found = 0\n",
        "print(\"Embedding dim: {}\".format(emb_weights.shape))\n",
        "\n",
        "for token, idx in vocab.items():\n",
        "    # emb.stoi is a dict of token to idx mapping\n",
        "    if token in emb.stoi:\n",
        "        emb_weights[idx] = emb[token]\n",
        "        words_found += 1\n",
        "\n",
        "print(\"vocab size: \", vocab_size)\n",
        "print(words_found, \" words are found in GloVe\")\n",
        "\n",
        "# Convert numpy matrix to tensor\n",
        "emb_weights = torch.from_numpy(emb_weights).float()\n",
        "\n",
        "emb_weights.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding dim: (85034, 100)\n",
            "vocab size:  85034\n",
            "50456  words are found in GloVe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([85034, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8C6zB6MP4bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emb_layer(emb_weights):\n",
        "    vocab_size, emb_dim = emb_weights.shape\n",
        "    emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "    emb_layer.load_state_dict({'weight': emb_weights})\n",
        "\n",
        "    return emb_layer\n",
        "\n",
        "def loss_fn(pos_sim, neg_sim):\n",
        "    margin = 0.2\n",
        "\n",
        "    loss = margin - pos_sim + neg_sim\n",
        "    if loss.data[0] < 0:\n",
        "        loss.data[0] = 0\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxdQU4qJTV73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QA_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, dropout):\n",
        "\n",
        "        super(QA_LSTM, self).__init__()\n",
        "\n",
        "        # Shape - (max_seq_len, emb_dim)\n",
        "        self.embedding = create_emb_layer(emb_weights)\n",
        "\n",
        "        self.shared_lstm = nn.LSTM(emb_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.cos = nn.CosineSimilarity(dim=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, q, a):\n",
        "        # embedding\n",
        "        q = self.embedding(q) # (bs, L, E)\n",
        "        a = self.embedding(a) # (bs, L, E)\n",
        "\n",
        "        # LSTM\n",
        "        q, (hidden, cell) = self.shared_lstm(q) # (bs, L, 2H)\n",
        "        a, (hidden, cell) = self.shared_lstm(a) # (bs, L, 2H)\n",
        "\n",
        "        # Output shape (batch size, seq_len, num_direction * hidden_size)\n",
        "        # There are n of word level biLSTM representations for the seq where n is the number of seq len\n",
        "        # Use max pooling to generate the best representation\n",
        "        q = torch.max(q, 1)[0] \n",
        "        a = torch.max(a, 1)[0] # (bs, 2H)\n",
        "\n",
        "        q = self.dropout(q)\n",
        "        a = self.dropout(a)\n",
        "\n",
        "        return self.cos(q, a) # (bs,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HZNF3LQC8Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, q_lst, pos_ans, neg_ans):\n",
        "        'Initialization'\n",
        "        self.q_lst = q_lst\n",
        "        self.pos_ans_lst = pos_ans\n",
        "        self.neg_ans_lst = neg_ans\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.q_lst)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.q_lst[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        q = self.q_lst[index]\n",
        "        pos_ans = self.pos_ans_lst[index]\n",
        "        neg_ans = self.neg_ans_lst[index]\n",
        "\n",
        "        return q, pos_ans, neg_ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OP2Qcdd8xmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_set, optimizer, batch_size):\n",
        "\n",
        "    # Cumulated Training loss\n",
        "    training_loss = 0.0\n",
        "\n",
        "    q_lst = []\n",
        "    pos_lst = []\n",
        "    neg_lst = []\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        " \n",
        "    for i, seq in enumerate(train_set):\n",
        "\n",
        "        ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = vectorize(q_text, vocab, max_seq_len)\n",
        "\n",
        "        q_lst.append(q_vec)\n",
        "\n",
        "        pos_ans_text = docid_to_text[pos_ans]\n",
        "        pos_ans_vec = vectorize(pos_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        pos_lst.append(pos_ans_vec)\n",
        "\n",
        "        neg_ans_text = docid_to_text[neg_ans]\n",
        "        neg_ans_vec = vectorize(neg_ans_text, vocab, max_seq_len)\n",
        "        neg_lst.append(neg_ans_vec)\n",
        "\n",
        "    q_lst = torch.tensor(q_lst)\n",
        "    pos_lst = torch.tensor(pos_lst)\n",
        "    neg_lst = torch.tensor(neg_lst)\n",
        "\n",
        "    train_data = Dataset(q_lst, pos_lst, neg_lst)\n",
        "\n",
        "    train_loader = data.DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "    for ques, pos_ans, neg_ans in tqdm(train_loader):\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for q in ques:\n",
        "            batch_q = q.to(device)\n",
        "\n",
        "        for p in pos_ans:\n",
        "            batch_pos = p.to(device)\n",
        "\n",
        "        for n in neg_ans:\n",
        "            batch_neg = n.to(device)\n",
        "            \n",
        "        # 2. Compute predictions\n",
        "        pos_sim = model(batch_q, batch_pos)    \n",
        "        neg_sim = model(batch_q, batch_neg)\n",
        "\n",
        "        # 3. Compute loss\n",
        "        loss = loss_fn(pos_sim, neg_sim)\n",
        "\n",
        "        # 4. Use loss to compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "            \n",
        "        training_loss += loss.item()\n",
        "            \n",
        "    return training_loss / len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwOnHJH28nUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, valid_set, batch_size):\n",
        "\n",
        "    # Cumulated Training loss\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    q_lst = []\n",
        "    pos_lst = []\n",
        "    neg_lst = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        " \n",
        "    for i, seq in enumerate(valid_set):\n",
        "\n",
        "        ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = vectorize(q_text, vocab, max_seq_len)\n",
        "\n",
        "        q_lst.append(q_vec)\n",
        "\n",
        "        pos_ans_text = docid_to_text[pos_ans]\n",
        "        pos_ans_vec = vectorize(pos_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        pos_lst.append(pos_ans_vec)\n",
        "\n",
        "        neg_ans_text = docid_to_text[neg_ans]\n",
        "        neg_ans_vec = vectorize(neg_ans_text, vocab, max_seq_len)\n",
        "\n",
        "        neg_lst.append(neg_ans_vec)\n",
        "\n",
        "    q_lst = torch.tensor(q_lst)\n",
        "    pos_lst = torch.tensor(pos_lst)\n",
        "    neg_lst = torch.tensor(neg_lst)\n",
        "\n",
        "    valid_data = Dataset(q_lst, pos_lst, neg_lst)\n",
        "\n",
        "    valid_loader = data.DataLoader(valid_data, batch_size=batch_size)\n",
        "        \n",
        "    # Don't calculate the gradients\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for ques, pos_ans, neg_ans in tqdm(valid_loader):\n",
        "\n",
        "            for q in ques:\n",
        "                batch_q = q.to(device)\n",
        "\n",
        "            for p in pos_ans:\n",
        "                batch_pos = p.to(device)\n",
        "\n",
        "            for n in neg_ans:\n",
        "                batch_neg = n.to(device)\n",
        "                \n",
        "            pos_sim = model(batch_q, batch_pos)    \n",
        "            neg_sim = model(batch_q, batch_neg)\n",
        "\n",
        "            loss = loss_fn(pos_sim, neg_sim)\n",
        "                \n",
        "            valid_loss += loss.item()\n",
        "                \n",
        "        return valid_loss / len(valid_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjVcmJMS1yML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = QA_LSTM(vocab_size, emb_dim, hidden_size, dropout)\n",
        "# model = model.to(device)\n",
        "\n",
        "# seq = train_set[0]\n",
        "# ques, pos_ans, neg_ans = seq[0], seq[1], seq[2]\n",
        "\n",
        "# q_text = qid_to_text[ques]\n",
        "# q_vec = torch.tensor(vectorize(q_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "# pos_ans_text = docid_to_text[pos_ans]\n",
        "# pos_ans_vec = torch.tensor(vectorize(pos_ans_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "# neg_ans_text = docid_to_text[neg_ans]\n",
        "# neg_ans_vec = torch.tensor(vectorize(neg_ans_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "# pos_sim = model(q_vec, pos_ans_vec)\n",
        "# neg_sim = model(q_vec, neg_ans_vec)\n",
        "\n",
        "# loss_fn(pos_sim, neg_sim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xylTrRNW-Cyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = QA_LSTM(vocab_size, emb_dim, hidden_size, dropout)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Lowest validation lost\n",
        "# best_valid_loss = float('inf')\n",
        "\n",
        "# for epoch in range(n_epochs):\n",
        "\n",
        "#     # Evaluate training loss\n",
        "#     train_loss = train(model, train_set, optimizer, batch_size)\n",
        "#     # Evaluate validation loss\n",
        "#     valid_loss = validate(model, valid_set, batch_size)\n",
        "    \n",
        "#     # At each epoch, if the validation loss is the best\n",
        "#     # if valid_loss < best_valid_loss:\n",
        "#     #     best_valid_loss = valid_loss\n",
        "#     #     now = datetime.now()\n",
        "#     #     current_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "#         # Save the parameters of the model\n",
        "#     torch.save(model.state_dict(), str(epoch)+'_model-eu.pt')\n",
        "\n",
        "#     # torch.save(model.state_dict(),'model-0.pt')\n",
        "\n",
        "#     print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "#     print(\"\\t Train Loss: {}\".format(round(train_loss, 3)))\n",
        "#     print(\"\\t Validation Loss: {}\\n\".format(round(valid_loss, 3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9YIIOZt3waV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model, test_set, qid_rel, max_seq_len, k):\n",
        "    \n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "\n",
        "        ques, pos_ans, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = qid_to_text[ques]\n",
        "        q_vec = torch.tensor(vectorize(q_text, vocab, max_seq_len)).to(device)\n",
        "\n",
        "        cands_text = [docid_to_text[c] if c is not 0 else \"\" for c in cands]\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        for cand in cands_text:\n",
        "            a_vec = torch.tensor(vectorize(cand, vocab, max_seq_len)).to(device)\n",
        "            scores.append(model(q_vec, a_vec).item())\n",
        "\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        qid_pred_rank[ques] = ranked_ans\n",
        "\n",
        "    # return qid_pred_rank\n",
        "    MRR, average_ndcg, precision = evaluate(qid_pred_rank, qid_rel, k)\n",
        "\n",
        "    return qid_pred_rank, MRR, average_ndcg, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Kd6zOOBkbc",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OnYU9rDU4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy_test_label = dict(itertools.islice(test_qid_rel.items(), 10))\n",
        "\n",
        "# toy_test_set = test_set[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcwee7fkEmB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print((toy_test_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQkx2N6nDgNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Load the model with the best validation loss\n",
        "# model.load_state_dict(torch.load('20200203_071508model-50.pt'))\n",
        "\n",
        "# rank = eval(model, toy_test_set, toy_test_label, max_seq_len, k=10)\n",
        "\n",
        "# # print(test_set[0][1])\n",
        "\n",
        "# # print(rank[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmVtTBwxEr3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy_test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36pTUBTwEZbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rank[80]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99aL2wiN4pFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "f30c5657-7624-422b-b8ea-a1d1fd231e1e"
      },
      "source": [
        "# Load the model with the best validation loss\n",
        "model.load_state_dict(torch.load('20200203_103058_model-eu.pt'))\n",
        "\n",
        "k = 10\n",
        "\n",
        "qid_pred_rank, MRR, average_ndcg, precision = eval(model, test_set_full, test_qid_rel, max_seq_len, k=10)\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 330/330 [28:12<00:00,  5.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 330 queries: 0.04858448433124229\n",
            "\n",
            "MRR@10 for 330 queries: 0.029186329186329185\n",
            "\n",
            "Average Precision@1: 0.012121212121212121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEfYbV7nNe4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'qid_pred_rank.pickle', qid_pred_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heqAUd-pXoEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "5dc41c5a-5591-4e29-db0b-e31d45254a88"
      },
      "source": [
        "take(10, test_qid_rel.items())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, [14255]),\n",
              " (25, [107584, 562777]),\n",
              " (43, [76662]),\n",
              " (80, [252473]),\n",
              " (460, [591357, 556021, 555097]),\n",
              " (474, [430901]),\n",
              " (509, [377152]),\n",
              " (523, [338348]),\n",
              " (541, [533825]),\n",
              " (543, [504419])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnGpK9laWKU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "7b72f40c-1f8a-4dca-b1c5-51ef5a761619"
      },
      "source": [
        "i = 541\n",
        "\n",
        "print(\" \".join(qid_to_text[i]))\n",
        "print()\n",
        "docid = test_qid_rel[i]\n",
        "\n",
        "print(\" \".join(docid_to_text[docid[0]]))\n",
        "print()\n",
        "print(\" \".join(docid_to_text[qid_pred_rank[i][0]]))\n",
        "print()\n",
        "print(\" \".join(docid_to_text[test_set_full[8][2][0]]))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "can i deduct my individual health insurance premium in tax\n",
            "\n",
            "yes you can see the instructions for line 29 of form 1040 self employed health insurance premiums are an above the line deduction\n",
            "\n",
            "now i have kept this money and after interval of 6 month or year whenever the usd price go up i do exchange with indian currency and deposit in my account now do i have to pay tax on this money no you are not required to pay any tax as the income was accrued when your were nri for tax purposes the foreign currency upto usd 2000 can be held by an individual without any time limit ie you can convert then whenever you want there is nothing that needs to be declared in tax returns\n",
            "\n",
            "the basic idea is that the average person cant deduct health care costs unless theyre really onerous but a business can and as a self employed person you can deduct those costs from the businesses earnings as long as the business is really generating enough profit to cover the health insurance costs thats why most people get their health insurance from their employer actually the relevant irs rules say you may be able to deduct premiums paid for medical and dental insurance and qualified long term care insurance for you your spouse and your dependents if you are a self employed individual with a net profit reported on schedule c form 1040 for 2010 thanks to the small business jobs act of 2010 you can even deduct the premium from your income before deducting the self employment tax source im sure that when you get your tax returns and instructions for 2010 this will all be spelled out\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}